const sklearn={
    "sklearn.cluster.AffinityPropagation": {
        "cls": "Block",
        "typename": "AffinityPropagation",
        "desc": "Perform Affinity Propagation Clustering of data.  Read more in the :ref:`User Guide <affinity_propagation>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "damping": {
                "type": "number",
                "desc": "Damping factor (between 0.5 and 1) is the extent towhich the current value is maintained relative toincoming values (weighted 1 - damping). This in orderto avoid numerical oscillations when updating thesevalues (messages).",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations.",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "convergence_iter": {
                "type": "number",
                "desc": "Number of iterations with no change in the numberof estimated clusters that stops the convergence.",
                "default": "15",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Make a copy of input data.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "preference": {
                "type": "array-like of shape (n_samples,) or float, default=None",
                "desc": "Preferences for each point - points with larger values ofpreferences are more likely to be chosen as exemplars. The numberof exemplars, ie of clusters, is influenced by the inputpreferences value. If the preferences are not passed as arguments,they will be set to the median of the input similarities.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "affinity": {
                "type": "option(euclidean, precomputed)",
                "desc": "Which affinity to use. At the moment 'precomputed' and``euclidean`` are supported. 'euclidean' uses thenegative squared euclidean distance between points.",
                "default": "'euclidean'",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Whether to be verbose.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Pseudo-random number generator to control the starting state.Use an int for reproducible results across function calls.See the :term:`Glossary <random_state>`... versionadded:: 0.23this parameter was previously hardcoded as 0.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.AffinityPropagation"
        }
    },
    "sklearn.cluster.AgglomerativeClustering": {
        "cls": "Block",
        "typename": "AgglomerativeClustering",
        "desc": "Agglomerative Clustering  Recursively merges the pair of clusters that minimally increases a given linkage distance.  Read more in the :ref:`User Guide <hierarchical_clustering>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_clusters": {
                "type": "number",
                "desc": "The number of clusters to find. It must be ``None`` if``distance_threshold`` is not ``None``.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "affinity": {
                "type": "string",
                "desc": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\"manhattan\", \"cosine\", or \"precomputed\".If linkage is \"ward\", only \"euclidean\" is accepted.If \"precomputed\", a distance matrix (instead of a similarity matrix)is needed as input for the fit method.",
                "default": "'euclidean'",
                "dictKeyOf": "initkargs"
            },
            "memory": {
                "type": "string",
                "desc": "Used to cache the output of the computation of the tree.By default, no caching is done. If a string is given, it is thepath to the caching directory.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "connectivity": {
                "type": "array-like or callable, default=None",
                "desc": "Connectivity matrix. Defines for each sample the neighboringsamples following a given structure of the data.This can be a connectivity matrix itself or a callable that transformsthe data into a connectivity matrix, such as derived fromkneighbors_graph. Default is ``None``, i.e, thehierarchical clustering algorithm is unstructured.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "compute_full_tree": {
                "type": "'auto' or bool, default='auto'",
                "desc": "Stop early the construction of the tree at ``n_clusters``. This isuseful to decrease computation time if the number of clusters is notsmall compared to the number of samples. This option is useful onlywhen specifying a connectivity matrix. Note also that when varying thenumber of clusters and using caching, it may be advantageous to computethe full tree. It must be ``True`` if ``distance_threshold`` is not``None``. By default `compute_full_tree` is \"auto\", which is equivalentto `True` when `distance_threshold` is not `None` or that `n_clusters`is inferior to the maximum between 100 or `0.02 * n_samples`.Otherwise, \"auto\" is equivalent to `False`.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "linkage": {
                "type": "option(ward, complete, average, single)",
                "desc": "Which linkage criterion to use. The linkage criterion determines whichdistance to use between sets of observation. The algorithm will mergethe pairs of cluster that minimize this criterion.- 'ward' minimizes the variance of the clusters being merged.- 'average' uses the average of the distances of each observation ofthe two sets.- 'complete' or 'maximum' linkage uses the maximum distances betweenall observations of the two sets.- 'single' uses the minimum of the distances between all observationsof the two sets... versionadded:: 0.20Added the 'single' option",
                "default": "'ward'",
                "dictKeyOf": "initkargs"
            },
            "distance_threshold": {
                "type": "number",
                "desc": "The linkage distance threshold above which, clusters will not bemerged. If not ``None``, ``n_clusters`` must be ``None`` and``compute_full_tree`` must be ``True``... versionadded:: 0.21",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "compute_distances": {
                "type": "boolean",
                "desc": "Computes distances between clusters even if `distance_threshold` is notused. This can be used to make dendrogram visualization, but introducesa computational and memory overhead... versionadded:: 0.24",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.AgglomerativeClustering"
        }
    },
    "sklearn.cluster.Birch": {
        "cls": "Block",
        "typename": "Birch",
        "desc": "Implements the BIRCH clustering algorithm.  It is a memory-efficient, online-learning algorithm provided as an alternative to :class:`MiniBatchKMeans`. It constructs a tree data structure with the cluster centroids being read off the leaf. These can be either the final cluster centroids or can be provided as input to another clustering algorithm such as :class:`AgglomerativeClustering`.  Read more in the :ref:`User Guide <birch>`.  .. versionadded:: 0.16",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "threshold": {
                "type": "number",
                "desc": "The radius of the subcluster obtained by merging a new sample and theclosest subcluster should be lesser than the threshold. Otherwise a newsubcluster is started. Setting this value to be very low promotessplitting and vice-versa.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "branching_factor": {
                "type": "number",
                "desc": "Maximum number of CF subclusters in each node. If a new samples enterssuch that the number of subclusters exceed the branching_factor thenthat node is split into two nodes with the subclusters redistributedin each. The parent subcluster of that node is removed and two newsubclusters are added as parents of the 2 split nodes.",
                "default": "50",
                "dictKeyOf": "initkargs"
            },
            "n_clusters": {
                "type": "number",
                "desc": "Number of clusters after the final clustering step, which treats thesubclusters from the leaves as new samples.- `None` : the final clustering step is not performed and thesubclusters are returned as they are.- :mod:`sklearn.cluster` Estimator : If a model is provided, the modelis fit treating the subclusters as new samples and the initial datais mapped to the label of the closest subcluster.- `int` : the model fit is :class:`AgglomerativeClustering` with`n_clusters` set to be equal to the int.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "compute_labels": {
                "type": "boolean",
                "desc": "Whether or not to compute labels for each fit.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Whether or not to make a copy of the given data. If set to False,the initial data will be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.Birch"
        }
    },
    "sklearn.cluster.DBSCAN": {
        "cls": "Block",
        "typename": "DBSCAN",
        "desc": "Perform DBSCAN clustering from vector array or distance matrix.  DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.  Read more in the :ref:`User Guide <dbscan>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "eps": {
                "type": "number",
                "desc": "The maximum distance between two samples for one to be consideredas in the neighborhood of the other. This is not a maximum boundon the distances of points within a cluster. This is the mostimportant DBSCAN parameter to choose appropriately for your data setand distance function.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "min_samples": {
                "type": "number",
                "desc": "The number of samples (or total weight) in a neighborhood for a pointto be considered as a core point. This includes the point itself.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string or callable, it must be one ofthe options allowed by :func:`sklearn.metrics.pairwise_distances` forits metric parameter.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square. X may be a :term:`Glossary <sparse graph>`, in whichcase only \"nonzero\" elements may be considered neighbors for DBSCAN... versionadded:: 0.17metric *precomputed* to accept precomputed sparse matrix.",
                "default": "'euclidean'",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function... versionadded:: 0.19",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "The algorithm to be used by the NearestNeighbors moduleto compute pointwise distances and find nearest neighbors.See NearestNeighbors module documentation for details.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or cKDTree. This can affect the speedof the construction and query, as well as the memory requiredto store the tree. The optimal value dependson the nature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "The power of the Minkowski metric to be used to calculate distancebetween points. If None, then ``p=2`` (equivalent to the Euclideandistance).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.DBSCAN"
        }
    },
    "sklearn.cluster.FeatureAgglomeration": {
        "cls": "Block",
        "typename": "FeatureAgglomeration",
        "desc": "Agglomerate features.  Similar to AgglomerativeClustering, but recursively merges features instead of samples.  Read more in the :ref:`User Guide <hierarchical_clustering>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_clusters": {
                "type": "number",
                "desc": "The number of clusters to find. It must be ``None`` if``distance_threshold`` is not ``None``.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "affinity": {
                "type": "string",
                "desc": "Metric used to compute the linkage. Can be \"euclidean\", \"l1\", \"l2\",\"manhattan\", \"cosine\", or 'precomputed'.If linkage is \"ward\", only \"euclidean\" is accepted.",
                "default": "'euclidean'",
                "dictKeyOf": "initkargs"
            },
            "memory": {
                "type": "string",
                "desc": "Used to cache the output of the computation of the tree.By default, no caching is done. If a string is given, it is thepath to the caching directory.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "connectivity": {
                "type": "array-like or callable, default=None",
                "desc": "Connectivity matrix. Defines for each feature the neighboringfeatures following a given structure of the data.This can be a connectivity matrix itself or a callable that transformsthe data into a connectivity matrix, such as derived fromkneighbors_graph. Default is None, i.e, thehierarchical clustering algorithm is unstructured.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "compute_full_tree": {
                "type": "'auto' or bool, default='auto'",
                "desc": "Stop early the construction of the tree at n_clusters. This is usefulto decrease computation time if the number of clusters is not smallcompared to the number of features. This option is useful only whenspecifying a connectivity matrix. Note also that when varying thenumber of clusters and using caching, it may be advantageous to computethe full tree. It must be ``True`` if ``distance_threshold`` is not``None``. By default `compute_full_tree` is \"auto\", which is equivalentto `True` when `distance_threshold` is not `None` or that `n_clusters`is inferior to the maximum between 100 or `0.02 * n_samples`.Otherwise, \"auto\" is equivalent to `False`.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "linkage": {
                "type": "option(ward, complete, average, single)",
                "desc": "Which linkage criterion to use. The linkage criterion determines whichdistance to use between sets of features. The algorithm will mergethe pairs of cluster that minimize this criterion.- ward minimizes the variance of the clusters being merged.- average uses the average of the distances of each feature ofthe two sets.- complete or maximum linkage uses the maximum distances betweenall features of the two sets.- single uses the minimum of the distances between all featuresof the two sets.",
                "default": "'ward'",
                "dictKeyOf": "initkargs"
            },
            "pooling_func": {
                "type": "callable, default=np.mean",
                "desc": "This combines the values of agglomerated features into a singlevalue, and should accept an array of shape [M, N] and the keywordargument `axis=1`, and reduce it to an array of size [M].",
                "default": "np.mean",
                "dictKeyOf": "initkargs"
            },
            "distance_threshold": {
                "type": "number",
                "desc": "The linkage distance threshold above which, clusters will not bemerged. If not ``None``, ``n_clusters`` must be ``None`` and``compute_full_tree`` must be ``True``... versionadded:: 0.21",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "compute_distances": {
                "type": "boolean",
                "desc": "Computes distances between clusters even if `distance_threshold` is notused. This can be used to make dendrogram visualization, but introducesa computational and memory overhead... versionadded:: 0.24",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.FeatureAgglomeration"
        }
    },
    "sklearn.cluster.KMeans": {
        "cls": "Block",
        "typename": "KMeans",
        "desc": "K-Means clustering.  Read more in the :ref:`User Guide <k_means>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_clusters": {
                "type": "number",
                "desc": "The number of clusters to form as well as the number ofcentroids to generate.",
                "default": "8",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "option(k-means++, random)",
                "desc": "Method for initialization:'k-means++' : selects initial cluster centers for k-meanclustering in a smart way to speed up convergence. See sectionNotes in k_init for more details.'random': choose `n_clusters` observations (rows) at random from datafor the initial centroids.If an array is passed, it should be of shape (n_clusters, n_features)and gives the initial centers.If a callable is passed, it should take arguments X, n_clusters and arandom state and return an initialization.",
                "default": "'k-means++'",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "Number of time the k-means algorithm will be run with differentcentroid seeds. The final results will be the best output ofn_init consecutive runs in terms of inertia.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations of the k-means algorithm for asingle run.",
                "default": "300",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Relative tolerance with regards to Frobenius norm of the differencein the cluster centers of two consecutive iterations to declareconvergence.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "precompute_distances": {
                "type": "option(auto, True, False)",
                "desc": "Precompute distances (faster but takes more memory).'auto' : do not precompute distances if n_samples * n_clusters > 12million. This corresponds to about 100MB overhead per job usingdouble precision.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "True": {
                "type": "always precompute distances.",
                "desc": "",
                "dictKeyOf": "initkargs"
            },
            "False": {
                "type": "never precompute distances.",
                "desc": ".. deprecated:: 0.23'precompute_distances' was deprecated in version 0.22 and will beremoved in 1.0 (renaming of 0.25). It has no effect.",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Verbosity mode.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for centroid initialization. Usean int to make the randomness deterministic.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "copy_x": {
                "type": "boolean",
                "desc": "When pre-computing distances it is more numerically accurate to centerthe data first. If copy_x is True (default), then the original data isnot modified. If False, the original data is modified, and put backbefore the function returns, but small numerical differences may beintroduced by subtracting and then adding the data mean. Note that ifthe original data is not C-contiguous, a copy will be made even ifcopy_x is False. If the original data is sparse, but not in CSR format,a copy will be made even if copy_x is False.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of OpenMP threads to use for the computation. Parallelism issample-wise on the main cython loop which assigns each sample to itsclosest center.``None`` or ``-1`` means using all processors... deprecated:: 0.23``n_jobs`` was deprecated in version 0.23 and will be removed in1.0 (renaming of 0.25).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(\"auto\", \"full\", \"elkan\")",
                "desc": "K-means algorithm to use. The classical EM-style algorithm is \"full\".The \"elkan\" variation is more efficient on data with well-definedclusters, by using the triangle inequality. However it's more memoryintensive due to the allocation of an extra array of shape(n_samples, n_clusters).For now \"auto\" (kept for backward compatibiliy) chooses \"elkan\" but itmight change in the future for a better heuristic... versionchanged:: 0.18Added Elkan algorithm",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.KMeans"
        }
    },
    "sklearn.cluster.MeanShift": {
        "cls": "Block",
        "typename": "MeanShift",
        "desc": "Mean shift clustering using a flat kernel.  Mean shift clustering aims to discover \"blobs\" in a smooth density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids.  Seeding is performed using a binning technique for scalability.  Read more in the :ref:`User Guide <mean_shift>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "bandwidth": {
                "type": "number",
                "desc": "Bandwidth used in the RBF kernel.If not given, the bandwidth is estimated usingsklearn.cluster.estimate_bandwidth; see the documentation for thatfunction for hints on scalability (see also the Notes, below).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "seeds": {
                "type": "array-like of shape (n_samples, n_features), default=None",
                "desc": "Seeds used to initialize kernels. If not set,the seeds are calculated by clustering.get_bin_seedswith bandwidth as the grid size and default values forother parameters.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "bin_seeding": {
                "type": "boolean",
                "desc": "If true, initial kernel locations are not locations of allpoints, but rather the location of the discretized version ofpoints, where points are binned onto a grid whose coarsenesscorresponds to the bandwidth. Setting this option to True will speedup the algorithm because fewer seeds will be initialized.The default value is False.Ignored if seeds argument is not None.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "min_bin_freq": {
                "type": "number",
                "desc": "To speed up the algorithm, accept only those bins with at leastmin_bin_freq points as seeds.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "cluster_all": {
                "type": "boolean",
                "desc": "If true, then all points are clustered, even those orphans that arenot within any kernel. Orphans are assigned to the nearest kernel.If false, then orphans are given cluster label -1.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This works by computingeach of the n_init runs in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations, per seed point before the clusteringoperation terminates (for that seed point), if has not converged yet... versionadded:: 0.22",
                "default": "300",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.MeanShift"
        }
    },
    "sklearn.cluster.MiniBatchKMeans": {
        "cls": "Block",
        "typename": "MiniBatchKMeans",
        "desc": "Mini-Batch K-Means clustering.  Read more in the :ref:`User Guide <mini_batch_kmeans>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_clusters": {
                "type": "number",
                "desc": "The number of clusters to form as well as the number ofcentroids to generate.",
                "default": "8",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "option(k-means++, random)",
                "desc": "Method for initialization:'k-means++' : selects initial cluster centers for k-meanclustering in a smart way to speed up convergence. See sectionNotes in k_init for more details.'random': choose `n_clusters` observations (rows) at random from datafor the initial centroids.If an array is passed, it should be of shape (n_clusters, n_features)and gives the initial centers.If a callable is passed, it should take arguments X, n_clusters and arandom state and return an initialization.",
                "default": "'k-means++'",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations over the complete dataset beforestopping independently of any early stopping criterion heuristics.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "Size of the mini batches.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Verbosity mode.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "compute_labels": {
                "type": "boolean",
                "desc": "Compute label assignment and inertia for the complete datasetonce the minibatch optimization has converged in fit.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for centroid initialization andrandom reassignment. Use an int to make the randomness deterministic.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Control early stopping based on the relative center changes asmeasured by a smoothed, variance-normalized of the mean centersquared position changes. This early stopping heuristics iscloser to the one used for the batch variant of the algorithmsbut induces a slight computational and memory overhead over theinertia heuristic.To disable convergence detection based on normalized centerchange, set tol to 0.0 (default).",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_no_improvement": {
                "type": "number",
                "desc": "Control early stopping based on the consecutive number of minibatches that does not yield an improvement on the smoothed inertia.To disable convergence detection based on inertia, setmax_no_improvement to None.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "init_size": {
                "type": "number",
                "desc": "Number of samples to randomly sample for speeding up theinitialization (sometimes at the expense of accuracy): theonly algorithm is initialized by running a batch KMeans on arandom subset of the data. This needs to be larger than n_clusters.If `None`, `init_size= 3 * batch_size`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "Number of random initializations that are tried.In contrast to KMeans, the algorithm is only run once, using thebest of the ``n_init`` initializations as measured by inertia.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "reassignment_ratio": {
                "type": "number",
                "desc": "Control the fraction of the maximum number of counts for acenter to be reassigned. A higher value means that low countcenters are more easily reassigned, which means that themodel will take longer to converge, but should converge in abetter clustering.",
                "default": "0.01",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.MiniBatchKMeans"
        }
    },
    "sklearn.cluster.OPTICS": {
        "cls": "Block",
        "typename": "OPTICS",
        "desc": "Estimate clustering structure from vector array.  OPTICS (Ordering Points To Identify the Clustering Structure), closely related to DBSCAN, finds core sample of high density and expands clusters from them [1]_. Unlike DBSCAN, keeps cluster hierarchy for a variable neighborhood radius. Better suited for usage on large datasets than the current sklearn implementation of DBSCAN.  Clusters are then extracted using a DBSCAN-like method (cluster_method = 'dbscan') or an automatic technique proposed in [1]_ (cluster_method = 'xi').  This implementation deviates from the original OPTICS by first performing k-nearest-neighborhood searches on all points to identify core sizes, then computing only the distances to unprocessed points when constructing the cluster order. Note that we do not employ a heap to manage the expansion candidates, so the time complexity will be O(n^2).  Read more in the :ref:`User Guide <optics>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "min_samples": {
                "type": "number",
                "desc": "The number of samples in a neighborhood for a point to be considered asa core point. Also, up and down steep regions can't have more than``min_samples`` consecutive non-steep points. Expressed as an absolutenumber or a fraction of the number of samples (rounded to be at least2).",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "max_eps": {
                "type": "number",
                "desc": "The maximum distance between two samples for one to be considered asin the neighborhood of the other. Default value of ``np.inf`` willidentify clusters across all scales; reducing ``max_eps`` will resultin shorter run times.",
                "default": "np.inf",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "Metric to use for distance computation. Any metric from scikit-learnor scipy.spatial.distance can be used.If metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays as input and return one value indicating thedistance between them. This works for Scipy's metrics, but is lessefficient than passing the metric name as a string. If metric is\"precomputed\", X is assumed to be a distance matrix and must be square.Valid values for metric are:- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2','manhattan']- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev','correlation', 'dice', 'hamming', 'jaccard', 'kulsinski','mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao','seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean','yule']See the documentation for scipy.spatial.distance for details on thesemetrics.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Parameter for the Minkowski metric from:class:`~sklearn.metrics.pairwise_distances`. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cluster_method": {
                "type": "string",
                "desc": "The extraction method used to extract clusters using the calculatedreachability and ordering. Possible values are \"xi\" and \"dbscan\".",
                "default": "'xi'",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "The maximum distance between two samples for one to be considered asin the neighborhood of the other. By default it assumes the same valueas ``max_eps``.Used only when ``cluster_method='dbscan'``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "xi": {
                "type": "number",
                "desc": "Determines the minimum steepness on the reachability plot thatconstitutes a cluster boundary. For example, an upwards point in thereachability plot is defined by the ratio from one point to itssuccessor being at most 1-xi.Used only when ``cluster_method='xi'``.",
                "default": "0.05",
                "dictKeyOf": "initkargs"
            },
            "predecessor_correction": {
                "type": "boolean",
                "desc": "Correct clusters according to the predecessors calculated by OPTICS[2]_. This parameter has minimal effect on most datasets.Used only when ``cluster_method='xi'``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "min_cluster_size": {
                "type": "number",
                "desc": "Minimum number of samples in an OPTICS cluster, expressed as anabsolute number or a fraction of the number of samples (rounded to beat least 2). If ``None``, the value of ``min_samples`` is used instead.Used only when ``cluster_method='xi'``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method. (default)Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to :class:`BallTree` or :class:`KDTree`. This canaffect the speed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.OPTICS"
        }
    },
    "sklearn.cluster.SpectralBiclustering": {
        "cls": "Block",
        "typename": "SpectralBiclustering",
        "desc": "Spectral biclustering (Kluger, 2003).  Partitions rows and columns under the assumption that the data has an underlying checkerboard structure. For instance, if there are two row partitions and three column partitions, each row will belong to three biclusters, and each column will belong to two biclusters. The outer product of the corresponding row and column label vectors gives this checkerboard structure.  Read more in the :ref:`User Guide <spectral_biclustering>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_clusters": {
                "type": "number",
                "desc": "The number of row and column clusters in the checkerboardstructure.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "method": {
                "type": "option(bistochastic, scale, log)",
                "desc": "Method of normalizing and converting singular vectors intobiclusters. May be one of 'scale', 'bistochastic', or 'log'.The authors recommend using 'log'. If the data is sparse,however, log normalization will not work, which is why thedefault is 'bistochastic'... warning::if `method='log'`, the data must be sparse.",
                "default": "'bistochastic'",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "Number of singular vectors to check.",
                "default": "6",
                "dictKeyOf": "initkargs"
            },
            "n_best": {
                "type": "number",
                "desc": "Number of best singular vectors to which to project the datafor clustering.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "svd_method": {
                "type": "option(randomized, arpack)",
                "desc": "Selects the algorithm for finding singular vectors. May be'randomized' or 'arpack'. If 'randomized', uses:func:`~sklearn.utils.extmath.randomized_svd`, which may be fasterfor large matrices. If 'arpack', uses`scipy.sparse.linalg.svds`, which is more accurate, butpossibly slower in some cases.",
                "default": "'randomized'",
                "dictKeyOf": "initkargs"
            },
            "n_svd_vecs": {
                "type": "number",
                "desc": "Number of vectors to use in calculating the SVD. Correspondsto `ncv` when `svd_method=arpack` and `n_oversamples` when`svd_method` is 'randomized`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "mini_batch": {
                "type": "boolean",
                "desc": "Whether to use mini-batch k-means, which is faster but may getdifferent results.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "option(k-means++, random)",
                "desc": "Method for initialization of k-means algorithm; defaults to'k-means++'.",
                "default": "'k-means++'",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "Number of random initializations that are tried with thek-means algorithm.If mini-batch k-means is used, the best initialization ischosen and the algorithm runs once. Otherwise, the algorithmis run for each initialization and the best solution chosen.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This works by breakingdown the pairwise matrix into n_jobs even slices and computing them inparallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... deprecated:: 0.23``n_jobs`` was deprecated in version 0.23 and will be removed in1.0 (renaming of 0.25).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for randomizing the singular value decomposition and the k-meansinitialization. Use an int to make the randomness deterministic.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.SpectralBiclustering"
        }
    },
    "sklearn.cluster.SpectralClustering": {
        "cls": "Block",
        "typename": "SpectralClustering",
        "desc": "Apply clustering to a projection of the normalized Laplacian.  In practice Spectral Clustering is very useful when the structure of the individual clusters is highly non-convex, or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster, such as when clusters are nested circles on the 2D plane.  If the affinity matrix is the adjacency matrix of a graph, this method can be used to find normalized graph cuts.  When calling ``fit``, an affinity matrix is constructed using either a kernel function such the Gaussian (aka RBF) kernel with Euclidean distance ``d(X, X)``::          np.exp(-gamma * d(X,X) ** 2)  or a k-nearest neighbors connectivity matrix.  Alternatively, a user-provided affinity matrix can be specified by setting ``affinity='precomputed'``.  Read more in the :ref:`User Guide <spectral_clustering>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_clusters": {
                "type": "number",
                "desc": "The dimension of the projection subspace.",
                "default": "8",
                "dictKeyOf": "initkargs"
            },
            "eigen_solver": {
                "type": "option(arpack, lobpcg, amg)",
                "desc": "The eigenvalue decomposition strategy to use. AMG requires pyamgto be installed. It can be faster on very large, sparse problems,but may also lead to instabilities. If None, then ``'arpack'`` isused.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "Number of eigenvectors to use for the spectral embedding",
                "default": "n_clusters",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "A pseudo random number generator used for the initialization of thelobpcg eigenvectors decomposition when ``eigen_solver='amg'`` and bythe K-Means initialization. Use an int to make the randomnessdeterministic.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "Number of time the k-means algorithm will be run with differentcentroid seeds. The final results will be the best output of n_initconsecutive runs in terms of inertia. Only used if``assign_labels='kmeans'``.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "number",
                "desc": "Kernel coefficient for rbf, poly, sigmoid, laplacian and chi2 kernels.Ignored for ``affinity='nearest_neighbors'``.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "affinity": {
                "type": "string",
                "desc": "How to construct the affinity matrix.- 'nearest_neighbors': construct the affinity matrix by computing agraph of nearest neighbors.- 'rbf': construct the affinity matrix using a radial basis function(RBF) kernel.- 'precomputed': interpret ``X`` as a precomputed affinity matrix,where larger values indicate greater similarity between instances.- 'precomputed_nearest_neighbors': interpret ``X`` as a sparse graphof precomputed distances, and construct a binary affinity matrixfrom the ``n_neighbors`` nearest neighbors of each instance.- one of the kernels supported by:func:`~sklearn.metrics.pairwise_kernels`.Only kernels that produce similarity scores (non-negative values thatincrease with similarity) should be used. This property is not checkedby the clustering algorithm.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of neighbors to use when constructing the affinity matrix usingthe nearest neighbors method. Ignored for ``affinity='rbf'``.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "eigen_tol": {
                "type": "number",
                "desc": "Stopping criterion for eigendecomposition of the Laplacian matrixwhen ``eigen_solver='arpack'``.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "assign_labels": {
                "type": "option(kmeans, discretize)",
                "desc": "The strategy for assigning labels in the embedding space. There are twoways to assign labels after the Laplacian embedding. k-means is apopular choice, but it can be sensitive to initialization.Discretization is another approach which is less sensitive to randominitialization.",
                "default": "'kmeans'",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel. Ignored by other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Zero coefficient for polynomial and sigmoid kernels.Ignored by other kernels.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "kernel_params": {
                "type": "dict of str to any, default=None",
                "desc": "Parameters (keyword arguments) and values for kernel passed ascallable object. Ignored by other kernels.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run when `affinity='nearest_neighbors'`or `affinity='precomputed_nearest_neighbors'`. The neighbors searchwill be done in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Verbosity mode... versionadded:: 0.24",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.SpectralClustering"
        }
    },
    "sklearn.cluster.SpectralCoclustering": {
        "cls": "Block",
        "typename": "SpectralCoclustering",
        "desc": "Spectral Co-Clustering algorithm (Dhillon, 2001).  Clusters rows and columns of an array `X` to solve the relaxed normalized cut of the bipartite graph created from `X` as follows: the edge between row vertex `i` and column vertex `j` has weight `X[i, j]`.  The resulting bicluster structure is block-diagonal, since each row and each column belongs to exactly one bicluster.  Supports sparse matrices, as long as they are nonnegative.  Read more in the :ref:`User Guide <spectral_coclustering>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cluster",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_clusters": {
                "type": "number",
                "desc": "The number of biclusters to find.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "svd_method": {
                "type": "option(randomized, arpack)",
                "desc": "Selects the algorithm for finding singular vectors. May be'randomized' or 'arpack'. If 'randomized', use:func:`sklearn.utils.extmath.randomized_svd`, which may be fasterfor large matrices. If 'arpack', use:func:`scipy.sparse.linalg.svds`, which is more accurate, butpossibly slower in some cases.",
                "default": "'randomized'",
                "dictKeyOf": "initkargs"
            },
            "n_svd_vecs": {
                "type": "number",
                "desc": "Number of vectors to use in calculating the SVD. Correspondsto `ncv` when `svd_method=arpack` and `n_oversamples` when`svd_method` is 'randomized`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "mini_batch": {
                "type": "boolean",
                "desc": "Whether to use mini-batch k-means, which is faster but may getdifferent results.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "option(k-means++, random, or ndarray of shape             (n_clusters, n_features), default=k-means++)",
                "desc": "Method for initialization of k-means algorithm; defaults to'k-means++'.",
                "default": "'k-means++'",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "Number of random initializations that are tried with thek-means algorithm.If mini-batch k-means is used, the best initialization ischosen and the algorithm runs once. Otherwise, the algorithmis run for each initialization and the best solution chosen.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This works by breakingdown the pairwise matrix into n_jobs even slices and computing them inparallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... deprecated:: 0.23``n_jobs`` was deprecated in version 0.23 and will be removed in1.0 (renaming of 0.25).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for randomizing the singular value decomposition and the k-meansinitialization. Use an int to make the randomness deterministic.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cluster.SpectralCoclustering"
        }
    },
    "sklearn.compose.ColumnTransformer": {
        "cls": "Block",
        "typename": "ColumnTransformer",
        "desc": "Applies transformers to columns of an array or pandas DataFrame.  This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer.  Read more in the :ref:`User Guide <column_transformer>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.compose",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "transformers": {
                "type": "list of tuples",
                "desc": "List of (name, transformer, columns) tuples specifying thetransformer objects to be applied to subsets of the data.",
                "dictKeyOf": "initkargs"
            },
            "name": {
                "type": "string",
                "desc": "Like in Pipeline and FeatureUnion, this allows the transformer andits parameters to be set using ``set_params`` and searched in gridsearch.",
                "dictKeyOf": "initkargs"
            },
            "transformer": {
                "type": "option(drop, passthrough)",
                "desc": "Estimator must support :term:`fit` and :term:`transform`.Special-cased strings 'drop' and 'passthrough' are accepted aswell, to indicate to drop the columns or to pass them throughuntransformed, respectively.",
                "dictKeyOf": "initkargs"
            },
            "columns": {
                "type": " str, array-like of str, int, array-like of int,                 array-like of bool, slice or callable",
                "desc": "Indexes the data on its second axis. Integers are interpreted aspositional columns, while strings can reference DataFrame columnsby name. A scalar string or int should be used where``transformer`` expects X to be a 1d array-like (vector),otherwise a 2d array will be passed to the transformer.A callable is passed the input data `X` and can return any of theabove. To select multiple columns by name or dtype, you can use:obj:`make_column_selector`.",
                "dictKeyOf": "initkargs"
            },
            "remainder": {
                "type": "option(drop, passthrough)",
                "desc": "By default, only the specified columns in `transformers` aretransformed and combined in the output, and the non-specifiedcolumns are dropped. (default of ``'drop'``).By specifying ``remainder='passthrough'``, all remaining columns thatwere not specified in `transformers` will be automatically passedthrough. This subset of columns is concatenated with the output ofthe transformers.By setting ``remainder`` to be an estimator, the remainingnon-specified columns will use the ``remainder`` estimator. Theestimator must support :term:`fit` and :term:`transform`.Note that using this feature requires that the DataFrame columnsinput at :term:`fit` and :term:`transform` have identical order.",
                "default": "'drop'",
                "dictKeyOf": "initkargs"
            },
            "sparse_threshold": {
                "type": "number",
                "desc": "If the output of the different transformers contains sparse matrices,these will be stacked as a sparse matrix if the overall density islower than this value. Use ``sparse_threshold=0`` to always returndense. When the transformed output consists of all dense data, thestacked result will be dense, and this keyword will be ignored.",
                "default": "0.3",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of jobs to run in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "transformer_weights": {
                "type": "dict, default=None",
                "desc": "Multiplicative weights for features per transformer. The output of thetransformer is multiplied by these weights. Keys are transformer names,values the weights.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "If True, the time elapsed while fitting each transformer will beprinted as it is completed.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.compose.ColumnTransformer"
        }
    },
    "sklearn.compose.TransformedTargetRegressor": {
        "cls": "Block",
        "typename": "TransformedTargetRegressor",
        "desc": "Meta-estimator to regress on a transformed target.  Useful for applying a non-linear transformation to the target ``y`` in regression problems. This transformation can be given as a Transformer such as the QuantileTransformer or as a function and its inverse such as ``log`` and ``exp``.  The computation during ``fit`` is::      regressor.fit(X, func(y))  or::      regressor.fit(X, transformer.transform(y))  The computation during ``predict`` is::      inverse_func(regressor.predict(X))  or::      transformer.inverse_transform(regressor.predict(X))  Read more in the :ref:`User Guide <transformed_target_regressor>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.compose",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "regressor": {
                "type": "object, default=None",
                "desc": "Regressor object such as derived from ``RegressorMixin``. Thisregressor will automatically be cloned each time prior to fitting.If regressor is ``None``, ``LinearRegression()`` is created and used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "transformer": {
                "type": "object, default=None",
                "desc": "Estimator object such as derived from ``TransformerMixin``. Cannot beset at the same time as ``func`` and ``inverse_func``. If``transformer`` is ``None`` as well as ``func`` and ``inverse_func``,the transformer will be an identity transformer. Note that thetransformer will be cloned during fitting. Also, the transformer isrestricting ``y`` to be a numpy array.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "func": {
                "type": "function, default=None",
                "desc": "Function to apply to ``y`` before passing to ``fit``. Cannot be set atthe same time as ``transformer``. The function needs to return a2-dimensional array. If ``func`` is ``None``, the function used will bethe identity function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "inverse_func": {
                "type": "function, default=None",
                "desc": "Function to apply to the prediction of the regressor. Cannot be set atthe same time as ``transformer`` as well. The function needs to returna 2-dimensional array. The inverse function is used to returnpredictions to the same space of the original training labels.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "check_inverse": {
                "type": "boolean",
                "desc": "Whether to check that ``transform`` followed by ``inverse_transform``or ``func`` followed by ``inverse_func`` leads to the original targets.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.compose.TransformedTargetRegressor"
        }
    },
    "sklearn.compose.make_column_selector": {
        "cls": "Block",
        "typename": "make_column_selector",
        "desc": "Create a callable to select columns to be used with :class:`ColumnTransformer`.  :func:`make_column_selector` can select columns based on datatype or the columns name with a regex. When using multiple selection criteria, **all** criteria must match for a column to be selected.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.compose",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "pattern": {
                "type": "string",
                "desc": "Name of columns containing this regex pattern will be included. IfNone, column selection will not be selected based on pattern.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "dtype_include": {
                "type": "column dtype or list of column dtypes, default=None",
                "desc": "A selection of dtypes to include. For more details, see:meth:`pandas.DataFrame.select_dtypes`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "dtype_exclude": {
                "type": "column dtype or list of column dtypes, default=None",
                "desc": "A selection of dtypes to exclude. For more details, see:meth:`pandas.DataFrame.select_dtypes`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.compose.make_column_selector"
        }
    },
    "sklearn.covariance.EllipticEnvelope": {
        "cls": "Block",
        "typename": "EllipticEnvelope",
        "desc": "An object for detecting outliers in a Gaussian distributed dataset.  Read more in the :ref:`User Guide <outlier_detection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "store_precision": {
                "type": "boolean",
                "desc": "Specify if the estimated precision is stored.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, the support of robust location and covariance estimatesis computed, and a covariance estimate is recomputed from it,without centering the data.Useful to work with data whose mean is significantly equal tozero but is not exactly zero.If False, the robust location and covariance are directly computedwith the FastMCD algorithm without additional treatment.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "support_fraction": {
                "type": "number",
                "desc": "The proportion of points to be included in the support of the rawMCD estimate. If None, the minimum value of support_fraction willbe used within the algorithm: `[n_sample + n_features + 1] / 2`.Range is (0, 1).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "contamination": {
                "type": "number",
                "desc": "The amount of contamination of the data set, i.e. the proportionof outliers in the data set. Range is (0, 0.5).",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines the pseudo random number generator for shufflingthe data. Pass an int for reproducible results across multiple functioncalls. See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.EllipticEnvelope"
        }
    },
    "sklearn.covariance.EmpiricalCovariance": {
        "cls": "Block",
        "typename": "EmpiricalCovariance",
        "desc": "Maximum likelihood covariance estimator  Read more in the :ref:`User Guide <covariance>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "store_precision": {
                "type": "boolean",
                "desc": "Specifies if the estimated precision is stored.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, data are not centered before computation.Useful when working with data whose mean is almost, but not exactlyzero.If False (default), data are centered before computation.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.EmpiricalCovariance"
        }
    },
    "sklearn.covariance.GraphicalLasso": {
        "cls": "Block",
        "typename": "GraphicalLasso",
        "desc": "Sparse inverse covariance estimation with an l1-penalized estimator.  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  .. versionchanged:: v0.20     GraphLasso has been renamed to GraphicalLasso",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "The regularization parameter: the higher alpha, the moreregularization, the sparser the inverse covariance.Range is (0, inf].",
                "default": "0.01",
                "dictKeyOf": "initkargs"
            },
            "mode": {
                "type": "option(cd, lars)",
                "desc": "The Lasso solver to use: coordinate descent or LARS. Use LARS forvery sparse underlying graphs, where p > n. Elsewhere prefer cdwhich is more numerically stable.",
                "default": "'cd'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance to declare convergence: if the dual gap goes belowthis value, iterations are stopped. Range is (0, inf].",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "enet_tol": {
                "type": "number",
                "desc": "The tolerance for the elastic net solver used to calculate the descentdirection. This parameter controls the accuracy of the search directionfor a given column update, not of the overall parameter estimate. Onlyused for mode='cd'. Range is (0, inf].",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "If verbose is True, the objective function and dual gap areplotted at each iteration.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, data are not centered before computation.Useful when working with data whose mean is almost, but not exactlyzero.If False, data are centered before computation.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.GraphicalLasso"
        }
    },
    "sklearn.covariance.GraphicalLassoCV": {
        "cls": "Block",
        "typename": "GraphicalLassoCV",
        "desc": "Sparse inverse covariance w/ cross-validated choice of the l1 penalty.  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <sparse_inverse_covariance>`.  .. versionchanged:: v0.20     GraphLassoCV has been renamed to GraphicalLassoCV",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alphas": {
                "type": "number",
                "desc": "If an integer is given, it fixes the number of points on thegrids of alpha to be used. If a list is given, it gives thegrid to be used. See the notes in the class docstring formore details. Range is (0, inf] when floats given.",
                "default": "4",
                "dictKeyOf": "initkargs"
            },
            "n_refinements": {
                "type": "number",
                "desc": "The number of times the grid is refined. Not used if explicitvalues of alphas are passed. Range is [1, inf).",
                "default": "4",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- integer, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.20``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance to declare convergence: if the dual gap goes belowthis value, iterations are stopped. Range is (0, inf].",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "enet_tol": {
                "type": "number",
                "desc": "The tolerance for the elastic net solver used to calculate the descentdirection. This parameter controls the accuracy of the search directionfor a given column update, not of the overall parameter estimate. Onlyused for mode='cd'. Range is (0, inf].",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "mode": {
                "type": "option(cd, lars)",
                "desc": "The Lasso solver to use: coordinate descent or LARS. Use LARS forvery sparse underlying graphs, where number of features is greaterthan number of samples. Elsewhere prefer cd which is more numericallystable.",
                "default": "'cd'",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "number of jobs to run in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionchanged:: v0.20`n_jobs` default changed from 1 to None",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "If verbose is True, the objective function and duality gap areprinted at each iteration.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, data are not centered before computation.Useful when working with data whose mean is almost, but not exactlyzero.If False, data are centered before computation.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.GraphicalLassoCV"
        }
    },
    "sklearn.covariance.LedoitWolf": {
        "cls": "Block",
        "typename": "LedoitWolf",
        "desc": "LedoitWolf Estimator  Ledoit-Wolf is a particular form of shrinkage, where the shrinkage coefficient is computed using O. Ledoit and M. Wolf's formula as described in \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\", Ledoit and Wolf, Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411.  Read more in the :ref:`User Guide <shrunk_covariance>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "store_precision": {
                "type": "boolean",
                "desc": "Specify if the estimated precision is stored.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, data will not be centered before computation.Useful when working with data whose mean is almost, but not exactlyzero.If False (default), data will be centered before computation.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "block_size": {
                "type": "number",
                "desc": "Size of blocks into which the covariance matrix will be splitduring its Ledoit-Wolf estimation. This is purely a memoryoptimization and does not affect results.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.LedoitWolf"
        }
    },
    "sklearn.covariance.MinCovDet": {
        "cls": "Block",
        "typename": "MinCovDet",
        "desc": "Minimum Covariance Determinant (MCD): robust estimator of covariance.  The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data, but could still be relevant on data drawn from a unimodal, symmetric distribution. It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets.  Read more in the :ref:`User Guide <robust_covariance>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "store_precision": {
                "type": "boolean",
                "desc": "Specify if the estimated precision is stored.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, the support of the robust location and the covarianceestimates is computed, and a covariance estimate is recomputed fromit, without centering the data.Useful to work with data whose mean is significantly equal tozero but is not exactly zero.If False, the robust location and covariance are directly computedwith the FastMCD algorithm without additional treatment.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "support_fraction": {
                "type": "number",
                "desc": "The proportion of points to be included in the support of the rawMCD estimate. Default is None, which implies that the minimumvalue of support_fraction will be used within the algorithm:`(n_sample + n_features + 1) / 2`. The parameter must be in the range(0, 1).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines the pseudo random number generator for shuffling the data.Pass an int for reproducible results across multiple function calls.See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.MinCovDet"
        }
    },
    "sklearn.covariance.OAS": {
        "cls": "Block",
        "typename": "OAS",
        "desc": "Oracle Approximating Shrinkage Estimator  Read more in the :ref:`User Guide <shrunk_covariance>`.  OAS is a particular form of shrinkage described in \"Shrinkage Algorithms for MMSE Covariance Estimation\" Chen et al., IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010.  The formula used here does not correspond to the one given in the article. In the original article, formula (23) states that 2/p is multiplied by Trace(cov*cov) in both the numerator and denominator, but this operation is omitted because for a large p, the value of 2/p is so small that it doesn't affect the value of the estimator.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "store_precision": {
                "type": "boolean",
                "desc": "Specify if the estimated precision is stored.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, data will not be centered before computation.Useful when working with data whose mean is almost, but not exactlyzero.If False (default), data will be centered before computation.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.OAS"
        }
    },
    "sklearn.covariance.ShrunkCovariance": {
        "cls": "Block",
        "typename": "ShrunkCovariance",
        "desc": "Covariance estimator with shrinkage  Read more in the :ref:`User Guide <shrunk_covariance>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.covariance",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "store_precision": {
                "type": "boolean",
                "desc": "Specify if the estimated precision is stored",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "assume_centered": {
                "type": "boolean",
                "desc": "If True, data will not be centered before computation.Useful when working with data whose mean is almost, but not exactlyzero.If False, data will be centered before computation.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "shrinkage": {
                "type": "number",
                "desc": "Coefficient in the convex combination used for the computationof the shrunk estimate. Range is [0, 1].",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.covariance.ShrunkCovariance"
        }
    },
    "sklearn.cross_decomposition.CCA": {
        "cls": "Block",
        "typename": "CCA",
        "desc": "Canonical Correlation Analysis, also known as \"Mode B\" PLS.  Read more in the :ref:`User Guide <cross_decomposition>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cross_decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components to keep. Should be in `[1, min(n_samples,n_features, n_targets)]`.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "scale": {
                "type": "boolean",
                "desc": "Whether to scale `X` and `Y`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "the maximum number of iterations of the power method.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance used as convergence criteria in the power method: thealgorithm stops whenever the squared norm of `u_i - u_{i-1}` is lessthan `tol`, where `u` corresponds to the left singular vector.",
                "default": "1e-06",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Whether to copy `X` and `Y` in fit before applying centering, andpotentially scaling. If False, these operations will be done inplace,modifying both arrays.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cross_decomposition.CCA"
        }
    },
    "sklearn.cross_decomposition.PLSCanonical": {
        "cls": "Block",
        "typename": "PLSCanonical",
        "desc": "Partial Least Squares transformer and regressor.  Read more in the :ref:`User Guide <cross_decomposition>`.  .. versionadded:: 0.8",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cross_decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components to keep. Should be in `[1, min(n_samples,n_features, n_targets)]`.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "scale": {
                "type": "boolean",
                "desc": "Whether to scale `X` and `Y`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(nipals, svd)",
                "desc": "The algorithm used to estimate the first singular vectors of thecross-covariance matrix. 'nipals' uses the power method while 'svd'will compute the whole SVD.",
                "default": "'nipals'",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "the maximum number of iterations of the power method when`algorithm='nipals'`. Ignored otherwise.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance used as convergence criteria in the power method: thealgorithm stops whenever the squared norm of `u_i - u_{i-1}` is lessthan `tol`, where `u` corresponds to the left singular vector.",
                "default": "1e-06",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Whether to copy `X` and `Y` in fit before applying centering, andpotentially scaling. If False, these operations will be done inplace,modifying both arrays.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cross_decomposition.PLSCanonical"
        }
    },
    "sklearn.cross_decomposition.PLSRegression": {
        "cls": "Block",
        "typename": "PLSRegression",
        "desc": "PLS regression  PLSRegression is also known as PLS2 or PLS1, depending on the number of targets.  Read more in the :ref:`User Guide <cross_decomposition>`.  .. versionadded:: 0.8",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cross_decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components to keep. Should be in `[1, min(n_samples,n_features, n_targets)]`.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "scale": {
                "type": "boolean",
                "desc": "Whether to scale `X` and `Y`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations of the power method when`algorithm='nipals'`. Ignored otherwise.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance used as convergence criteria in the power method: thealgorithm stops whenever the squared norm of `u_i - u_{i-1}` is lessthan `tol`, where `u` corresponds to the left singular vector.",
                "default": "1e-06",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Whether to copy `X` and `Y` in fit before applying centering, andpotentially scaling. If False, these operations will be done inplace,modifying both arrays.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cross_decomposition.PLSRegression"
        }
    },
    "sklearn.cross_decomposition.PLSSVD": {
        "cls": "Block",
        "typename": "PLSSVD",
        "desc": "Partial Least Square SVD.  This transformer simply performs a SVD on the crosscovariance matrix X'Y. It is able to project both the training data `X` and the targets `Y`. The training data X is projected on the left singular vectors, while the targets are projected on the right singular vectors.  Read more in the :ref:`User Guide <cross_decomposition>`.  .. versionadded:: 0.8",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.cross_decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "The number of components to keep. Should be in `[1,min(n_samples, n_features, n_targets)]`.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "scale": {
                "type": "boolean",
                "desc": "Whether to scale `X` and `Y`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Whether to copy `X` and `Y` in fit before applying centering, andpotentially scaling. If False, these operations will be done inplace,modifying both arrays.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.cross_decomposition.PLSSVD"
        }
    },
    "sklearn.decomposition.DictionaryLearning": {
        "cls": "Block",
        "typename": "DictionaryLearning",
        "desc": "Dictionary learning  Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.  Solves the optimization problem::      (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1                 (U,V)                 with || V_k ||_2 = 1 for all  0 <= k < n_components  Read more in the :ref:`User Guide <DictionaryLearning>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of dictionary elements to extract.",
                "default": "n_features",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Sparsity controlling parameter.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for numerical error.",
                "default": "1e-8",
                "dictKeyOf": "initkargs"
            },
            "fit_algorithm": {
                "type": "option(lars, cd)",
                "desc": "* `'lars'`: uses the least angle regression method to solve the lassoproblem (:func:`~sklearn.linear_model.lars_path`);* `'cd'`: uses the coordinate descent method to compute theLasso solution (:class:`~sklearn.linear_model.Lasso`). Lars will befaster if the estimated components are sparse... versionadded:: 0.17*cd* coordinate descent method to improve speed.",
                "default": "'lars'",
                "dictKeyOf": "initkargs"
            },
            "transform_algorithm": {
                "type": "option(lasso_lars, lasso_cd, lars, omp,             threshold)",
                "desc": "Algorithm used to transform the data:- `'lars'`: uses the least angle regression method(:func:`~sklearn.linear_model.lars_path`);- `'lasso_lars'`: uses Lars to compute the Lasso solution.- `'lasso_cd'`: uses the coordinate descent method to compute theLasso solution (:class:`~sklearn.linear_model.Lasso`). `'lasso_lars'`will be faster if the estimated components are sparse.- `'omp'`: uses orthogonal matching pursuit to estimate the sparsesolution.- `'threshold'`: squashes to zero all coefficients less than alpha fromthe projection ``dictionary * X'``... versionadded:: 0.17*lasso_cd* coordinate descent method to improve speed.",
                "default": "'omp'",
                "dictKeyOf": "initkargs"
            },
            "transform_n_nonzero_coefs": {
                "type": "number",
                "desc": "Number of nonzero coefficients to target in each column of thesolution. This is only used by `algorithm='lars'` and `algorithm='omp'`and is overridden by `alpha` in the `omp` case. If `None`, then`transform_n_nonzero_coefs=int(n_features / 10)`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "transform_alpha": {
                "type": "number",
                "desc": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is thepenalty applied to the L1 norm.If `algorithm='threshold'`, `alpha` is the absolute value of thethreshold below which coefficients will be squashed to zero.If `algorithm='omp'`, `alpha` is the tolerance parameter: the value ofthe reconstruction error targeted. In this case, it overrides`n_nonzero_coefs`.If `None`, default to 1.0",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "code_init": {
                "type": "ndarray of shape (n_samples, n_components), default=None",
                "desc": "Initial value for the code, for warm restart. Only used if `code_init`and `dict_init` are not None.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "dict_init": {
                "type": "ndarray of shape (n_components, n_features), default=None",
                "desc": "Initial values for the dictionary, for warm restart. Only used if`code_init` and `dict_init` are not None.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "To control the verbosity of the procedure.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "split_sign": {
                "type": "boolean",
                "desc": "Whether to split the sparse feature vector into the concatenation ofits negative part and its positive part. This can improve theperformance of downstream classifiers.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for initializing the dictionary when ``dict_init`` is notspecified, randomly shuffling the data when ``shuffle`` is set to``True``, and updating the dictionary. Pass an int for reproducibleresults across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "positive_code": {
                "type": "boolean",
                "desc": "Whether to enforce positivity when finding the code... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "positive_dict": {
                "type": "boolean",
                "desc": "Whether to enforce positivity when finding the dictionary.. versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "transform_max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform if `algorithm='lasso_cd'` or`'lasso_lars'`... versionadded:: 0.22",
                "default": "1000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.DictionaryLearning"
        }
    },
    "sklearn.decomposition.FactorAnalysis": {
        "cls": "Block",
        "typename": "FactorAnalysis",
        "desc": "Factor Analysis (FA).  A simple linear generative model with Gaussian latent variables.  The observations are assumed to be caused by a linear transformation of lower dimensional latent factors and added Gaussian noise. Without loss of generality the factors are distributed according to a Gaussian with zero mean and unit covariance. The noise is also zero mean and has an arbitrary diagonal covariance matrix.  If we would restrict the model further, by assuming that the Gaussian noise is even isotropic (all diagonal entries are the same) we would obtain :class:`PPCA`.  FactorAnalysis performs a maximum likelihood estimate of the so-called `loading` matrix, the transformation of the latent variables to the observed ones, using SVD based approach.  Read more in the :ref:`User Guide <FA>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Dimensionality of latent space, the number of componentsof ``X`` that are obtained after ``transform``.If None, n_components is set to the number of features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Stopping tolerance for log-likelihood increase.",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Whether to make a copy of X. If ``False``, the input X gets overwrittenduring fitting.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "noise_variance_init": {
                "type": "ndarray of shape (n_features,), default=None",
                "desc": "The initial guess of the noise variance for each feature.If None, it defaults to np.ones(n_features).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "svd_method": {
                "type": "option(lapack, randomized)",
                "desc": "Which SVD method to use. If 'lapack' use standard SVD fromscipy.linalg, if 'randomized' use fast ``randomized_svd`` function.Defaults to 'randomized'. For most applications 'randomized' willbe sufficiently precise while providing significant speed gains.Accuracy can also be improved by setting higher values for`iterated_power`. If this is not sufficient, for maximum precisionyou should choose 'lapack'.",
                "default": "'randomized'",
                "dictKeyOf": "initkargs"
            },
            "iterated_power": {
                "type": "number",
                "desc": "Number of iterations for the power method. 3 by default. Only usedif ``svd_method`` equals 'randomized'.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "rotation": {
                "type": "option(varimax, quartimax)",
                "desc": "If not None, apply the indicated rotation. Currently, varimax andquartimax are implemented. See`\"The varimax criterion for analytic rotation in factor analysis\"<https://link.springer.com/article/10.1007%2FBF02289233>`_H. F. Kaiser, 1958... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Only used when ``svd_method`` equals 'randomized'. Pass an int forreproducible results across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.FactorAnalysis"
        }
    },
    "sklearn.decomposition.FastICA": {
        "cls": "Block",
        "typename": "FastICA",
        "desc": "FastICA: a fast algorithm for Independent Component Analysis.  Read more in the :ref:`User Guide <ICA>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components to use. If None is passed, all are used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(parallel, deflation)",
                "desc": "Apply parallel or deflational algorithm for FastICA.",
                "default": "'parallel'",
                "dictKeyOf": "initkargs"
            },
            "whiten": {
                "type": "boolean",
                "desc": "If whiten is false, the data is already considered to bewhitened, and no whitening is performed.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "fun": {
                "type": "option(logcosh, exp, cube)",
                "desc": "The functional form of the G function used in theapproximation to neg-entropy. Could be either 'logcosh', 'exp',or 'cube'.You can also provide your own function. It should return a tuplecontaining the value of the function, and of its derivative, in thepoint. Example::def my_g(x):return x ** 3, (3 * x ** 2).mean(axis=-1)",
                "default": "'logcosh'",
                "dictKeyOf": "initkargs"
            },
            "fun_args": {
                "type": "dict, default=None",
                "desc": "Arguments to send to the functional form.If empty and if fun='logcosh', fun_args will take value{'alpha' : 1.0}.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations during fit.",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance on update at each iteration.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "w_init": {
                "type": "ndarray of shape (n_components, n_components), default=None",
                "desc": "The mixing matrix to be used to initialize the algorithm.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used to initialize ``w_init`` when not specified, with anormal distribution. Pass an int, for reproducible resultsacross multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.FastICA"
        }
    },
    "sklearn.decomposition.IncrementalPCA": {
        "cls": "Block",
        "typename": "IncrementalPCA",
        "desc": "Incremental principal components analysis (IPCA).  Linear dimensionality reduction using Singular Value Decomposition of the data, keeping only the most significant singular vectors to project the data to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.  Depending on the size of the input data, this algorithm can be much more memory efficient than a PCA, and allows sparse input.  This algorithm has constant memory complexity, on the order of ``batch_size * n_features``, enabling use of np.memmap files without loading the entire file into memory. For sparse matrices, the input is converted to dense in batches (in order to be able to subtract the mean) which avoids storing the entire dense matrix at any one time.  The computational overhead of each SVD is ``O(batch_size * n_features ** 2)``, but only 2 * batch_size samples remain in memory at a time. There will be ``n_samples / batch_size`` SVD computations to get the principal components, versus 1 large SVD of complexity ``O(n_samples * n_features ** 2)`` for PCA.  Read more in the :ref:`User Guide <IncrementalPCA>`.  .. versionadded:: 0.16",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components to keep. If ``n_components`` is ``None``,then ``n_components`` is set to ``min(n_samples, n_features)``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "whiten": {
                "type": "boolean",
                "desc": "When True (False by default) the ``components_`` vectors are dividedby ``n_samples`` times ``components_`` to ensure uncorrelated outputswith unit component-wise variances.Whitening will remove some information from the transformed signal(the relative variance scales of the components) but can sometimesimprove the predictive accuracy of the downstream estimators bymaking data respect some hard-wired assumptions.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "If False, X will be overwritten. ``copy=False`` can be used tosave memory but is unsafe for general use.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "The number of samples to use for each batch. Only used when calling``fit``. If ``batch_size`` is ``None``, then ``batch_size``is inferred from the data and set to ``5 * n_features``, to provide abalance between approximation accuracy and memory consumption.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.IncrementalPCA"
        }
    },
    "sklearn.decomposition.KernelPCA": {
        "cls": "Block",
        "typename": "KernelPCA",
        "desc": "Kernel Principal component analysis (KPCA).  Non-linear dimensionality reduction through the use of kernels (see :ref:`metrics`).  Read more in the :ref:`User Guide <kernel_PCA>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components. If None, all non-zero components are kept.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "kernel": {
                "type": "option(linear, poly,             rbf, sigmoid, cosine, precomputed)",
                "desc": "Kernel used for PCA.",
                "default": "'linear'",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "number",
                "desc": "Kernel coefficient for rbf, poly and sigmoid kernels. Ignored by otherkernels. If ``gamma`` is ``None``, then it is set to ``1/n_features``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree for poly kernels. Ignored by other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Independent term in poly and sigmoid kernels.Ignored by other kernels.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "kernel_params": {
                "type": "dict, default=None",
                "desc": "Parameters (keyword arguments) andvalues for kernel passed as callable object.Ignored by other kernels.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Hyperparameter of the ridge regression that learns theinverse transform (when fit_inverse_transform=True).",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_inverse_transform": {
                "type": "boolean",
                "desc": "Learn the inverse transform for non-precomputed kernels.(i.e. learn to find the pre-image of a point)",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "eigen_solver": {
                "type": "option(auto, dense, arpack)",
                "desc": "Select eigensolver to use. If n_components is much less thanthe number of training samples, arpack may be more efficientthan the dense eigensolver.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Convergence tolerance for arpack.If 0, optimal value will be chosen by arpack.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations for arpack.If None, optimal value will be chosen by arpack.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "remove_zero_eig": {
                "type": "boolean",
                "desc": "If True, then all components with zero eigenvalues are removed, sothat the number of components in the output may be < n_components(and sometimes even zero due to numerical instability).When n_components is None, this parameter is ignored and componentswith zero eigenvalues are removed regardless.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used when ``eigen_solver`` == 'arpack'. Pass an int for reproducibleresults across multiple function calls.See :term:`Glossary <random_state>`... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, input X is copied and stored by the model in the `X_fit_`attribute. If no further changes will be done to X, setting`copy_X=False` saves memory by storing a reference... versionadded:: 0.18",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.KernelPCA"
        }
    },
    "sklearn.decomposition.LatentDirichletAllocation": {
        "cls": "Block",
        "typename": "LatentDirichletAllocation",
        "desc": "Latent Dirichlet Allocation with online variational Bayes algorithm  .. versionadded:: 0.17  Read more in the :ref:`User Guide <LatentDirichletAllocation>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of topics... versionchanged:: 0.19``n_topics`` was renamed to ``n_components``",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "doc_topic_prior": {
                "type": "number",
                "desc": "Prior of document topic distribution `theta`. If the value is None,defaults to `1 / n_components`.In [1]_, this is called `alpha`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "topic_word_prior": {
                "type": "number",
                "desc": "Prior of topic word distribution `beta`. If the value is None, defaultsto `1 / n_components`.In [1]_, this is called `eta`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "learning_method": {
                "type": "option(batch, online)",
                "desc": "Method used to update `_component`. Only used in :meth:`fit` method.In general, if the data size is large, the online update will be muchfaster than the batch update.Valid options::'batch': Batch variational Bayes method. Use all training data ineach EM update.Old `components_` will be overwritten in each iteration.'online': Online variational Bayes method. In each EM update, usemini-batch of training data to update the ``components_``variable incrementally. The learning rate is controlled by the``learning_decay`` and the ``learning_offset`` parameters... versionchanged:: 0.20The default learning method is now ``\"batch\"``.",
                "default": "'batch'",
                "dictKeyOf": "initkargs"
            },
            "learning_decay": {
                "type": "number",
                "desc": "It is a parameter that control learning rate in the online learningmethod. The value should be set between (0.5, 1.0] to guaranteeasymptotic convergence. When the value is 0.0 and batch_size is``n_samples``, the update method is same as batch learning. In theliterature, this is called kappa.",
                "default": "0.7",
                "dictKeyOf": "initkargs"
            },
            "learning_offset": {
                "type": "number",
                "desc": "A (positive) parameter that downweights early iterations in onlinelearning. It should be greater than 1.0. In the literature, this iscalled tau_0.",
                "default": "10.",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "Number of documents to use in each EM iteration. Only used in onlinelearning.",
                "default": "128",
                "dictKeyOf": "initkargs"
            },
            "evaluate_every": {
                "type": "number",
                "desc": "How often to evaluate perplexity. Only used in `fit` method.set it to 0 or negative number to not evaluate perplexity intraining at all. Evaluating perplexity can help you check convergencein training process, but it will also increase total training time.Evaluating perplexity in every iteration might increase training timeup to two-fold.",
                "default": "-1",
                "dictKeyOf": "initkargs"
            },
            "total_samples": {
                "type": "number",
                "desc": "Total number of documents. Only used in the :meth:`partial_fit` method.",
                "default": "1e6",
                "dictKeyOf": "initkargs"
            },
            "perp_tol": {
                "type": "number",
                "desc": "Perplexity tolerance in batch learning. Only used when``evaluate_every`` is greater than 0.",
                "default": "1e-1",
                "dictKeyOf": "initkargs"
            },
            "mean_change_tol": {
                "type": "number",
                "desc": "Stopping tolerance for updating document topic distribution in E-step.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "max_doc_update_iter": {
                "type": "number",
                "desc": "Max number of iterations for updating document topic distribution inthe E-step.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use in the E-step.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Verbosity level.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Pass an int for reproducible results across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.LatentDirichletAllocation"
        }
    },
    "sklearn.decomposition.MiniBatchDictionaryLearning": {
        "cls": "Block",
        "typename": "MiniBatchDictionaryLearning",
        "desc": "Mini-batch dictionary learning  Finds a dictionary (a set of atoms) that can best be used to represent data using a sparse code.  Solves the optimization problem::     (U^*,V^*) = argmin 0.5 || X - U V ||_2^2 + alpha * || U ||_1                 (U,V)                 with || V_k ||_2 = 1 for all  0 <= k < n_components  Read more in the :ref:`User Guide <DictionaryLearning>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of dictionary elements to extract.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Sparsity controlling parameter.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "n_iter": {
                "type": "number",
                "desc": "Total number of iterations to perform.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "fit_algorithm": {
                "type": "option(lars, cd)",
                "desc": "The algorithm used:- `'lars'`: uses the least angle regression method to solve the lassoproblem (`linear_model.lars_path`)- `'cd'`: uses the coordinate descent method to compute theLasso solution (`linear_model.Lasso`). Lars will be faster ifthe estimated components are sparse.",
                "default": "'lars'",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "Number of samples in each mini-batch.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether to shuffle the samples before forming batches.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "dict_init": {
                "type": "ndarray of shape (n_components, n_features), default=None",
                "desc": "initial value of the dictionary for warm restart scenarios",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "transform_algorithm": {
                "type": "option(lasso_lars, lasso_cd, lars, omp,             threshold)",
                "desc": "Algorithm used to transform the data:- `'lars'`: uses the least angle regression method(`linear_model.lars_path`);- `'lasso_lars'`: uses Lars to compute the Lasso solution.- `'lasso_cd'`: uses the coordinate descent method to compute theLasso solution (`linear_model.Lasso`). `'lasso_lars'` will be fasterif the estimated components are sparse.- `'omp'`: uses orthogonal matching pursuit to estimate the sparsesolution.- `'threshold'`: squashes to zero all coefficients less than alpha fromthe projection ``dictionary * X'``.",
                "default": "'omp'",
                "dictKeyOf": "initkargs"
            },
            "transform_n_nonzero_coefs": {
                "type": "number",
                "desc": "Number of nonzero coefficients to target in each column of thesolution. This is only used by `algorithm='lars'` and `algorithm='omp'`and is overridden by `alpha` in the `omp` case. If `None`, then`transform_n_nonzero_coefs=int(n_features / 10)`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "transform_alpha": {
                "type": "number",
                "desc": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is thepenalty applied to the L1 norm.If `algorithm='threshold'`, `alpha` is the absolute value of thethreshold below which coefficients will be squashed to zero.If `algorithm='omp'`, `alpha` is the tolerance parameter: the value ofthe reconstruction error targeted. In this case, it overrides`n_nonzero_coefs`.If `None`, default to 1.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "To control the verbosity of the procedure.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "split_sign": {
                "type": "boolean",
                "desc": "Whether to split the sparse feature vector into the concatenation ofits negative part and its positive part. This can improve theperformance of downstream classifiers.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for initializing the dictionary when ``dict_init`` is notspecified, randomly shuffling the data when ``shuffle`` is set to``True``, and updating the dictionary. Pass an int for reproducibleresults across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "positive_code": {
                "type": "boolean",
                "desc": "Whether to enforce positivity when finding the code... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "positive_dict": {
                "type": "boolean",
                "desc": "Whether to enforce positivity when finding the dictionary... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "transform_max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform if `algorithm='lasso_cd'` or`'lasso_lars'`... versionadded:: 0.22",
                "default": "1000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.MiniBatchDictionaryLearning"
        }
    },
    "sklearn.decomposition.MiniBatchSparsePCA": {
        "cls": "Block",
        "typename": "MiniBatchSparsePCA",
        "desc": "Mini-batch Sparse Principal Components Analysis  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  Read more in the :ref:`User Guide <SparsePCA>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "number of sparse atoms to extract",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Sparsity controlling parameter. Higher values lead to sparsercomponents.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "ridge_alpha": {
                "type": "number",
                "desc": "Amount of ridge shrinkage to apply in order to improveconditioning when calling the transform method.",
                "default": "0.01",
                "dictKeyOf": "initkargs"
            },
            "n_iter": {
                "type": "number",
                "desc": "number of iterations to perform for each mini batch",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "callback": {
                "type": "callable, default=None",
                "desc": "callable that gets invoked every five iterations",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "the number of features to take in each mini batch",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity; the higher, the more messages. Defaults to 0.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "whether to shuffle the data before splitting it in batches",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "method": {
                "type": "option(lars, cd)",
                "desc": "lars: uses the least angle regression method to solve the lasso problem(linear_model.lars_path)cd: uses the coordinate descent method to compute theLasso solution (linear_model.Lasso). Lars will be faster ifthe estimated components are sparse.",
                "default": "'lars'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for random shuffling when ``shuffle`` is set to ``True``,during online dictionary learning. Pass an int for reproducible resultsacross multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.MiniBatchSparsePCA"
        }
    },
    "sklearn.decomposition.NMF": {
        "cls": "Block",
        "typename": "NMF",
        "desc": "Non-Negative Matrix Factorization (NMF).  Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction.  The objective function is:      .. math::          0.5 * ||X - WH||_{loss}^2 + alpha * l1_{ratio} * ||vec(W)||_1          + alpha * l1_{ratio} * ||vec(H)||_1          + 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2          + 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2  Where:  :math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)  :math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)  The generic norm :math:`||X - WH||_{loss}` may represent the Frobenius norm or another supported beta-divergence loss. The choice between options is controlled by the `beta_loss` parameter.  The objective function is minimized with an alternating minimization of W and H.  Read more in the :ref:`User Guide <NMF>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components, if n_components is not set all featuresare kept.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "option(random, nndsvd, nndsvda, nndsvdar, custom)",
                "desc": "Method used to initialize the procedure.Default: None.Valid options:- `None`: 'nndsvd' if n_components <= min(n_samples, n_features),otherwise random.- `'random'`: non-negative random matrices, scaled with:sqrt(X.mean() / n_components)- `'nndsvd'`: Nonnegative Double Singular Value Decomposition (NNDSVD)initialization (better for sparseness)- `'nndsvda'`: NNDSVD with zeros filled with the average of X(better when sparsity is not desired)- `'nndsvdar'` NNDSVD with zeros filled with small random values(generally faster, less accurate alternative to NNDSVDafor when sparsity is not desired)- `'custom'`: use custom matrices W and H",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "solver": {
                "type": "option(cd, mu)",
                "desc": "Numerical solver to use:'cd' is a Coordinate Descent solver.'mu' is a Multiplicative Update solver... versionadded:: 0.17Coordinate Descent solver... versionadded:: 0.19Multiplicative Update solver.",
                "default": "'cd'",
                "dictKeyOf": "initkargs"
            },
            "beta_loss": {
                "type": "number",
                "desc": "Beta divergence to be minimized, measuring the distance between Xand the dot product WH. Note that values different from 'frobenius'(or 2) and 'kullback-leibler' (or 1) lead to significantly slowerfits. Note that for beta_loss <= 0 (or 'itakura-saito'), the inputmatrix X cannot contain zeros. Used only in 'mu' solver... versionadded:: 0.19",
                "default": "'frobenius'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance of the stopping condition.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations before timing out.",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for initialisation (when ``init`` == 'nndsvdar' or'random'), and in Coordinate Descent. Pass an int for reproducibleresults across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the regularization terms. Set it to zero tohave no regularization... versionadded:: 0.17*alpha* used in the Coordinate Descent solver.",
                "default": "0.",
                "dictKeyOf": "initkargs"
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The regularization mixing parameter, with 0 <= l1_ratio <= 1.For l1_ratio = 0 the penalty is an elementwise L2 penalty(aka Frobenius Norm).For l1_ratio = 1 it is an elementwise L1 penalty.For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2... versionadded:: 0.17Regularization parameter *l1_ratio* used in the Coordinate Descentsolver.",
                "default": "0.",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Whether to be verbose.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "If true, randomize the order of coordinates in the CD solver... versionadded:: 0.17*shuffle* parameter used in the Coordinate Descent solver.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "regularization": {
                "type": "option(both, components, transformation, None)",
                "desc": "Select whether the regularization affects the components (H), thetransformation (W), both or none of them... versionadded:: 0.24",
                "default": "'both'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.NMF"
        }
    },
    "sklearn.decomposition.PCA": {
        "cls": "Block",
        "typename": "PCA",
        "desc": "Principal component analysis (PCA).  Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.  It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of Halko et al. 2009, depending on the shape of the input data and the number of components to extract.  It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.  Notice that this class does not support sparse input. See :class:`TruncatedSVD` for an alternative with sparse data.  Read more in the :ref:`User Guide <PCA>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components to keep.if n_components is not set all components are kept::n_components == min(n_samples, n_features)If ``n_components == 'mle'`` and ``svd_solver == 'full'``, Minka'sMLE is used to guess the dimension. Use of ``n_components == 'mle'``will interpret ``svd_solver == 'auto'`` as ``svd_solver == 'full'``.If ``0 < n_components < 1`` and ``svd_solver == 'full'``, select thenumber of components such that the amount of variance that needs to beexplained is greater than the percentage specified by n_components.If ``svd_solver == 'arpack'``, the number of components must bestrictly less than the minimum of n_features and n_samples.Hence, the None case results in::n_components == min(n_samples, n_features) - 1",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "If False, data passed to fit are overwritten and runningfit(X).transform(X) will not yield the expected results,use fit_transform(X) instead.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "whiten": {
                "type": "boolean",
                "desc": "When True (False by default) the `components_` vectors are multipliedby the square root of n_samples and then divided by the singular valuesto ensure uncorrelated outputs with unit component-wise variances.Whitening will remove some information from the transformed signal(the relative variance scales of the components) but can sometimeimprove the predictive accuracy of the downstream estimators bymaking their data respect some hard-wired assumptions.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "svd_solver": {
                "type": "option(auto, full, arpack, randomized)",
                "desc": "If auto :The solver is selected by a default policy based on `X.shape` and`n_components`: if the input data is larger than 500x500 and thenumber of components to extract is lower than 80% of the smallestdimension of the data, then the more efficient 'randomized'method is enabled. Otherwise the exact full SVD is computed andoptionally truncated afterwards.If full :run exact full SVD calling the standard LAPACK solver via`scipy.linalg.svd` and select the components by postprocessingIf arpack :run SVD truncated to n_components calling ARPACK solver via`scipy.sparse.linalg.svds`. It requires strictly0 < n_components < min(X.shape)If randomized :run randomized SVD by the method of Halko et al... versionadded:: 0.18.0",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for singular values computed by svd_solver == 'arpack'.Must be of range [0.0, infinity)... versionadded:: 0.18.0",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "iterated_power": {
                "type": "number",
                "desc": "Number of iterations for the power method computed bysvd_solver == 'randomized'.Must be of range [0, infinity)... versionadded:: 0.18.0",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used when the 'arpack' or 'randomized' solvers are used. Pass an intfor reproducible results across multiple function calls.See :term:`Glossary <random_state>`... versionadded:: 0.18.0",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.PCA"
        }
    },
    "sklearn.decomposition.SparseCoder": {
        "cls": "Block",
        "typename": "SparseCoder",
        "desc": "Sparse coding  Finds a sparse representation of data against a fixed, precomputed dictionary.  Each row of the result is the solution to a sparse coding problem. The goal is to find a sparse array `code` such that::      X ~= code * dictionary  Read more in the :ref:`User Guide <SparseCoder>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "dictionary": {
                "type": "ndarray of shape (n_components, n_features)",
                "desc": "The dictionary atoms used for sparse coding. Lines are assumed to benormalized to unit norm.",
                "dictKeyOf": "initkargs"
            },
            "transform_algorithm": {
                "type": "option(lasso_lars, lasso_cd, lars, omp,             threshold)",
                "desc": "Algorithm used to transform the data:- `'lars'`: uses the least angle regression method(`linear_model.lars_path`);- `'lasso_lars'`: uses Lars to compute the Lasso solution;- `'lasso_cd'`: uses the coordinate descent method to compute theLasso solution (linear_model.Lasso). `'lasso_lars'` will be faster ifthe estimated components are sparse;- `'omp'`: uses orthogonal matching pursuit to estimate the sparsesolution;- `'threshold'`: squashes to zero all coefficients less than alpha fromthe projection ``dictionary * X'``.",
                "default": "'omp'",
                "dictKeyOf": "initkargs"
            },
            "transform_n_nonzero_coefs": {
                "type": "number",
                "desc": "Number of nonzero coefficients to target in each column of thesolution. This is only used by `algorithm='lars'` and `algorithm='omp'`and is overridden by `alpha` in the `omp` case. If `None`, then`transform_n_nonzero_coefs=int(n_features / 10)`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "transform_alpha": {
                "type": "number",
                "desc": "If `algorithm='lasso_lars'` or `algorithm='lasso_cd'`, `alpha` is thepenalty applied to the L1 norm.If `algorithm='threshold'`, `alpha` is the absolute value of thethreshold below which coefficients will be squashed to zero.If `algorithm='omp'`, `alpha` is the tolerance parameter: the value ofthe reconstruction error targeted. In this case, it overrides`n_nonzero_coefs`.If `None`, default to 1.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "split_sign": {
                "type": "boolean",
                "desc": "Whether to split the sparse feature vector into the concatenation ofits negative part and its positive part. This can improve theperformance of downstream classifiers.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "positive_code": {
                "type": "boolean",
                "desc": "Whether to enforce positivity when finding the code... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "transform_max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform if `algorithm='lasso_cd'` or`lasso_lars`... versionadded:: 0.22",
                "default": "1000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.SparseCoder"
        }
    },
    "sklearn.decomposition.SparsePCA": {
        "cls": "Block",
        "typename": "SparsePCA",
        "desc": "Sparse Principal Components Analysis (SparsePCA).  Finds the set of sparse components that can optimally reconstruct the data.  The amount of sparseness is controllable by the coefficient of the L1 penalty, given by the parameter alpha.  Read more in the :ref:`User Guide <SparsePCA>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of sparse atoms to extract.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Sparsity controlling parameter. Higher values lead to sparsercomponents.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "ridge_alpha": {
                "type": "number",
                "desc": "Amount of ridge shrinkage to apply in order to improveconditioning when calling the transform method.",
                "default": "0.01",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for the stopping condition.",
                "default": "1e-8",
                "dictKeyOf": "initkargs"
            },
            "method": {
                "type": "option(lars, cd)",
                "desc": "lars: uses the least angle regression method to solve the lasso problem(linear_model.lars_path)cd: uses the coordinate descent method to compute theLasso solution (linear_model.Lasso). Lars will be faster ifthe estimated components are sparse.",
                "default": "'lars'",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "U_init": {
                "type": "ndarray of shape (n_samples, n_components), default=None",
                "desc": "Initial values for the loadings for warm restart scenarios. Only usedif `U_init` and `V_init` are not None.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "V_init": {
                "type": "ndarray of shape (n_components, n_features), default=None",
                "desc": "Initial values for the components for warm restart scenarios. Only usedif `U_init` and `V_init` are not None.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity; the higher, the more messages. Defaults to 0.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used during dictionary learning. Pass an int for reproducible resultsacross multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.SparsePCA"
        }
    },
    "sklearn.decomposition.TruncatedSVD": {
        "cls": "Block",
        "typename": "TruncatedSVD",
        "desc": "Dimensionality reduction using truncated SVD (aka LSA).  This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently.  In particular, truncated SVD works on term count/tf-idf matrices as returned by the vectorizers in :mod:`sklearn.feature_extraction.text`. In that context, it is known as latent semantic analysis (LSA).  This estimator supports two algorithms: a fast randomized SVD solver, and a \"naive\" algorithm that uses ARPACK as an eigensolver on `X * X.T` or `X.T * X`, whichever is more efficient.  Read more in the :ref:`User Guide <LSA>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.decomposition",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Desired dimensionality of output data.Must be strictly less than the number of features.The default value is useful for visualisation. For LSA, a value of100 is recommended.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(arpack, randomized)",
                "desc": "SVD solver to use. Either \"arpack\" for the ARPACK wrapper in SciPy(scipy.sparse.linalg.svds), or \"randomized\" for the randomizedalgorithm due to Halko (2009).",
                "default": "'randomized'",
                "dictKeyOf": "initkargs"
            },
            "n_iter": {
                "type": "number",
                "desc": "Number of iterations for randomized SVD solver. Not used by ARPACK. Thedefault is larger than the default in:func:`~sklearn.utils.extmath.randomized_svd` to handle sparsematrices that may have large slowly decaying spectrum.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used during randomized svd. Pass an int for reproducible results acrossmultiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for ARPACK. 0 means machine precision. Ignored by randomizedSVD solver.",
                "default": "0.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.decomposition.TruncatedSVD"
        }
    },
    "sklearn.discriminant_analysis.LinearDiscriminantAnalysis": {
        "cls": "Block",
        "typename": "LinearDiscriminantAnalysis",
        "desc": "Linear Discriminant Analysis  A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.  The fitted model can also be used to reduce the dimensionality of the input by projecting it to the most discriminative directions, using the `transform` method.  .. versionadded:: 0.17    *LinearDiscriminantAnalysis*.  Read more in the :ref:`User Guide <lda_qda>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.discriminant_analysis",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "solver": {
                "type": "option(svd, lsqr, eigen)",
                "desc": "Solver to use, possible values:- 'svd': Singular value decomposition (default).Does not compute the covariance matrix, therefore this solver isrecommended for data with a large number of features.- 'lsqr': Least squares solution.Can be combined with shrinkage or custom covariance estimator.- 'eigen': Eigenvalue decomposition.Can be combined with shrinkage or custom covariance estimator.",
                "default": "'svd'",
                "dictKeyOf": "initkargs"
            },
            "shrinkage": {
                "type": "'auto' or float, default=None",
                "desc": "Shrinkage parameter, possible values:- None: no shrinkage (default).- 'auto': automatic shrinkage using the Ledoit-Wolf lemma.- float between 0 and 1: fixed shrinkage parameter.This should be left to None if `covariance_estimator` is used.Note that shrinkage works only with 'lsqr' and 'eigen' solvers.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "priors": {
                "type": "array-like of shape (n_classes,), default=None",
                "desc": "The class prior probabilities. By default, the class proportions areinferred from the training data.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "Number of components (<= min(n_classes - 1, n_features)) fordimensionality reduction. If None, will be set tomin(n_classes - 1, n_features). This parameter only affects the`transform` method.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "store_covariance": {
                "type": "boolean",
                "desc": "If True, explicitely compute the weighted within-class covariancematrix when solver is 'svd'. The matrix is always computedand stored for the other solvers... versionadded:: 0.17",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Absolute threshold for a singular value of X to be consideredsignificant, used to estimate the rank of X. Dimensions whosesingular values are non-significant are discarded. Only used ifsolver is 'svd'... versionadded:: 0.17",
                "default": "1.0e-4",
                "dictKeyOf": "initkargs"
            },
            "covariance_estimator": {
                "type": "covariance estimator, default=None",
                "desc": "If not None, `covariance_estimator` is used to estimatethe covariance matrices instead of relying on the empiricalcovariance estimator (with potential shrinkage).The object should have a fit method and a ``covariance_`` attributelike the estimators in :mod:`sklearn.covariance`.if None the shrinkage parameter drives the estimate.This should be left to None if `shrinkage` is used.Note that `covariance_estimator` works only with 'lsqr' and 'eigen'solvers... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.discriminant_analysis.LinearDiscriminantAnalysis"
        }
    },
    "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis": {
        "cls": "Block",
        "typename": "QuadraticDiscriminantAnalysis",
        "desc": "Quadratic Discriminant Analysis  A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes' rule.  The model fits a Gaussian density to each class.  .. versionadded:: 0.17    *QuadraticDiscriminantAnalysis*  Read more in the :ref:`User Guide <lda_qda>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.discriminant_analysis",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "priors": {
                "type": "ndarray of shape (n_classes,), default=None",
                "desc": "Class priors. By default, the class proportions are inferred from thetraining data.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "reg_param": {
                "type": "number",
                "desc": "Regularizes the per-class covariance estimates by transforming S2 as``S2 = (1 - reg_param) * S2 + reg_param * np.eye(n_features)``,where S2 corresponds to the `scaling_` attribute of a given class.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "store_covariance": {
                "type": "boolean",
                "desc": "If True, the class covariance matrices are explicitely computed andstored in the `self.covariance_` attribute... versionadded:: 0.17",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Absolute threshold for a singular value to be considered significant,used to estimate the rank of `Xk` where `Xk` is the centered matrixof samples in class k. This parameter does not affect thepredictions. It only controls a warning that is raised when featuresare considered to be colinear... versionadded:: 0.17",
                "default": "1.0e-4",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"
        }
    },
    "sklearn.discriminant_analysis.StandardScaler": {
        "cls": "Block",
        "typename": "StandardScaler",
        "desc": "Standardize features by removing the mean and scaling to unit variance  The standard score of a sample `x` is calculated as:      z = (x - u) / s  where `u` is the mean of the training samples or zero if `with_mean=False`, and `s` is the standard deviation of the training samples or one if `with_std=False`.  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth:`transform`.  Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).  For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.  This scaler can also be applied to sparse CSR or CSC matrices by passing `with_mean=False` to avoid breaking the sparsity structure of the data.  Read more in the :ref:`User Guide <preprocessing_scaler>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.discriminant_analysis",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "copy": {
                "type": "boolean",
                "desc": "If False, try to avoid a copy and do inplace scaling instead.This is not guaranteed to always work inplace; e.g. if the data isnot a NumPy array or scipy.sparse CSR matrix, a copy may still bereturned.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "with_mean": {
                "type": "boolean",
                "desc": "If True, center the data before scaling.This does not work (and will raise an exception) when attempted onsparse matrices, because centering them entails building a densematrix which in common use cases is likely to be too large to fit inmemory.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "with_std": {
                "type": "boolean",
                "desc": "If True, scale the data to unit variance (or equivalently,unit standard deviation).",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.discriminant_analysis.StandardScaler"
        }
    },
    "sklearn.dummy.DummyClassifier": {
        "cls": "Block",
        "typename": "DummyClassifier",
        "desc": "DummyClassifier is a classifier that makes predictions using simple rules.  This classifier is useful as a simple baseline to compare with other (real) classifiers. Do not use it for real problems.  Read more in the :ref:`User Guide <dummy_estimators>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.dummy",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "strategy": {
                "type": "option(\"stratified\", \"most_frequent\", \"prior\", \"uniform\",             \"constant\")",
                "desc": "Strategy to use to generate predictions.* \"stratified\": generates predictions by respecting the trainingset's class distribution.* \"most_frequent\": always predicts the most frequent label in thetraining set.* \"prior\": always predicts the class that maximizes the class prior(like \"most_frequent\") and ``predict_proba`` returns the class prior.* \"uniform\": generates predictions uniformly at random.* \"constant\": always predicts a constant label that is provided bythe user. This is useful for metrics that evaluate a non-majorityclass.. versionchanged:: 0.24The default value of `strategy` has changed to \"prior\" in version0.24.",
                "default": "\"prior\"",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness to generate the predictions when``strategy='stratified'`` or ``strategy='uniform'``.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "constant": {
                "type": "number",
                "desc": "The explicit constant as predicted by the \"constant\" strategy. Thisparameter is useful only for the \"constant\" strategy.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.dummy.DummyClassifier"
        }
    },
    "sklearn.dummy.DummyRegressor": {
        "cls": "Block",
        "typename": "DummyRegressor",
        "desc": "DummyRegressor is a regressor that makes predictions using simple rules.  This regressor is useful as a simple baseline to compare with other (real) regressors. Do not use it for real problems.  Read more in the :ref:`User Guide <dummy_estimators>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.dummy",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "strategy": {
                "type": "option(\"mean\", \"median\", \"quantile\", \"constant\")",
                "desc": "Strategy to use to generate predictions.* \"mean\": always predicts the mean of the training set* \"median\": always predicts the median of the training set* \"quantile\": always predicts a specified quantile of the training set,provided with the quantile parameter.* \"constant\": always predicts a constant value that is provided bythe user.",
                "default": "\"mean\"",
                "dictKeyOf": "initkargs"
            },
            "constant": {
                "type": "number",
                "desc": "The explicit constant as predicted by the \"constant\" strategy. Thisparameter is useful only for the \"constant\" strategy.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "quantile": {
                "type": "number",
                "desc": "The quantile to predict using the \"quantile\" strategy. A quantile of0.5 corresponds to the median, while 0.0 to the minimum and 1.0 to themaximum.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.dummy.DummyRegressor"
        }
    },
    "sklearn.ensemble.AdaBoostClassifier": {
        "cls": "Parent",
        "typename": "AdaBoostClassifier",
        "desc": "An AdaBoost classifier.  An AdaBoost [1] classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases.  This class implements the algorithm known as AdaBoost-SAMME [2].  Read more in the :ref:`User Guide <adaboost>`.  .. versionadded:: 0.14",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The maximum number of estimators at which boosting is terminated.In case of perfect fit, the learning procedure is stopped early.",
                "default": "50",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "number",
                "desc": "Weight applied to each classifier at each boosting iteration. A higherlearning rate increases the contribution of each classifier. There isa trade-off between the `learning_rate` and `n_estimators` parameters.",
                "default": "1.",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(SAMME, SAMME.R)",
                "desc": "If 'SAMME.R' then use the SAMME.R real boosting algorithm.``base_estimator`` must support calculation of class probabilities.If 'SAMME' then use the SAMME discrete boosting algorithm.The SAMME.R algorithm typically converges faster than SAMME,achieving a lower test error with fewer boosting iterations.",
                "default": "'SAMME.R'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random seed given at each `base_estimator` at eachboosting iteration.Thus, it is only used when `base_estimator` exposes a `random_state`.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.AdaBoostClassifier",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.ensemble.AdaBoostRegressor": {
        "cls": "Parent",
        "typename": "AdaBoostRegressor",
        "desc": "An AdaBoost regressor.  An AdaBoost [1] regressor is a meta-estimator that begins by fitting a regressor on the original dataset and then fits additional copies of the regressor on the same dataset but where the weights of instances are adjusted according to the error of the current prediction. As such, subsequent regressors focus more on difficult cases.  This class implements the algorithm known as AdaBoost.R2 [2].  Read more in the :ref:`User Guide <adaboost>`.  .. versionadded:: 0.14",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The maximum number of estimators at which boosting is terminated.In case of perfect fit, the learning procedure is stopped early.",
                "default": "50",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "number",
                "desc": "Weight applied to each classifier at each boosting iteration. A higherlearning rate increases the contribution of each classifier. There isa trade-off between the `learning_rate` and `n_estimators` parameters.",
                "default": "1.",
                "dictKeyOf": "initkargs"
            },
            "loss": {
                "type": "option(linear, square, exponential)",
                "desc": "The loss function to use when updating the weights after eachboosting iteration.",
                "default": "'linear'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random seed given at each `base_estimator` at eachboosting iteration.Thus, it is only used when `base_estimator` exposes a `random_state`.In addition, it controls the bootstrap of the weights used to train the`base_estimator` at each boosting iteration.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.AdaBoostRegressor",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.ensemble.BaggingClassifier": {
        "cls": "Parent",
        "typename": "BaggingClassifier",
        "desc": "A Bagging classifier.  A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]_. If samples are drawn with replacement, then the method is known as Bagging [2]_. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]_. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4]_.  Read more in the :ref:`User Guide <bagging>`.  .. versionadded:: 0.15",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of base estimators in the ensemble.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "max_samples": {
                "type": "number",
                "desc": "The number of samples to draw from X to train each base estimator (withreplacement by default, see `bootstrap` for more details).- If int, then draw `max_samples` samples.- If float, then draw `max_samples * X.shape[0]` samples.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The number of features to draw from X to train each base estimator (without replacement by default, see `bootstrap_features` for moredetails).- If int, then draw `max_features` features.- If float, then draw `max_features * X.shape[1]` features.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "bootstrap": {
                "type": "boolean",
                "desc": "Whether samples are drawn with replacement. If False, samplingwithout replacement is performed.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "bootstrap_features": {
                "type": "boolean",
                "desc": "Whether features are drawn with replacement.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "oob_score": {
                "type": "boolean",
                "desc": "Whether to use out-of-bag samples to estimatethe generalization error. Only available if bootstrap=True.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fita whole new ensemble. See :term:`the Glossary <warm_start>`... versionadded:: 0.17*warm_start* constructor parameter.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel for both :meth:`fit` and:meth:`predict`. ``None`` means 1 unless in a:obj:`joblib.parallel_backend` context. ``-1`` means using allprocessors. See :term:`Glossary <n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random resampling of the original dataset(sample wise and feature wise).If the base estimator accepts a `random_state` attribute, a differentseed is generated for each instance in the ensemble.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity when fitting and predicting.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.BaggingClassifier",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.ensemble.BaggingRegressor": {
        "cls": "Parent",
        "typename": "BaggingRegressor",
        "desc": "A Bagging regressor.  A Bagging regressor is an ensemble meta-estimator that fits base regressors each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.  This algorithm encompasses several works from the literature. When random subsets of the dataset are drawn as random subsets of the samples, then this algorithm is known as Pasting [1]_. If samples are drawn with replacement, then the method is known as Bagging [2]_. When random subsets of the dataset are drawn as random subsets of the features, then the method is known as Random Subspaces [3]_. Finally, when base estimators are built on subsets of both samples and features, then the method is known as Random Patches [4]_.  Read more in the :ref:`User Guide <bagging>`.  .. versionadded:: 0.15",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of base estimators in the ensemble.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "max_samples": {
                "type": "number",
                "desc": "The number of samples to draw from X to train each base estimator (withreplacement by default, see `bootstrap` for more details).- If int, then draw `max_samples` samples.- If float, then draw `max_samples * X.shape[0]` samples.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The number of features to draw from X to train each base estimator (without replacement by default, see `bootstrap_features` for moredetails).- If int, then draw `max_features` features.- If float, then draw `max_features * X.shape[1]` features.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "bootstrap": {
                "type": "boolean",
                "desc": "Whether samples are drawn with replacement. If False, samplingwithout replacement is performed.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "bootstrap_features": {
                "type": "boolean",
                "desc": "Whether features are drawn with replacement.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "oob_score": {
                "type": "boolean",
                "desc": "Whether to use out-of-bag samples to estimatethe generalization error. Only available if bootstrap=True.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fita whole new ensemble. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel for both :meth:`fit` and:meth:`predict`. ``None`` means 1 unless in a:obj:`joblib.parallel_backend` context. ``-1`` means using allprocessors. See :term:`Glossary <n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random resampling of the original dataset(sample wise and feature wise).If the base estimator accepts a `random_state` attribute, a differentseed is generated for each instance in the ensemble.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity when fitting and predicting.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.BaggingRegressor",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.ensemble.BaseEnsemble": {
        "cls": "Parent",
        "typename": "BaseEnsemble",
        "desc": "Base class for all ensemble classes.  Warning: This class should not be used directly. Use derived classes instead.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of estimators in the ensemble.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "estimator_params": {
                "type": "list of str, default=tuple()",
                "desc": "The list of attributes to use as parameters when instantiating anew base estimator. If none are given, default parameters are used.",
                "default": "tuple()",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.BaseEnsemble",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.ensemble.ExtraTreesClassifier": {
        "cls": "Block",
        "typename": "ExtraTreesClassifier",
        "desc": "An extra-trees classifier.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of trees in the forest... versionchanged:: 0.22The default value of ``n_estimators`` changed from 10 to 100in 0.22.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "criterion": {
                "type": "option(\"gini\", \"entropy\")",
                "desc": "The function to measure the quality of a split. Supported criteria are\"gini\" for the Gini impurity and \"entropy\" for the information gain.",
                "default": "\"gini\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "option(\"auto\", \"sqrt\", \"log2\")",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`round(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=sqrt(n_features)`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow trees with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "bootstrap": {
                "type": "boolean",
                "desc": "Whether bootstrap samples are used when building trees. If False, thewhole dataset is used to build each tree.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "oob_score": {
                "type": "boolean",
                "desc": "Whether to use out-of-bag samples to estimate the generalization score.Only available if bootstrap=True.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,:meth:`decision_path` and :meth:`apply` are all parallelized over thetrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`context. ``-1`` means using all processors. See :term:`Glossary<n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls 3 sources of randomness:- the bootstrapping of the samples used when building trees(if ``bootstrap=True``)- the sampling of the features to consider when looking for the bestsplit at each node (if ``max_features < n_features``)- the draw of the splits for each of the `max_features`See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity when fitting and predicting.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "option(\"balanced\", \"balanced_subsample\")",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If not given, all classes are supposed to have weight one. Formulti-output problems, a list of dicts can be provided in the sameorder as the columns of y.Note that for multioutput (including multilabel) weights should bedefined for each class of every column in its own dict. For example,for four-class multilabel classification weights should be[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of[{1:1}, {2:5}, {3:1}, {4:1}].The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``The \"balanced_subsample\" mode is the same as \"balanced\" except thatweights are computed based on the bootstrap sample for every treegrown.For multi-output, the weights of each column of y will be multiplied.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_samples": {
                "type": "number",
                "desc": "If bootstrap is True, the number of samples to draw from Xto train each base estimator.- If None (default), then draw `X.shape[0]` samples.- If int, then draw `max_samples` samples.- If float, then draw `max_samples * X.shape[0]` samples. Thus,`max_samples` should be in the interval `(0, 1)`... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.ExtraTreesClassifier"
        }
    },
    "sklearn.ensemble.ExtraTreesRegressor": {
        "cls": "Block",
        "typename": "ExtraTreesRegressor",
        "desc": "An extra-trees regressor.  This class implements a meta estimator that fits a number of randomized decision trees (a.k.a. extra-trees) on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.  Read more in the :ref:`User Guide <forest>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of trees in the forest... versionchanged:: 0.22The default value of ``n_estimators`` changed from 10 to 100in 0.22.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "criterion": {
                "type": "option(\"mse\", \"mae\")",
                "desc": "The function to measure the quality of a split. Supported criteriaare \"mse\" for the mean squared error, which is equal to variancereduction as feature selection criterion, and \"mae\" for the meanabsolute error... versionadded:: 0.18Mean Absolute Error (MAE) criterion.",
                "default": "\"mse\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "option(\"auto\", \"sqrt\", \"log2\")",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`round(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=n_features`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow trees with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "bootstrap": {
                "type": "boolean",
                "desc": "Whether bootstrap samples are used when building trees. If False, thewhole dataset is used to build each tree.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "oob_score": {
                "type": "boolean",
                "desc": "Whether to use out-of-bag samples to estimate the generalization score.Only available if bootstrap=True.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,:meth:`decision_path` and :meth:`apply` are all parallelized over thetrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`context. ``-1`` means using all processors. See :term:`Glossary<n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls 3 sources of randomness:- the bootstrapping of the samples used when building trees(if ``bootstrap=True``)- the sampling of the features to consider when looking for the bestsplit at each node (if ``max_features < n_features``)- the draw of the splits for each of the `max_features`See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity when fitting and predicting.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_samples": {
                "type": "number",
                "desc": "If bootstrap is True, the number of samples to draw from Xto train each base estimator.- If None (default), then draw `X.shape[0]` samples.- If int, then draw `max_samples` samples.- If float, then draw `max_samples * X.shape[0]` samples. Thus,`max_samples` should be in the interval `(0, 1)`... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.ExtraTreesRegressor"
        }
    },
    "sklearn.ensemble.GradientBoostingClassifier": {
        "cls": "Block",
        "typename": "GradientBoostingClassifier",
        "desc": "Gradient Boosting for classification.  GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage ``n_classes_`` regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.  Read more in the :ref:`User Guide <gradient_boosting>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "loss": {
                "type": "option(deviance, exponential)",
                "desc": "The loss function to be optimized. 'deviance' refers todeviance (= logistic regression) for classificationwith probabilistic outputs. For loss 'exponential' gradientboosting recovers the AdaBoost algorithm.",
                "default": "'deviance'",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "number",
                "desc": "Learning rate shrinks the contribution of each tree by `learning_rate`.There is a trade-off between learning_rate and n_estimators.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of boosting stages to perform. Gradient boostingis fairly robust to over-fitting so a large number usuallyresults in better performance.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "subsample": {
                "type": "number",
                "desc": "The fraction of samples to be used for fitting the individual baselearners. If smaller than 1.0 this results in Stochastic GradientBoosting. `subsample` interacts with the parameter `n_estimators`.Choosing `subsample < 1.0` leads to a reduction of varianceand an increase in bias.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "criterion": {
                "type": "option(friedman_mse, mse, mae)",
                "desc": "The function to measure the quality of a split. Supported criteriaare 'friedman_mse' for the mean squared error with improvementscore by Friedman, 'mse' for mean squared error, and 'mae' forthe mean absolute error. The default value of 'friedman_mse' isgenerally the best as it can provide a better approximation insome cases... versionadded:: 0.18.. deprecated:: 0.24`criterion='mae'` is deprecated and will be removed in version1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or `'mse'`instead, as trees should use a least-square criterion inGradient Boosting.",
                "default": "'friedman_mse'",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the individual regression estimators. The maximumdepth limits the number of nodes in the tree. Tune this parameterfor best performance; the best value depends on the interactionof the input variables.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "estimator or 'zero', default=None",
                "desc": "An estimator object that is used to compute the initial predictions.``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If'zero', the initial raw predictions are set to zero. By default, a``DummyEstimator`` predicting the classes priors is used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random seed given to each Tree estimator at eachboosting iteration.In addition, it controls the random permutation of the features ateach split (see Notes for more details).It also controls the random spliting of the training data to obtain avalidation set if `n_iter_no_change` is not None.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "option(auto, sqrt, log2)",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`int(max_features * n_features)` features are considered at eachsplit.- If 'auto', then `max_features=sqrt(n_features)`.- If 'sqrt', then `max_features=sqrt(n_features)`.- If 'log2', then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Choosing `max_features < n_features` leads to a reduction of varianceand an increase in bias.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Enable verbose output. If 1 then it prints progress and performanceonce in a while (the more trees the lower the frequency). If greaterthan 1 then it prints progress and performance for every tree.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow trees with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just erase theprevious solution. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if ``n_iter_no_change`` is set to an integer... versionadded:: 0.20",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "``n_iter_no_change`` is used to decide if early stopping will be usedto terminate training when validation score is not improving. Bydefault it is set to None to disable early stopping. If set to anumber, it will set aside ``validation_fraction`` size of the trainingdata as validation and terminate training when validation score is notimproving in all of the previous ``n_iter_no_change`` numbers ofiterations. The split is stratified... versionadded:: 0.20",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for the early stopping. When the loss is not improvingby at least tol for ``n_iter_no_change`` iterations (if set to anumber), the training stops... versionadded:: 0.20",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.GradientBoostingClassifier"
        }
    },
    "sklearn.ensemble.GradientBoostingRegressor": {
        "cls": "Block",
        "typename": "GradientBoostingRegressor",
        "desc": "Gradient Boosting for regression.  GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.  Read more in the :ref:`User Guide <gradient_boosting>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "loss": {
                "type": "option(ls, lad, huber, quantile)",
                "desc": "Loss function to be optimized. 'ls' refers to least squaresregression. 'lad' (least absolute deviation) is a highly robustloss function solely based on order information of the inputvariables. 'huber' is a combination of the two. 'quantile'allows quantile regression (use `alpha` to specify the quantile).",
                "default": "'ls'",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "number",
                "desc": "Learning rate shrinks the contribution of each tree by `learning_rate`.There is a trade-off between learning_rate and n_estimators.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of boosting stages to perform. Gradient boostingis fairly robust to over-fitting so a large number usuallyresults in better performance.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "subsample": {
                "type": "number",
                "desc": "The fraction of samples to be used for fitting the individual baselearners. If smaller than 1.0 this results in Stochastic GradientBoosting. `subsample` interacts with the parameter `n_estimators`.Choosing `subsample < 1.0` leads to a reduction of varianceand an increase in bias.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "criterion": {
                "type": "option(friedman_mse, mse, mae)",
                "desc": "The function to measure the quality of a split. Supported criteriaare \"friedman_mse\" for the mean squared error with improvementscore by Friedman, \"mse\" for mean squared error, and \"mae\" forthe mean absolute error. The default value of \"friedman_mse\" isgenerally the best as it can provide a better approximation insome cases... versionadded:: 0.18.. deprecated:: 0.24`criterion='mae'` is deprecated and will be removed in version1.1 (renaming of 0.26). The correct way of minimizing the absoluteerror is to use `loss='lad'` instead.",
                "default": "'friedman_mse'",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "Maximum depth of the individual regression estimators. The maximumdepth limits the number of nodes in the tree. Tune this parameterfor best performance; the best value depends on the interactionof the input variables.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "estimator or 'zero', default=None",
                "desc": "An estimator object that is used to compute the initial predictions.``init`` has to provide :term:`fit` and :term:`predict`. If 'zero', theinitial raw predictions are set to zero. By default a``DummyEstimator`` is used, predicting either the average target value(for loss='ls'), or a quantile for the other losses.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random seed given to each Tree estimator at eachboosting iteration.In addition, it controls the random permutation of the features ateach split (see Notes for more details).It also controls the random spliting of the training data to obtain avalidation set if `n_iter_no_change` is not None.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "option(auto, sqrt, log2)",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`int(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=n_features`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Choosing `max_features < n_features` leads to a reduction of varianceand an increase in bias.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "The alpha-quantile of the huber loss function and the quantileloss function. Only if ``loss='huber'`` or ``loss='quantile'``.",
                "default": "0.9",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Enable verbose output. If 1 then it prints progress and performanceonce in a while (the more trees the lower the frequency). If greaterthan 1 then it prints progress and performance for every tree.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow trees with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just erase theprevious solution. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if ``n_iter_no_change`` is set to an integer... versionadded:: 0.20",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "``n_iter_no_change`` is used to decide if early stopping will be usedto terminate training when validation score is not improving. Bydefault it is set to None to disable early stopping. If set to anumber, it will set aside ``validation_fraction`` size of the trainingdata as validation and terminate training when validation score is notimproving in all of the previous ``n_iter_no_change`` numbers ofiterations... versionadded:: 0.20",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for the early stopping. When the loss is not improvingby at least tol for ``n_iter_no_change`` iterations (if set to anumber), the training stops... versionadded:: 0.20",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.GradientBoostingRegressor"
        }
    },
    "sklearn.ensemble.IsolationForest": {
        "cls": "Block",
        "typename": "IsolationForest",
        "desc": "Isolation Forest Algorithm.  Return the anomaly score of each sample using the IsolationForest algorithm  The IsolationForest 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.  Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.  This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.  Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies.  Read more in the :ref:`User Guide <isolation_forest>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of base estimators in the ensemble.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "max_samples": {
                "type": "\"auto\", int or float, default=\"auto\"",
                "desc": "The number of samples to draw from X to train each base estimator.- If int, then draw `max_samples` samples.- If float, then draw `max_samples * X.shape[0]` samples.- If \"auto\", then `max_samples=min(256, n_samples)`.If max_samples is larger than the number of samples provided,all samples will be used for all trees (no sampling).",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            },
            "contamination": {
                "type": "'auto' or float, default='auto'",
                "desc": "The amount of contamination of the data set, i.e. the proportionof outliers in the data set. Used when fitting to define the thresholdon the scores of the samples.- If 'auto', the threshold is determined as in theoriginal paper.- If float, the contamination should be in the range [0, 0.5]... versionchanged:: 0.22The default value of ``contamination`` changed from 0.1to ``'auto'``.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The number of features to draw from X to train each base estimator.- If int, then draw `max_features` features.- If float, then draw `max_features * X.shape[1]` features.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "bootstrap": {
                "type": "boolean",
                "desc": "If True, individual trees are fit on random subsets of the trainingdata sampled with replacement. If False, sampling without replacementis performed.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel for both :meth:`fit` and:meth:`predict`. ``None`` means 1 unless in a:obj:`joblib.parallel_backend` context. ``-1`` means using allprocessors. See :term:`Glossary <n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the pseudo-randomness of the selection of the featureand split values for each branching step and each tree in the forest.Pass an int for reproducible results across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity of the tree building process.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest. See :term:`the Glossary <warm_start>`... versionadded:: 0.21",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.IsolationForest"
        }
    },
    "sklearn.ensemble.RandomForestClassifier": {
        "cls": "Block",
        "typename": "RandomForestClassifier",
        "desc": "A random forest classifier.  A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.  Read more in the :ref:`User Guide <forest>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of trees in the forest... versionchanged:: 0.22The default value of ``n_estimators`` changed from 10 to 100in 0.22.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "criterion": {
                "type": "option(\"gini\", \"entropy\")",
                "desc": "The function to measure the quality of a split. Supported criteria are\"gini\" for the Gini impurity and \"entropy\" for the information gain.Note: this parameter is tree-specific.",
                "default": "\"gini\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "option(\"auto\", \"sqrt\", \"log2\")",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`round(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=sqrt(n_features)`.- If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow trees with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "bootstrap": {
                "type": "boolean",
                "desc": "Whether bootstrap samples are used when building trees. If False, thewhole dataset is used to build each tree.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "oob_score": {
                "type": "boolean",
                "desc": "Whether to use out-of-bag samples to estimate the generalization score.Only available if bootstrap=True.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,:meth:`decision_path` and :meth:`apply` are all parallelized over thetrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`context. ``-1`` means using all processors. See :term:`Glossary<n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls both the randomness of the bootstrapping of the samples usedwhen building trees (if ``bootstrap=True``) and the sampling of thefeatures to consider when looking for the best split at each node(if ``max_features < n_features``).See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity when fitting and predicting.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "option(\"balanced\", \"balanced_subsample\")",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If not given, all classes are supposed to have weight one. Formulti-output problems, a list of dicts can be provided in the sameorder as the columns of y.Note that for multioutput (including multilabel) weights should bedefined for each class of every column in its own dict. For example,for four-class multilabel classification weights should be[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of[{1:1}, {2:5}, {3:1}, {4:1}].The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``The \"balanced_subsample\" mode is the same as \"balanced\" except thatweights are computed based on the bootstrap sample for every treegrown.For multi-output, the weights of each column of y will be multiplied.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_samples": {
                "type": "number",
                "desc": "If bootstrap is True, the number of samples to draw from Xto train each base estimator.- If None (default), then draw `X.shape[0]` samples.- If int, then draw `max_samples` samples.- If float, then draw `max_samples * X.shape[0]` samples. Thus,`max_samples` should be in the interval `(0, 1)`... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.RandomForestClassifier"
        }
    },
    "sklearn.ensemble.RandomForestRegressor": {
        "cls": "Block",
        "typename": "RandomForestRegressor",
        "desc": "A random forest regressor.  A random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the `max_samples` parameter if `bootstrap=True` (default), otherwise the whole dataset is used to build each tree.  Read more in the :ref:`User Guide <forest>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "The number of trees in the forest... versionchanged:: 0.22The default value of ``n_estimators`` changed from 10 to 100in 0.22.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "criterion": {
                "type": "option(\"mse\", \"mae\")",
                "desc": "The function to measure the quality of a split. Supported criteriaare \"mse\" for the mean squared error, which is equal to variancereduction as feature selection criterion, and \"mae\" for the meanabsolute error... versionadded:: 0.18Mean Absolute Error (MAE) criterion.",
                "default": "\"mse\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "option(\"auto\", \"sqrt\", \"log2\")",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`round(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=n_features`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow trees with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "bootstrap": {
                "type": "boolean",
                "desc": "Whether bootstrap samples are used when building trees. If False, thewhole dataset is used to build each tree.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "oob_score": {
                "type": "boolean",
                "desc": "Whether to use out-of-bag samples to estimate the generalization score.Only available if bootstrap=True.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,:meth:`decision_path` and :meth:`apply` are all parallelized over thetrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`context. ``-1`` means using all processors. See :term:`Glossary<n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls both the randomness of the bootstrapping of the samples usedwhen building trees (if ``bootstrap=True``) and the sampling of thefeatures to consider when looking for the best split at each node(if ``max_features < n_features``).See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity when fitting and predicting.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_samples": {
                "type": "number",
                "desc": "If bootstrap is True, the number of samples to draw from Xto train each base estimator.- If None (default), then draw `X.shape[0]` samples.- If int, then draw `max_samples` samples.- If float, then draw `max_samples * X.shape[0]` samples. Thus,`max_samples` should be in the interval `(0, 1)`... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.RandomForestRegressor"
        }
    },
    "sklearn.ensemble.RandomTreesEmbedding": {
        "cls": "Block",
        "typename": "RandomTreesEmbedding",
        "desc": "An ensemble of totally random trees.  An unsupervised transformation of a dataset to a high-dimensional sparse representation. A datapoint is coded according to which leaf of each tree it is sorted into. Using a one-hot encoding of the leaves, this leads to a binary coding with as many ones as there are trees in the forest.  The dimensionality of the resulting representation is ``n_out <= n_estimators * max_leaf_nodes``. If ``max_leaf_nodes == None``, the number of leaf nodes is at most ``n_estimators * 2 ** max_depth``.  Read more in the :ref:`User Guide <random_trees_embedding>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_estimators": {
                "type": "number",
                "desc": "Number of trees in the forest... versionchanged:: 0.22The default value of ``n_estimators`` changed from 10 to 100in 0.22.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of each tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` is the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` is the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow trees with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "sparse_output": {
                "type": "boolean",
                "desc": "Whether or not to return a sparse CSR matrix, as default behavior,or to return a dense array compatible with dense pipeline operators.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel. :meth:`fit`, :meth:`transform`,:meth:`decision_path` and :meth:`apply` are all parallelized over thetrees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`context. ``-1`` means using all processors. See :term:`Glossary<n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the generation of the random `y` used to fit the treesand the draw of the splits for each feature at the trees' nodes.See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity when fitting and predicting.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fitand add more estimators to the ensemble, otherwise, just fit a wholenew forest. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.RandomTreesEmbedding"
        }
    },
    "sklearn.ensemble.StackingClassifier": {
        "cls": "Parent",
        "typename": "StackingClassifier",
        "desc": "Stack of estimators with a final classifier.  Stacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.  Note that `estimators_` are fitted on the full `X` while `final_estimator_` is trained using cross-validated predictions of the base estimators using `cross_val_predict`.  Read more in the :ref:`User Guide <stacking>`.  .. versionadded:: 0.22",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "final_estimator": {
                "type": "estimator, default=None",
                "desc": "A classifier which will be used to combine the base estimators.The default classifier is a:class:`~sklearn.linear_model.LogisticRegression`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy used in`cross_val_predict` to train `final_estimator`. Possible inputs forcv are:* None, to use the default 5-fold cross validation,* integer, to specify the number of folds in a (Stratified) KFold,* An object to be used as a cross-validation generator,* An iterable yielding train, test splits.For integer/None inputs, if the estimator is a classifier and y iseither binary or multiclass,:class:`~sklearn.model_selection.StratifiedKFold` is used.In all other cases, :class:`~sklearn.model_selection.KFold` is used.These splitters are instantiated with `shuffle=False` so the splitswill be the same across calls.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... note::A larger number of split will provide no benefits if the numberof training samples is large enough. Indeed, the training timewill increase. ``cv`` is not used for model evaluation but forprediction.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "stack_method": {
                "type": "option(auto, predict_proba, decision_function, predict)",
                "desc": "Methods called for each base estimator. It can be:* if 'auto', it will try to invoke, for each estimator,`'predict_proba'`, `'decision_function'` or `'predict'` in thatorder.* otherwise, one of `'predict_proba'`, `'decision_function'` or`'predict'`. If the method is not implemented by the estimator, itwill raise an error.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel all `estimators` `fit`.`None` means 1 unless in a `joblib.parallel_backend` context. -1 meansusing all processors. See Glossary for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "passthrough": {
                "type": "boolean",
                "desc": "When False, only the predictions of estimators will be used astraining data for `final_estimator`. When True, the`final_estimator` is trained on the predictions as well as theoriginal training data.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Verbosity level.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.StackingClassifier",
            "estname": "estimators",
            "multiple": true
        }
    },
    "sklearn.ensemble.StackingRegressor": {
        "cls": "Parent",
        "typename": "StackingRegressor",
        "desc": "Stack of estimators with a final regressor.  Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator.  Note that `estimators_` are fitted on the full `X` while `final_estimator_` is trained using cross-validated predictions of the base estimators using `cross_val_predict`.  Read more in the :ref:`User Guide <stacking>`.  .. versionadded:: 0.22",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "final_estimator": {
                "type": "estimator, default=None",
                "desc": "A regressor which will be used to combine the base estimators.The default regressor is a :class:`~sklearn.linear_model.RidgeCV`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy used in`cross_val_predict` to train `final_estimator`. Possible inputs forcv are:* None, to use the default 5-fold cross validation,* integer, to specify the number of folds in a (Stratified) KFold,* An object to be used as a cross-validation generator,* An iterable yielding train, test splits.For integer/None inputs, if the estimator is a classifier and y iseither binary or multiclass,:class:`~sklearn.model_selection.StratifiedKFold` is used.In all other cases, :class:`~sklearn.model_selection.KFold` is used.These splitters are instantiated with `shuffle=False` so the splitswill be the same across calls.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... note::A larger number of split will provide no benefits if the numberof training samples is large enough. Indeed, the training timewill increase. ``cv`` is not used for model evaluation but forprediction.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel for `fit` of all `estimators`.`None` means 1 unless in a `joblib.parallel_backend` context. -1 meansusing all processors. See Glossary for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "passthrough": {
                "type": "boolean",
                "desc": "When False, only the predictions of estimators will be used astraining data for `final_estimator`. When True, the`final_estimator` is trained on the predictions as well as theoriginal training data.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Verbosity level.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.StackingRegressor",
            "estname": "estimators",
            "multiple": true
        }
    },
    "sklearn.ensemble.VotingClassifier": {
        "cls": "Parent",
        "typename": "VotingClassifier",
        "desc": "Soft Voting/Majority Rule classifier for unfitted estimators.  Read more in the :ref:`User Guide <voting_classifier>`.  .. versionadded:: 0.17",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "voting": {
                "type": "option(hard, soft)",
                "desc": "If 'hard', uses predicted class labels for majority rule voting.Else if 'soft', predicts the class label based on the argmax ofthe sums of the predicted probabilities, which is recommended foran ensemble of well-calibrated classifiers.",
                "default": "'hard'",
                "dictKeyOf": "initkargs"
            },
            "weights": {
                "type": "array-like of shape (n_classifiers,), default=None",
                "desc": "Sequence of weights (`float` or `int`) to weight the occurrences ofpredicted class labels (`hard` voting) or class probabilitiesbefore averaging (`soft` voting). Uses uniform weights if `None`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel for ``fit``.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "flatten_transform": {
                "type": "boolean",
                "desc": "Affects shape of transform output only when voting='soft'If voting='soft' and flatten_transform=True, transform method returnsmatrix with shape (n_samples, n_classifiers * n_classes). Ifflatten_transform=False, it returns(n_classifiers, n_samples, n_classes).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "If True, the time elapsed while fitting will be printed as itis completed... versionadded:: 0.23",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.VotingClassifier",
            "estname": "estimators",
            "multiple": true
        }
    },
    "sklearn.ensemble.VotingRegressor": {
        "cls": "Parent",
        "typename": "VotingRegressor",
        "desc": "Prediction voting regressor for unfitted estimators.  A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction.  Read more in the :ref:`User Guide <voting_regressor>`.  .. versionadded:: 0.21",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.ensemble",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "weights": {
                "type": "array-like of shape (n_regressors,), default=None",
                "desc": "Sequence of weights (`float` or `int`) to weight the occurrences ofpredicted values before averaging. Uses uniform weights if `None`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel for ``fit``.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "If True, the time elapsed while fitting will be printed as itis completed... versionadded:: 0.23",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.ensemble.VotingRegressor",
            "estname": "estimators",
            "multiple": true
        }
    },
    "sklearn.feature_extraction.DictVectorizer": {
        "cls": "Block",
        "typename": "DictVectorizer",
        "desc": "Transforms lists of feature-value mappings to vectors.  This transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.  When feature values are strings, this transformer will do a binary one-hot (aka one-of-K) coding: one boolean-valued feature is constructed for each of the possible string values that the feature can take on. For instance, a feature \"f\" that can take on the values \"ham\" and \"spam\" will become two features in the output, one signifying \"f=ham\", the other \"f=spam\".  If a feature value is a sequence or set of strings, this transformer will iterate over the values and will count the occurrences of each string value.  However, note that this transformer will only do a binary one-hot encoding when feature values are of type string. If categorical features are represented as numeric values such as int or iterables of strings, the DictVectorizer can be followed by :class:`~sklearn.preprocessing.OneHotEncoder` to complete binary one-hot encoding.  Features that do not occur in a sample (mapping) will have a zero value in the resulting array/matrix.  Read more in the :ref:`User Guide <dict_feature_extraction>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "dtype": {
                "type": "dtype, default=np.float64",
                "desc": "The type of feature values. Passed to Numpy array/scipy.sparse matrixconstructors as the dtype argument.",
                "default": "np.float64",
                "dictKeyOf": "initkargs"
            },
            "separator": {
                "type": "string",
                "desc": "Separator string used when constructing new features for one-hotcoding.",
                "default": "\"=\"",
                "dictKeyOf": "initkargs"
            },
            "sparse": {
                "type": "boolean",
                "desc": "Whether transform should produce scipy.sparse matrices.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "sort": {
                "type": "boolean",
                "desc": "Whether ``feature_names_`` and ``vocabulary_`` should besorted when fitting.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.DictVectorizer"
        }
    },
    "sklearn.feature_extraction.FeatureHasher": {
        "cls": "Block",
        "typename": "FeatureHasher",
        "desc": "Implements feature hashing, aka the hashing trick.  This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.  Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.  This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.  Read more in the :ref:`User Guide <feature_hashing>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_features": {
                "type": "number",
                "desc": "The number of features (columns) in the output matrices. Small numbersof features are likely to cause hash collisions, but large numberswill cause larger coefficient dimensions in linear learners.",
                "default": "2**20",
                "dictKeyOf": "initkargs"
            },
            "input_type": {
                "type": "option(\"dict\", \"pair\", \"string\")",
                "desc": "Either \"dict\" (the default) to accept dictionaries over(feature_name, value); \"pair\" to accept pairs of (feature_name, value);or \"string\" to accept single strings.feature_name should be a string, while value should be a number.In the case of \"string\", a value of 1 is implied.The feature_name is hashed to find the appropriate column for thefeature. The value's sign might be flipped in the output (but seenon_negative, below).",
                "default": "\"dict\"",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "numpy dtype, default=np.float64",
                "desc": "The type of feature values. Passed to scipy.sparse matrix constructorsas the dtype argument. Do not set this to bool, np.boolean or anyunsigned integer type.",
                "default": "np.float64",
                "dictKeyOf": "initkargs"
            },
            "alternate_sign": {
                "type": "boolean",
                "desc": "When True, an alternating sign is added to the features as toapproximately conserve the inner product in the hashed space even forsmall n_features. This approach is similar to sparse random projection... versionchanged:: 0.19``alternate_sign`` replaces the now deprecated ``non_negative``parameter.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.FeatureHasher"
        }
    },
    "sklearn.feature_extraction.image.PatchExtractor": {
        "cls": "Block",
        "typename": "PatchExtractor",
        "desc": "Extracts patches from a collection of images  Read more in the :ref:`User Guide <image_feature_extraction>`.  .. versionadded:: 0.9",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction.image",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "patch_size": {
                "type": "tuple of int (patch_height, patch_width), default=None",
                "desc": "The dimensions of one patch.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_patches": {
                "type": "number",
                "desc": "The maximum number of patches per image to extract. If max_patches is afloat in (0, 1), it is taken to mean a proportion of the total numberof patches.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines the random number generator used for random sampling when`max_patches` is not None. Use an int to make the randomnessdeterministic.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.image.PatchExtractor"
        }
    },
    "sklearn.feature_extraction.text.CountVectorizer": {
        "cls": "Block",
        "typename": "CountVectorizer",
        "desc": "Convert a collection of text documents to a matrix of token counts  This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.  If you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.  Read more in the :ref:`User Guide <text_feature_extraction>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction.text",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "input": {
                "type": "option(filename, file, content)",
                "desc": "- If `'filename'`, the sequence passed as an argument to fit isexpected to be a list of filenames that need reading to fetchthe raw content to analyze.- If `'file'`, the sequence items must have a 'read' method (file-likeobject) that is called to fetch the bytes in memory.- If `'content'`, the input is expected to be a sequence of items thatcan be of type string or byte.",
                "default": "'content'",
                "dictKeyOf": "initkargs"
            },
            "encoding": {
                "type": "string",
                "desc": "If bytes or files are given to analyze, this encoding is used todecode.",
                "default": "'utf-8'",
                "dictKeyOf": "initkargs"
            },
            "decode_error": {
                "type": "option(strict, ignore, replace)",
                "desc": "Instruction on what to do if a byte sequence is given to analyze thatcontains characters not of the given `encoding`. By default, it is'strict', meaning that a UnicodeDecodeError will be raised. Othervalues are 'ignore' and 'replace'.",
                "default": "'strict'",
                "dictKeyOf": "initkargs"
            },
            "strip_accents": {
                "type": "option(ascii, unicode)",
                "desc": "Remove accents and perform other character normalizationduring the preprocessing step.'ascii' is a fast method that only works on characters that havean direct ASCII mapping.'unicode' is a slightly slower method that works on any characters.None (default) does nothing.Both 'ascii' and 'unicode' use NFKD normalization from:func:`unicodedata.normalize`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "lowercase": {
                "type": "boolean",
                "desc": "Convert all characters to lowercase before tokenizing.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "preprocessor": {
                "type": "callable, default=None",
                "desc": "Override the preprocessing (strip_accents and lowercase) stage whilepreserving the tokenizing and n-grams generation steps.Only applies if ``analyzer is not callable``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tokenizer": {
                "type": "callable, default=None",
                "desc": "Override the string tokenization step while preserving thepreprocessing and n-grams generation steps.Only applies if ``analyzer == 'word'``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "stop_words": {
                "type": "option(english)",
                "desc": "If 'english', a built-in stop word list for English is used.There are several known issues with 'english' and you shouldconsider an alternative (see :ref:`stop_words`).If a list, that list is assumed to contain stop words, all of whichwill be removed from the resulting tokens.Only applies if ``analyzer == 'word'``.If None, no stop words will be used. max_df can be set to a valuein the range [0.7, 1.0) to automatically detect and filter stopwords based on intra corpus document frequency of terms.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "token_pattern": {
                "type": "string",
                "desc": "Regular expression denoting what constitutes a \"token\", only usedif ``analyzer == 'word'``. The default regexp select tokens of 2or more alphanumeric characters (punctuation is completely ignoredand always treated as a token separator).If there is a capturing group in token_pattern then thecaptured group content, not the entire match, becomes the token.At most one capturing group is permitted.",
                "default": "r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"",
                "dictKeyOf": "initkargs"
            },
            "ngram_range": {
                "type": "tuple (min_n, max_n), default=(1, 1)",
                "desc": "The lower and upper boundary of the range of n-values for differentword n-grams or char n-grams to be extracted. All values of n suchsuch that min_n <= n <= max_n will be used. For example an``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` meansunigrams and bigrams, and ``(2, 2)`` means only bigrams.Only applies if ``analyzer is not callable``.",
                "default": "(1",
                "dictKeyOf": "initkargs"
            },
            "analyzer": {
                "type": "option(word, char, char_wb)",
                "desc": "Whether the feature should be made of word n-gram or charactern-grams.Option 'char_wb' creates character n-grams only from text insideword boundaries; n-grams at the edges of words are padded with space.If a callable is passed it is used to extract the sequence of featuresout of the raw, unprocessed input... versionchanged:: 0.21Since v0.21, if ``input`` is ``filename`` or ``file``, the data isfirst read from the file and then passed to the given callableanalyzer.",
                "default": "'word'",
                "dictKeyOf": "initkargs"
            },
            "max_df": {
                "type": "number",
                "desc": "When building the vocabulary ignore terms that have a documentfrequency strictly higher than the given threshold (corpus-specificstop words).If float, the parameter represents a proportion of documents, integerabsolute counts.This parameter is ignored if vocabulary is not None.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "min_df": {
                "type": "number",
                "desc": "When building the vocabulary ignore terms that have a documentfrequency strictly lower than the given threshold. This value is alsocalled cut-off in the literature.If float, the parameter represents a proportion of documents, integerabsolute counts.This parameter is ignored if vocabulary is not None.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "If not None, build a vocabulary that only consider the topmax_features ordered by term frequency across the corpus.This parameter is ignored if vocabulary is not None.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "vocabulary": {
                "type": "Mapping or iterable, default=None",
                "desc": "Either a Mapping (e.g., a dict) where keys are terms and values areindices in the feature matrix, or an iterable over terms. If notgiven, a vocabulary is determined from the input documents. Indicesin the mapping should not be repeated and should not have any gapbetween 0 and the largest index.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "binary": {
                "type": "boolean",
                "desc": "If True, all non zero counts are set to 1. This is useful for discreteprobabilistic models that model binary events rather than integercounts.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "type, default=np.int64",
                "desc": "Type of the matrix returned by fit_transform() or transform().",
                "default": "np.int64",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.text.CountVectorizer"
        }
    },
    "sklearn.feature_extraction.text.FeatureHasher": {
        "cls": "Block",
        "typename": "FeatureHasher",
        "desc": "Implements feature hashing, aka the hashing trick.  This class turns sequences of symbolic feature names (strings) into scipy.sparse matrices, using a hash function to compute the matrix column corresponding to a name. The hash function employed is the signed 32-bit version of Murmurhash3.  Feature names of type byte string are used as-is. Unicode strings are converted to UTF-8 first, but no Unicode normalization is done. Feature values must be (finite) numbers.  This class is a low-memory alternative to DictVectorizer and CountVectorizer, intended for large-scale (online) learning and situations where memory is tight, e.g. when running prediction code on embedded devices.  Read more in the :ref:`User Guide <feature_hashing>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction.text",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_features": {
                "type": "number",
                "desc": "The number of features (columns) in the output matrices. Small numbersof features are likely to cause hash collisions, but large numberswill cause larger coefficient dimensions in linear learners.",
                "default": "2**20",
                "dictKeyOf": "initkargs"
            },
            "input_type": {
                "type": "option(\"dict\", \"pair\", \"string\")",
                "desc": "Either \"dict\" (the default) to accept dictionaries over(feature_name, value); \"pair\" to accept pairs of (feature_name, value);or \"string\" to accept single strings.feature_name should be a string, while value should be a number.In the case of \"string\", a value of 1 is implied.The feature_name is hashed to find the appropriate column for thefeature. The value's sign might be flipped in the output (but seenon_negative, below).",
                "default": "\"dict\"",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "numpy dtype, default=np.float64",
                "desc": "The type of feature values. Passed to scipy.sparse matrix constructorsas the dtype argument. Do not set this to bool, np.boolean or anyunsigned integer type.",
                "default": "np.float64",
                "dictKeyOf": "initkargs"
            },
            "alternate_sign": {
                "type": "boolean",
                "desc": "When True, an alternating sign is added to the features as toapproximately conserve the inner product in the hashed space even forsmall n_features. This approach is similar to sparse random projection... versionchanged:: 0.19``alternate_sign`` replaces the now deprecated ``non_negative``parameter.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.text.FeatureHasher"
        }
    },
    "sklearn.feature_extraction.text.HashingVectorizer": {
        "cls": "Block",
        "typename": "HashingVectorizer",
        "desc": "Convert a collection of text documents to a matrix of token occurrences  It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm='l1' or projected on the euclidean unit sphere if norm='l2'.  This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.  This strategy has several advantages:  - it is very low memory scalable to large datasets as there is no need to   store a vocabulary dictionary in memory  - it is fast to pickle and un-pickle as it holds no state besides the   constructor parameters  - it can be used in a streaming (partial fit) or parallel pipeline as there   is no state computed during fit.  There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):  - there is no way to compute the inverse transform (from feature indices to   string feature names) which can be a problem when trying to introspect   which features are most important to a model.  - there can be collisions: distinct tokens can be mapped to the same   feature index. However in practice this is rarely an issue if n_features   is large enough (e.g. 2 ** 18 for text classification problems).  - no IDF weighting as this would render the transformer stateful.  The hash function employed is the signed 32-bit version of Murmurhash3.  Read more in the :ref:`User Guide <text_feature_extraction>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction.text",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "input": {
                "type": "option(filename, file, content)",
                "desc": "- If `'filename'`, the sequence passed as an argument to fit isexpected to be a list of filenames that need reading to fetchthe raw content to analyze.- If `'file'`, the sequence items must have a 'read' method (file-likeobject) that is called to fetch the bytes in memory.- If `'content'`, the input is expected to be a sequence of items thatcan be of type string or byte.",
                "default": "'content'",
                "dictKeyOf": "initkargs"
            },
            "encoding": {
                "type": "string",
                "desc": "If bytes or files are given to analyze, this encoding is used todecode.",
                "default": "'utf-8'",
                "dictKeyOf": "initkargs"
            },
            "decode_error": {
                "type": "option(strict, ignore, replace)",
                "desc": "Instruction on what to do if a byte sequence is given to analyze thatcontains characters not of the given `encoding`. By default, it is'strict', meaning that a UnicodeDecodeError will be raised. Othervalues are 'ignore' and 'replace'.",
                "default": "'strict'",
                "dictKeyOf": "initkargs"
            },
            "strip_accents": {
                "type": "option(ascii, unicode)",
                "desc": "Remove accents and perform other character normalizationduring the preprocessing step.'ascii' is a fast method that only works on characters that havean direct ASCII mapping.'unicode' is a slightly slower method that works on any characters.None (default) does nothing.Both 'ascii' and 'unicode' use NFKD normalization from:func:`unicodedata.normalize`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "lowercase": {
                "type": "boolean",
                "desc": "Convert all characters to lowercase before tokenizing.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "preprocessor": {
                "type": "callable, default=None",
                "desc": "Override the preprocessing (string transformation) stage whilepreserving the tokenizing and n-grams generation steps.Only applies if ``analyzer is not callable``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tokenizer": {
                "type": "callable, default=None",
                "desc": "Override the string tokenization step while preserving thepreprocessing and n-grams generation steps.Only applies if ``analyzer == 'word'``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "stop_words": {
                "type": "option(english)",
                "desc": "If 'english', a built-in stop word list for English is used.There are several known issues with 'english' and you shouldconsider an alternative (see :ref:`stop_words`).If a list, that list is assumed to contain stop words, all of whichwill be removed from the resulting tokens.Only applies if ``analyzer == 'word'``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "token_pattern": {
                "type": "string",
                "desc": "Regular expression denoting what constitutes a \"token\", only usedif ``analyzer == 'word'``. The default regexp selects tokens of 2or more alphanumeric characters (punctuation is completely ignoredand always treated as a token separator).If there is a capturing group in token_pattern then thecaptured group content, not the entire match, becomes the token.At most one capturing group is permitted.",
                "default": "r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"",
                "dictKeyOf": "initkargs"
            },
            "ngram_range": {
                "type": "tuple (min_n, max_n), default=(1, 1)",
                "desc": "The lower and upper boundary of the range of n-values for differentn-grams to be extracted. All values of n such that min_n <= n <= max_nwill be used. For example an ``ngram_range`` of ``(1, 1)`` means onlyunigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` meansonly bigrams.Only applies if ``analyzer is not callable``.",
                "default": "(1",
                "dictKeyOf": "initkargs"
            },
            "analyzer": {
                "type": "option(word, char, char_wb)",
                "desc": "Whether the feature should be made of word or character n-grams.Option 'char_wb' creates character n-grams only from text insideword boundaries; n-grams at the edges of words are padded with space.If a callable is passed it is used to extract the sequence of featuresout of the raw, unprocessed input... versionchanged:: 0.21Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the datais first read from the file and then passed to the given callableanalyzer.",
                "default": "'word'",
                "dictKeyOf": "initkargs"
            },
            "n_features": {
                "type": "number",
                "desc": "The number of features (columns) in the output matrices. Small numbersof features are likely to cause hash collisions, but large numberswill cause larger coefficient dimensions in linear learners.",
                "default": "(2 ** 20)",
                "dictKeyOf": "initkargs"
            },
            "binary": {
                "type": "boolean",
                "desc": "If True, all non zero counts are set to 1. This is useful for discreteprobabilistic models that model binary events rather than integercounts.",
                "default": "False.",
                "dictKeyOf": "initkargs"
            },
            "norm": {
                "type": "option(l1, l2)",
                "desc": "Norm used to normalize term vectors. None for no normalization.",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "alternate_sign": {
                "type": "boolean",
                "desc": "When True, an alternating sign is added to the features as toapproximately conserve the inner product in the hashed space even forsmall n_features. This approach is similar to sparse random projection... versionadded:: 0.19",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "type, default=np.float64",
                "desc": "Type of the matrix returned by fit_transform() or transform().",
                "default": "np.float64",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.text.HashingVectorizer"
        }
    },
    "sklearn.feature_extraction.text.TfidfTransformer": {
        "cls": "Block",
        "typename": "TfidfTransformer",
        "desc": "Transform a count matrix to a normalized tf or tf-idf representation  Tf means term-frequency while tf-idf means term-frequency times inverse document-frequency. This is a common term weighting scheme in information retrieval, that has also found good use in document classification.  The goal of using tf-idf instead of the raw frequencies of occurrence of a token in a given document is to scale down the impact of tokens that occur very frequently in a given corpus and that are hence empirically less informative than features that occur in a small fraction of the training corpus.  The formula that is used to compute the tf-idf for a term t of a document d in a document set is tf-idf(t, d) = tf(t, d) * idf(t), and the idf is computed as idf(t) = log [ n / df(t) ] + 1 (if ``smooth_idf=False``), where n is the total number of documents in the document set and df(t) is the document frequency of t; the document frequency is the number of documents in the document set that contain the term t. The effect of adding \"1\" to the idf in the equation above is that terms with zero idf, i.e., terms that occur in all documents in a training set, will not be entirely ignored. (Note that the idf formula above differs from the standard textbook notation that defines the idf as idf(t) = log [ n / (df(t) + 1) ]).  If ``smooth_idf=True`` (the default), the constant \"1\" is added to the numerator and denominator of the idf as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions: idf(t) = log [ (1 + n) / (1 + df(t)) ] + 1.  Furthermore, the formulas used to compute tf and idf depend on parameter settings that correspond to the SMART notation used in IR as follows:  Tf is \"n\" (natural) by default, \"l\" (logarithmic) when ``sublinear_tf=True``. Idf is \"t\" when use_idf is given, \"n\" (none) otherwise. Normalization is \"c\" (cosine) when ``norm='l2'``, \"n\" (none) when ``norm=None``.  Read more in the :ref:`User Guide <text_feature_extraction>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction.text",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "norm": {
                "type": "option(l1, l2)",
                "desc": "Each output row will have unit norm, either:* 'l2': Sum of squares of vector elements is 1. The cosinesimilarity between two vectors is their dot product when l2 norm hasbeen applied.* 'l1': Sum of absolute values of vector elements is 1.See :func:`preprocessing.normalize`",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "use_idf": {
                "type": "boolean",
                "desc": "Enable inverse-document-frequency reweighting.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "smooth_idf": {
                "type": "boolean",
                "desc": "Smooth idf weights by adding one to document frequencies, as if anextra document was seen containing every term in the collectionexactly once. Prevents zero divisions.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "sublinear_tf": {
                "type": "boolean",
                "desc": "Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.text.TfidfTransformer"
        }
    },
    "sklearn.feature_extraction.text.TfidfVectorizer": {
        "cls": "Block",
        "typename": "TfidfVectorizer",
        "desc": "Convert a collection of raw documents to a matrix of TF-IDF features.  Equivalent to :class:`CountVectorizer` followed by :class:`TfidfTransformer`.  Read more in the :ref:`User Guide <text_feature_extraction>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_extraction.text",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "input": {
                "type": "option(filename, file, content)",
                "desc": "- If `'filename'`, the sequence passed as an argument to fit isexpected to be a list of filenames that need reading to fetchthe raw content to analyze.- If `'file'`, the sequence items must have a 'read' method (file-likeobject) that is called to fetch the bytes in memory.- If `'content'`, the input is expected to be a sequence of items thatcan be of type string or byte.",
                "default": "'content'",
                "dictKeyOf": "initkargs"
            },
            "encoding": {
                "type": "string",
                "desc": "If bytes or files are given to analyze, this encoding is used todecode.",
                "default": "'utf-8'",
                "dictKeyOf": "initkargs"
            },
            "decode_error": {
                "type": "option(strict, ignore, replace)",
                "desc": "Instruction on what to do if a byte sequence is given to analyze thatcontains characters not of the given `encoding`. By default, it is'strict', meaning that a UnicodeDecodeError will be raised. Othervalues are 'ignore' and 'replace'.",
                "default": "'strict'",
                "dictKeyOf": "initkargs"
            },
            "strip_accents": {
                "type": "option(ascii, unicode)",
                "desc": "Remove accents and perform other character normalizationduring the preprocessing step.'ascii' is a fast method that only works on characters that havean direct ASCII mapping.'unicode' is a slightly slower method that works on any characters.None (default) does nothing.Both 'ascii' and 'unicode' use NFKD normalization from:func:`unicodedata.normalize`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "lowercase": {
                "type": "boolean",
                "desc": "Convert all characters to lowercase before tokenizing.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "preprocessor": {
                "type": "callable, default=None",
                "desc": "Override the preprocessing (string transformation) stage whilepreserving the tokenizing and n-grams generation steps.Only applies if ``analyzer is not callable``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tokenizer": {
                "type": "callable, default=None",
                "desc": "Override the string tokenization step while preserving thepreprocessing and n-grams generation steps.Only applies if ``analyzer == 'word'``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "analyzer": {
                "type": "option(word, char, char_wb)",
                "desc": "Whether the feature should be made of word or character n-grams.Option 'char_wb' creates character n-grams only from text insideword boundaries; n-grams at the edges of words are padded with space.If a callable is passed it is used to extract the sequence of featuresout of the raw, unprocessed input... versionchanged:: 0.21Since v0.21, if ``input`` is ``'filename'`` or ``'file'``, the datais first read from the file and then passed to the given callableanalyzer.",
                "default": "'word'",
                "dictKeyOf": "initkargs"
            },
            "stop_words": {
                "type": "option(english)",
                "desc": "If a string, it is passed to _check_stop_list and the appropriate stoplist is returned. 'english' is currently the only supported stringvalue.There are several known issues with 'english' and you shouldconsider an alternative (see :ref:`stop_words`).If a list, that list is assumed to contain stop words, all of whichwill be removed from the resulting tokens.Only applies if ``analyzer == 'word'``.If None, no stop words will be used. max_df can be set to a valuein the range [0.7, 1.0) to automatically detect and filter stopwords based on intra corpus document frequency of terms.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "token_pattern": {
                "type": "string",
                "desc": "Regular expression denoting what constitutes a \"token\", only usedif ``analyzer == 'word'``. The default regexp selects tokens of 2or more alphanumeric characters (punctuation is completely ignoredand always treated as a token separator).If there is a capturing group in token_pattern then thecaptured group content, not the entire match, becomes the token.At most one capturing group is permitted.",
                "default": "r\"(?u)\\\\b\\\\w\\\\w+\\\\b\"",
                "dictKeyOf": "initkargs"
            },
            "ngram_range": {
                "type": "tuple (min_n, max_n), default=(1, 1)",
                "desc": "The lower and upper boundary of the range of n-values for differentn-grams to be extracted. All values of n such that min_n <= n <= max_nwill be used. For example an ``ngram_range`` of ``(1, 1)`` means onlyunigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` meansonly bigrams.Only applies if ``analyzer is not callable``.",
                "default": "(1",
                "dictKeyOf": "initkargs"
            },
            "max_df": {
                "type": "number",
                "desc": "When building the vocabulary ignore terms that have a documentfrequency strictly higher than the given threshold (corpus-specificstop words).If float in range [0.0, 1.0], the parameter represents a proportion ofdocuments, integer absolute counts.This parameter is ignored if vocabulary is not None.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "min_df": {
                "type": "number",
                "desc": "When building the vocabulary ignore terms that have a documentfrequency strictly lower than the given threshold. This value is alsocalled cut-off in the literature.If float in range of [0.0, 1.0], the parameter represents a proportionof documents, integer absolute counts.This parameter is ignored if vocabulary is not None.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "If not None, build a vocabulary that only consider the topmax_features ordered by term frequency across the corpus.This parameter is ignored if vocabulary is not None.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "vocabulary": {
                "type": "Mapping or iterable, default=None",
                "desc": "Either a Mapping (e.g., a dict) where keys are terms and values areindices in the feature matrix, or an iterable over terms. If notgiven, a vocabulary is determined from the input documents.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "binary": {
                "type": "boolean",
                "desc": "If True, all non-zero term counts are set to 1. This does not meanoutputs will have only 0/1 values, only that the tf term in tf-idfis binary. (Set idf and normalization to False to get 0/1 outputs).",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "dtype, default=float64",
                "desc": "Type of the matrix returned by fit_transform() or transform().",
                "default": "float64",
                "dictKeyOf": "initkargs"
            },
            "norm": {
                "type": "option(l1, l2)",
                "desc": "Each output row will have unit norm, either:* 'l2': Sum of squares of vector elements is 1. The cosinesimilarity between two vectors is their dot product when l2 norm hasbeen applied.* 'l1': Sum of absolute values of vector elements is 1.See :func:`preprocessing.normalize`.",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "use_idf": {
                "type": "boolean",
                "desc": "Enable inverse-document-frequency reweighting.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "smooth_idf": {
                "type": "boolean",
                "desc": "Smooth idf weights by adding one to document frequencies, as if anextra document was seen containing every term in the collectionexactly once. Prevents zero divisions.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "sublinear_tf": {
                "type": "boolean",
                "desc": "Apply sublinear tf scaling, i.e. replace tf with 1 + log(tf).",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_extraction.text.TfidfVectorizer"
        }
    },
    "sklearn.feature_selection.GenericUnivariateSelect": {
        "cls": "Block",
        "typename": "GenericUnivariateSelect",
        "desc": "Univariate feature selector with configurable strategy.  Read more in the :ref:`User Guide <univariate_feature_selection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "score_func": {
                "type": "callable, default=f_classif",
                "desc": "Function taking two arrays X and y, and returning a pair of arrays(scores, pvalues). For modes 'percentile' or 'kbest' it can returna single array scores.",
                "default": "f_classif",
                "dictKeyOf": "initkargs"
            },
            "mode": {
                "type": "option(percentile, k_best, fpr, fdr, fwe)",
                "desc": "Feature selection mode.",
                "default": "'percentile'",
                "dictKeyOf": "initkargs"
            },
            "param": {
                "type": "number",
                "desc": "Parameter of the corresponding mode.",
                "default": "1e-5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.GenericUnivariateSelect"
        }
    },
    "sklearn.feature_selection.RFE": {
        "cls": "Parent",
        "typename": "RFE",
        "desc": "Feature ranking with recursive feature elimination.  Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through any specific attribute or callable. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached.  Read more in the :ref:`User Guide <rfe>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_features_to_select": {
                "type": "number",
                "desc": "The number of features to select. If `None`, half of the features areselected. If integer, the parameter is the absolute number of featuresto select. If float between 0 and 1, it is the fraction of features toselect... versionchanged:: 0.24Added float values for fractions.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "step": {
                "type": "number",
                "desc": "If greater than or equal to 1, then ``step`` corresponds to the(integer) number of features to remove at each iteration.If within (0.0, 1.0), then ``step`` corresponds to the percentage(rounded down) of features to remove at each iteration.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls verbosity of output.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "importance_getter": {
                "type": "string",
                "desc": "If 'auto', uses the feature importance either through a `coef_`or `feature_importances_` attributes of estimator.Also accepts a string that specifies an attribute name/pathfor extracting feature importance (implemented with `attrgetter`).For example, give `regressor_.coef_` in case of:class:`~sklearn.compose.TransformedTargetRegressor` or`named_steps.clf.feature_importances_` in case ofclass:`~sklearn.pipeline.Pipeline` with its last step named `clf`.If `callable`, overrides the default feature importance getter.The callable is passed with the fitted estimator and it shouldreturn importance for each feature... versionadded:: 0.24",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.RFE",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.feature_selection.RFECV": {
        "cls": "Parent",
        "typename": "RFECV",
        "desc": "Feature ranking with recursive feature elimination and cross-validated selection of the best number of features.  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <rfe>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "step": {
                "type": "number",
                "desc": "If greater than or equal to 1, then ``step`` corresponds to the(integer) number of features to remove at each iteration.If within (0.0, 1.0), then ``step`` corresponds to the percentage(rounded down) of features to remove at each iteration.Note that the last iteration may remove fewer than ``step`` features inorder to reach ``min_features_to_select``.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_features_to_select": {
                "type": "number",
                "desc": "The minimum number of features to be selected. This number of featureswill always be scored, even if the difference between the originalfeature count and ``min_features_to_select`` isn't divisible by``step``... versionadded:: 0.20",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- integer, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, if ``y`` is binary or multiclass,:class:`~sklearn.model_selection.StratifiedKFold` is used. If theestimator is a classifier or if ``y`` is neither binary nor multiclass,:class:`~sklearn.model_selection.KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value of None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "scoring": {
                "type": "string",
                "desc": "A string (see model evaluation documentation) ora scorer callable object / function with signature``scorer(estimator, X, y)``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls verbosity of output.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of cores to run in parallel while fitting across folds.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "importance_getter": {
                "type": "string",
                "desc": "If 'auto', uses the feature importance either through a `coef_`or `feature_importances_` attributes of estimator.Also accepts a string that specifies an attribute name/pathfor extracting feature importance.For example, give `regressor_.coef_` in case of:class:`~sklearn.compose.TransformedTargetRegressor` or`named_steps.clf.feature_importances_` in case of:class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.If `callable`, overrides the default feature importance getter.The callable is passed with the fitted estimator and it shouldreturn importance for each feature... versionadded:: 0.24",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.RFECV",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.feature_selection.SelectFdr": {
        "cls": "Block",
        "typename": "SelectFdr",
        "desc": "Filter: Select the p-values for an estimated false discovery rate  This uses the Benjamini-Hochberg procedure. ``alpha`` is an upper bound on the expected false discovery rate.  Read more in the :ref:`User Guide <univariate_feature_selection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "score_func": {
                "type": "callable, default=f_classif",
                "desc": "Function taking two arrays X and y, and returning a pair of arrays(scores, pvalues).Default is f_classif (see below \"See Also\"). The default function onlyworks with classification tasks.",
                "default": "f_classif",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "The highest uncorrected p-value for features to keep.",
                "default": "5e-2",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.SelectFdr"
        }
    },
    "sklearn.feature_selection.SelectFpr": {
        "cls": "Block",
        "typename": "SelectFpr",
        "desc": "Filter: Select the pvalues below alpha based on a FPR test.  FPR test stands for False Positive Rate test. It controls the total amount of false detections.  Read more in the :ref:`User Guide <univariate_feature_selection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "score_func": {
                "type": "callable, default=f_classif",
                "desc": "Function taking two arrays X and y, and returning a pair of arrays(scores, pvalues).Default is f_classif (see below \"See Also\"). The default function onlyworks with classification tasks.",
                "default": "f_classif",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "The highest p-value for features to be kept.",
                "default": "5e-2",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.SelectFpr"
        }
    },
    "sklearn.feature_selection.SelectFromModel": {
        "cls": "Parent",
        "typename": "SelectFromModel",
        "desc": "Meta-transformer for selecting features based on importance weights.  .. versionadded:: 0.17  Read more in the :ref:`User Guide <select_from_model>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "threshold": {
                "type": "string",
                "desc": "The threshold value to use for feature selection. Features whoseimportance is greater or equal are kept while the others arediscarded. If \"median\" (resp. \"mean\"), then the ``threshold`` value isthe median (resp. the mean) of the feature importances. A scalingfactor (e.g., \"1.25*mean\") may also be used. If None and if theestimator has a parameter penalty set to l1, either explicitlyor implicitly (e.g, Lasso), the threshold used is 1e-5.Otherwise, \"mean\" is used by default.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "prefit": {
                "type": "boolean",
                "desc": "Whether a prefit model is expected to be passed into the constructordirectly or not. If True, ``transform`` must be called directlyand SelectFromModel cannot be used with ``cross_val_score``,``GridSearchCV`` and similar utilities that clone the estimator.Otherwise train the model using ``fit`` and then ``transform`` to dofeature selection.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "norm_order": {
                "type": "non-zero int, inf, -inf, default=1",
                "desc": "Order of the norm used to filter the vectors of coefficients below``threshold`` in the case where the ``coef_`` attribute of theestimator is of dimension 2.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The maximum number of features to select.To only select based on ``max_features``, set ``threshold=-np.inf``... versionadded:: 0.20",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "importance_getter": {
                "type": "string",
                "desc": "If 'auto', uses the feature importance either through a ``coef_``attribute or ``feature_importances_`` attribute of estimator.Also accepts a string that specifies an attribute name/pathfor extracting feature importance (implemented with `attrgetter`).For example, give `regressor_.coef_` in case of:class:`~sklearn.compose.TransformedTargetRegressor` or`named_steps.clf.feature_importances_` in case of:class:`~sklearn.pipeline.Pipeline` with its last step named `clf`.If `callable`, overrides the default feature importance getter.The callable is passed with the fitted estimator and it shouldreturn importance for each feature... versionadded:: 0.24",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.SelectFromModel",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.feature_selection.SelectFwe": {
        "cls": "Block",
        "typename": "SelectFwe",
        "desc": "Filter: Select the p-values corresponding to Family-wise error rate  Read more in the :ref:`User Guide <univariate_feature_selection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "score_func": {
                "type": "callable, default=f_classif",
                "desc": "Function taking two arrays X and y, and returning a pair of arrays(scores, pvalues).Default is f_classif (see below \"See Also\"). The default function onlyworks with classification tasks.",
                "default": "f_classif",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "The highest uncorrected p-value for features to keep.",
                "default": "5e-2",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.SelectFwe"
        }
    },
    "sklearn.feature_selection.SelectKBest": {
        "cls": "Block",
        "typename": "SelectKBest",
        "desc": "Select features according to the k highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "score_func": {
                "type": "callable, default=f_classif",
                "desc": "Function taking two arrays X and y, and returning a pair of arrays(scores, pvalues) or a single array with scores.Default is f_classif (see below \"See Also\"). The default function onlyworks with classification tasks... versionadded:: 0.18",
                "default": "f_classif",
                "dictKeyOf": "initkargs"
            },
            "k": {
                "type": "number",
                "desc": "Number of top features to select.The \"all\" option bypasses selection, for use in a parameter search.",
                "default": "10",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.SelectKBest"
        }
    },
    "sklearn.feature_selection.SelectPercentile": {
        "cls": "Block",
        "typename": "SelectPercentile",
        "desc": "Select features according to a percentile of the highest scores.  Read more in the :ref:`User Guide <univariate_feature_selection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "score_func": {
                "type": "callable, default=f_classif",
                "desc": "Function taking two arrays X and y, and returning a pair of arrays(scores, pvalues) or a single array with scores.Default is f_classif (see below \"See Also\"). The default function onlyworks with classification tasks... versionadded:: 0.18",
                "default": "f_classif",
                "dictKeyOf": "initkargs"
            },
            "percentile": {
                "type": "number",
                "desc": "Percent of features to keep.",
                "default": "10",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.SelectPercentile"
        }
    },
    "sklearn.feature_selection.SequentialFeatureSelector": {
        "cls": "Parent",
        "typename": "SequentialFeatureSelector",
        "desc": "Transformer that performs Sequential Feature Selection.  This Sequential Feature Selector adds (forward selection) or removes (backward selection) features to form a feature subset in a greedy fashion. At each stage, this estimator chooses the best feature to add or remove based on the cross-validation score of an estimator.  Read more in the :ref:`User Guide <sequential_feature_selection>`.  .. versionadded:: 0.24",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_features_to_select": {
                "type": "number",
                "desc": "The number of features to select. If `None`, half of the features areselected. If integer, the parameter is the absolute number of featuresto select. If float between 0 and 1, it is the fraction of features toselect.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "direction": {
                "type": "option(forward, backward)",
                "desc": "Whether to perform forward selection or backward selection.",
                "default": "'forward'",
                "dictKeyOf": "initkargs"
            },
            "scoring": {
                "type": "string",
                "desc": "A single str (see :ref:`scoring_parameter`) or a callable(see :ref:`scoring`) to evaluate the predictions on the test set.NOTE that when using custom scorers, each scorer should return a singlevalue. Metric functions returning a list/array of values can be wrappedinto multiple scorers that return one value each.If None, the estimator's score method is used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross validation,- integer, to specify the number of folds in a `(Stratified)KFold`,- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, if the estimator is a classifier and ``y`` iseither binary or multiclass, :class:`StratifiedKFold` is used. In allother cases, :class:`KFold` is used. These splitters are instantiatedwith `shuffle=False` so the splits will be the same across calls.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of jobs to run in parallel. When evaluating a new feature toadd or remove, the cross-validation procedure is parallel over thefolds.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.SequentialFeatureSelector",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.feature_selection.VarianceThreshold": {
        "cls": "Block",
        "typename": "VarianceThreshold",
        "desc": "Feature selector that removes all low-variance features.  This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.  Read more in the :ref:`User Guide <variance_threshold>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.feature_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "threshold": {
                "type": "number",
                "desc": "Features with a training-set variance lower than this threshold willbe removed. The default is to keep all features with non-zero variance,i.e. remove the features that have the same value in all samples.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.feature_selection.VarianceThreshold"
        }
    },
    "sklearn.gaussian_process.GaussianProcessClassifier": {
        "cls": "Block",
        "typename": "GaussianProcessClassifier",
        "desc": "Gaussian process classification (GPC) based on Laplace approximation.  The implementation is based on Algorithm 3.1, 3.2, and 5.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.  Internally, the Laplace approximation is used for approximating the non-Gaussian posterior by a Gaussian.  Currently, the implementation is restricted to using the logistic link function. For multi-class classification, several binary one-versus rest classifiers are fitted. Note that this class thus does not implement a true multi-class Laplace approximation.  Read more in the :ref:`User Guide <gaussian_process>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "kernel instance, default=None",
                "desc": "The kernel specifying the covariance function of the GP. If None ispassed, the kernel \"1.0 * RBF(1.0)\" is used as default. Note thatthe kernel's hyperparameters are optimized during fitting.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "optimizer": {
                "type": "'fmin_l_bfgs_b' or callable, default='fmin_l_bfgs_b'",
                "desc": "Can either be one of the internally supported optimizers for optimizingthe kernel's parameters, specified by a string, or an externallydefined optimizer passed as a callable. If a callable is passed, itmust have the signature::def optimizer(obj_func, initial_theta, bounds):# * 'obj_func' is the objective function to be maximized, which#  takes the hyperparameters theta as parameter and an#  optional flag eval_gradient, which determines if the#  gradient is returned additionally to the function value# * 'initial_theta': the initial value for theta, which can be#  used by local optimizers# * 'bounds': the bounds on the values of theta....# Returned are the best found hyperparameters theta and# the corresponding value of the target function.return theta_opt, func_minPer default, the 'L-BFGS-B' algorithm from scipy.optimize.minimizeis used. If None is passed, the kernel's parameters are kept fixed.Available internal optimizers are::'fmin_l_bfgs_b'",
                "default": "'fmin_l_bfgs_b'",
                "dictKeyOf": "initkargs"
            },
            "n_restarts_optimizer": {
                "type": "number",
                "desc": "The number of restarts of the optimizer for finding the kernel'sparameters which maximize the log-marginal likelihood. The first runof the optimizer is performed from the kernel's initial parameters,the remaining ones (if any) from thetas sampled log-uniform randomlyfrom the space of allowed theta-values. If greater than 0, all boundsmust be finite. Note that n_restarts_optimizer=0 implies that onerun is performed.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "max_iter_predict": {
                "type": "number",
                "desc": "The maximum number of iterations in Newton's method for approximatingthe posterior during predict. Smaller values will reduce computationtime at the cost of worse results.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "If warm-starts are enabled, the solution of the last Newton iterationon the Laplace approximation of the posterior mode is used asinitialization for the next call of _posterior_mode(). This can speedup convergence when _posterior_mode is called several times on similarproblems as in hyperparameter optimization. See :term:`the Glossary<warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X_train": {
                "type": "boolean",
                "desc": "If True, a persistent copy of the training data is stored in theobject. Otherwise, just a reference to the training data is stored,which might cause predictions to change if the data is modifiedexternally.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation used to initialize the centers.Pass an int for reproducible results across multiple function calls.See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "multi_class": {
                "type": "option(one_vs_rest, one_vs_one)",
                "desc": "Specifies how multi-class classification problems are handled.Supported are 'one_vs_rest' and 'one_vs_one'. In 'one_vs_rest',one binary Gaussian process classifier is fitted for each class, whichis trained to separate this class from the rest. In 'one_vs_one', onebinary Gaussian process classifier is fitted for each pair of classes,which is trained to separate these two classes. The predictions ofthese binary predictors are combined into multi-class predictions.Note that 'one_vs_one' does not support predicting probabilityestimates.",
                "default": "'one_vs_rest'",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation: the specifiedmulticlass problems are computed in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.GaussianProcessClassifier"
        }
    },
    "sklearn.gaussian_process.GaussianProcessRegressor": {
        "cls": "Block",
        "typename": "GaussianProcessRegressor",
        "desc": "Gaussian process regression (GPR).  The implementation is based on Algorithm 2.1 of Gaussian Processes for Machine Learning (GPML) by Rasmussen and Williams.  In addition to standard scikit-learn estimator API, GaussianProcessRegressor:     * allows prediction without prior fitting (based on the GP prior)    * provides an additional method sample_y(X), which evaluates samples      drawn from the GPR (prior or posterior) at given inputs    * exposes a method log_marginal_likelihood(theta), which can be used      externally for other ways of selecting hyperparameters, e.g., via      Markov chain Monte Carlo.  Read more in the :ref:`User Guide <gaussian_process>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "kernel instance, default=None",
                "desc": "The kernel specifying the covariance function of the GP. If None ispassed, the kernel ``ConstantKernel(1.0, constant_value_bounds=\"fixed\"* RBF(1.0, length_scale_bounds=\"fixed\")`` is used as default. Note thatthe kernel hyperparameters are optimized during fitting unless thebounds are marked as \"fixed\".",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Value added to the diagonal of the kernel matrix during fitting.This can prevent a potential numerical issue during fitting, byensuring that the calculated values form a positive definite matrix.It can also be interpreted as the variance of additional Gaussianmeasurement noise on the training observations. Note that this isdifferent from using a `WhiteKernel`. If an array is passed, it musthave the same number of entries as the data used for fitting and isused as datapoint-dependent noise level. Allowing to specify thenoise level directly as a parameter is mainly for convenience andfor consistency with Ridge.",
                "default": "1e-10",
                "dictKeyOf": "initkargs"
            },
            "optimizer": {
                "type": "\"fmin_l_bfgs_b\" or callable, default=\"fmin_l_bfgs_b\"",
                "desc": "Can either be one of the internally supported optimizers for optimizingthe kernel's parameters, specified by a string, or an externallydefined optimizer passed as a callable. If a callable is passed, itmust have the signature::def optimizer(obj_func, initial_theta, bounds):# * 'obj_func' is the objective function to be minimized, which#  takes the hyperparameters theta as parameter and an#  optional flag eval_gradient, which determines if the#  gradient is returned additionally to the function value# * 'initial_theta': the initial value for theta, which can be#  used by local optimizers# * 'bounds': the bounds on the values of theta....# Returned are the best found hyperparameters theta and# the corresponding value of the target function.return theta_opt, func_minPer default, the 'L-BGFS-B' algorithm from scipy.optimize.minimizeis used. If None is passed, the kernel's parameters are kept fixed.Available internal optimizers are::'fmin_l_bfgs_b'",
                "default": "\"fmin_l_bfgs_b\"",
                "dictKeyOf": "initkargs"
            },
            "n_restarts_optimizer": {
                "type": "number",
                "desc": "The number of restarts of the optimizer for finding the kernel'sparameters which maximize the log-marginal likelihood. The first runof the optimizer is performed from the kernel's initial parameters,the remaining ones (if any) from thetas sampled log-uniform randomlyfrom the space of allowed theta-values. If greater than 0, all boundsmust be finite. Note that n_restarts_optimizer == 0 implies that onerun is performed.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "normalize_y": {
                "type": "boolean",
                "desc": "Whether the target values y are normalized, the mean and variance ofthe target values are set equal to 0 and 1 respectively. This isrecommended for cases where zero-mean, unit-variance priors are used.Note that, in this implementation, the normalisation is reversedbefore the GP predictions are reported... versionchanged:: 0.23",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X_train": {
                "type": "boolean",
                "desc": "If True, a persistent copy of the training data is stored in theobject. Otherwise, just a reference to the training data is stored,which might cause predictions to change if the data is modifiedexternally.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation used to initialize the centers.Pass an int for reproducible results across multiple function calls.See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.GaussianProcessRegressor"
        }
    },
    "sklearn.gaussian_process.kernels.CompoundKernel": {
        "cls": "Block",
        "typename": "CompoundKernel",
        "desc": "Kernel which is composed of a set of other kernels.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernels": {
                "type": "list of Kernels",
                "desc": "The other kernels",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.CompoundKernel"
        }
    },
    "sklearn.gaussian_process.kernels.ConstantKernel": {
        "cls": "Block",
        "typename": "ConstantKernel",
        "desc": "Constant kernel.  Can be used as part of a product-kernel where it scales the magnitude of the other factor (kernel) or as part of a sum-kernel, where it modifies the mean of the Gaussian process.  .. math::     k(x_1, x_2) = constant\\_value \\;\\forall\\; x_1, x_2  Adding a constant kernel is equivalent to adding a constant::          kernel = RBF() + ConstantKernel(constant_value=2)  is the same as::          kernel = RBF() + 2   Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "constant_value": {
                "type": "number",
                "desc": "The constant value which defines the covariance:k(x_1, x_2) = constant_value",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "constant_value_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on `constant_value`.If set to \"fixed\", `constant_value` cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.ConstantKernel"
        }
    },
    "sklearn.gaussian_process.kernels.DotProduct": {
        "cls": "Block",
        "typename": "DotProduct",
        "desc": "Dot-Product kernel.  The DotProduct kernel is non-stationary and can be obtained from linear regression by putting :math:`N(0, 1)` priors on the coefficients of :math:`x_d (d = 1, . . . , D)` and a prior of :math:`N(0, \\sigma_0^2)` on the bias. The DotProduct kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter sigma_0 :math:`\\sigma` which controls the inhomogenity of the kernel. For :math:`\\sigma_0^2 =0`, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by  .. math::     k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j  The DotProduct kernel is commonly combined with exponentiation.  See [1]_, Chapter 4, Section 4.2, for further details regarding the DotProduct kernel.  Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "sigma_0": {
                "type": "number",
                "desc": "Parameter controlling the inhomogenity of the kernel. If sigma_0=0,the kernel is homogenous.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "sigma_0_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'sigma_0'.If set to \"fixed\", 'sigma_0' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.DotProduct"
        }
    },
    "sklearn.gaussian_process.kernels.ExpSineSquared": {
        "cls": "Block",
        "typename": "ExpSineSquared",
        "desc": "Exp-Sine-Squared kernel (aka periodic kernel).  The ExpSineSquared kernel allows one to model functions which repeat themselves exactly. It is parameterized by a length scale parameter :math:`l>0` and a periodicity parameter :math:`p>0`. Only the isotropic variant where :math:`l` is a scalar is supported at the moment. The kernel is given by:  .. math::     k(x_i, x_j) = \\text{exp}\\left(-     \\frac{ 2\\sin^2(\\pi d(x_i, x_j)/p) }{ l^ 2} \\right)  where :math:`l` is the length scale of the kernel, :math:`p` the periodicity of the kernel and :math:`d(\\\\cdot,\\\\cdot)` is the Euclidean distance.  Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "length_scale": {
                "type": "number",
                "desc": "The length scale of the kernel.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "periodicity": {
                "type": "number",
                "desc": "The periodicity of the kernel.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "length_scale_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'length_scale'.If set to \"fixed\", 'length_scale' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            },
            "periodicity_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'periodicity'.If set to \"fixed\", 'periodicity' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.ExpSineSquared"
        }
    },
    "sklearn.gaussian_process.kernels.Exponentiation": {
        "cls": "Block",
        "typename": "Exponentiation",
        "desc": "The Exponentiation kernel takes one base kernel and a scalar parameter :math:`p` and combines them via  .. math::     k_{exp}(X, Y) = k(X, Y) ^p  Note that the `__pow__` magic method is overridden, so `Exponentiation(RBF(), 2)` is equivalent to using the ** operator with `RBF() ** 2`.   Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "Kernel",
                "desc": "The base kernel",
                "dictKeyOf": "initkargs"
            },
            "exponent": {
                "type": "number",
                "desc": "The exponent for the base kernel",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.Exponentiation"
        }
    },
    "sklearn.gaussian_process.kernels.Matern": {
        "cls": "Block",
        "typename": "Matern",
        "desc": "Matern kernel.  The class of Matern kernels is a generalization of the :class:`RBF`. It has an additional parameter :math:`\\nu` which controls the smoothness of the resulting function. The smaller :math:`\\nu`, the less smooth the approximated function is. As :math:`\\nu\\rightarrow\\infty`, the kernel becomes equivalent to the :class:`RBF` kernel. When :math:`\\nu = 1/2`, the Matrn kernel becomes identical to the absolute exponential kernel. Important intermediate values are :math:`\\nu=1.5` (once differentiable functions) and :math:`\\nu=2.5` (twice differentiable functions).  The kernel is given by:  .. math::      k(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(      \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )      \\Bigg)^\\nu K_\\nu\\Bigg(      \\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)    where :math:`d(\\cdot,\\cdot)` is the Euclidean distance, :math:`K_{\\nu}(\\cdot)` is a modified Bessel function and :math:`\\Gamma(\\cdot)` is the gamma function. See [1]_, Chapter 4, Section 4.2, for details regarding the different variants of the Matern kernel.  Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "length_scale": {
                "type": "number",
                "desc": "The length scale of the kernel. If a float, an isotropic kernel isused. If an array, an anisotropic kernel is used where each dimensionof l defines the length-scale of the respective feature dimension.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "length_scale_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'length_scale'.If set to \"fixed\", 'length_scale' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            },
            "nu": {
                "type": "number",
                "desc": "The parameter nu controlling the smoothness of the learned function.The smaller nu, the less smooth the approximated function is.For nu=inf, the kernel becomes equivalent to the RBF kernel and fornu=0.5 to the absolute exponential kernel. Important intermediatevalues are nu=1.5 (once differentiable functions) and nu=2.5(twice differentiable functions). Note that values of nu not in[0.5, 1.5, 2.5, inf] incur a considerably higher computational cost(appr. 10 times higher) since they require to evaluate the modifiedBessel function. Furthermore, in contrast to l, nu is kept fixed toits initial value and not optimized.",
                "default": "1.5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.Matern"
        }
    },
    "sklearn.gaussian_process.kernels.PairwiseKernel": {
        "cls": "Block",
        "typename": "PairwiseKernel",
        "desc": "Wrapper for kernels in sklearn.metrics.pairwise.  A thin wrapper around the functionality of the kernels in sklearn.metrics.pairwise.  Note: Evaluation of eval_gradient is not analytic but numeric and all       kernels support only isotropic distances. The parameter gamma is       considered to be a hyperparameter and may be optimized. The other       kernel parameters are set directly at initialization and are kept       fixed.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "gamma": {
                "type": "number",
                "desc": "Parameter gamma of the pairwise kernel specified by metric. It shouldbe positive.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "gamma_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'gamma'.If set to \"fixed\", 'gamma' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "option(\"linear\", \"additive_chi2\", \"chi2\", \"poly\", \"polynomial\",               \"rbf\", \"laplacian\", \"sigmoid\", \"cosine\")",
                "desc": "The metric to use when calculating kernel between instances in afeature array. If metric is a string, it must be one of the metricsin pairwise.PAIRWISE_KERNEL_FUNCTIONS.If metric is \"precomputed\", X is assumed to be a kernel matrix.Alternatively, if metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays from X as input and return a value indicatingthe distance between them.",
                "default": "\"linear\"",
                "dictKeyOf": "initkargs"
            },
            "pairwise_kernels_kwargs": {
                "type": "dict, default=None",
                "desc": "All entries of this dict (if any) are passed as keyword arguments tothe pairwise kernel function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.PairwiseKernel"
        }
    },
    "sklearn.gaussian_process.kernels.Product": {
        "cls": "Block",
        "typename": "Product",
        "desc": "The `Product` kernel takes two kernels :math:`k_1` and :math:`k_2` and combines them via  .. math::     k_{prod}(X, Y) = k_1(X, Y) * k_2(X, Y)  Note that the `__mul__` magic method is overridden, so `Product(RBF(), RBF())` is equivalent to using the * operator with `RBF() * RBF()`.  Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "k1": {
                "type": "Kernel",
                "desc": "The first base-kernel of the product-kernel",
                "dictKeyOf": "initkargs"
            },
            "k2": {
                "type": "Kernel",
                "desc": "The second base-kernel of the product-kernel",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.Product"
        }
    },
    "sklearn.gaussian_process.kernels.RBF": {
        "cls": "Block",
        "typename": "RBF",
        "desc": "Radial-basis function kernel (aka squared-exponential kernel).  The RBF kernel is a stationary kernel. It is also known as the \"squared exponential\" kernel. It is parameterized by a length scale parameter :math:`l>0`, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs X (anisotropic variant of the kernel). The kernel is given by:  .. math::     k(x_i, x_j) = \\exp\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)  where :math:`l` is the length scale of the kernel and :math:`d(\\cdot,\\cdot)` is the Euclidean distance. For advice on how to set the length scale parameter, see e.g. [1]_.  This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. See [2]_, Chapter 4, Section 4.2, for further details of the RBF kernel.  Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "length_scale": {
                "type": "number",
                "desc": "The length scale of the kernel. If a float, an isotropic kernel isused. If an array, an anisotropic kernel is used where each dimensionof l defines the length-scale of the respective feature dimension.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "length_scale_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'length_scale'.If set to \"fixed\", 'length_scale' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.RBF"
        }
    },
    "sklearn.gaussian_process.kernels.RationalQuadratic": {
        "cls": "Block",
        "typename": "RationalQuadratic",
        "desc": "Rational Quadratic kernel.  The RationalQuadratic kernel can be seen as a scale mixture (an infinite sum) of RBF kernels with different characteristic length scales. It is parameterized by a length scale parameter :math:`l>0` and a scale mixture parameter :math:`\\alpha>0`. Only the isotropic variant where length_scale :math:`l` is a scalar is supported at the moment. The kernel is given by:  .. math::     k(x_i, x_j) = \\left(     1 + \\frac{d(x_i, x_j)^2 }{ 2\\alpha  l^2}\\right)^{-\\alpha}  where :math:`\\alpha` is the scale mixture parameter, :math:`l` is the length scale of the kernel and :math:`d(\\cdot,\\cdot)` is the Euclidean distance. For advice on how to set the parameters, see e.g. [1]_.  Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "length_scale": {
                "type": "number",
                "desc": "The length scale of the kernel.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Scale mixture parameter",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "length_scale_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'length_scale'.If set to \"fixed\", 'length_scale' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            },
            "alpha_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'alpha'.If set to \"fixed\", 'alpha' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.RationalQuadratic"
        }
    },
    "sklearn.gaussian_process.kernels.Sum": {
        "cls": "Block",
        "typename": "Sum",
        "desc": "The `Sum` kernel takes two kernels :math:`k_1` and :math:`k_2` and combines them via  .. math::     k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)  Note that the `__add__` magic method is overridden, so `Sum(RBF(), RBF())` is equivalent to using the + operator with `RBF() + RBF()`.   Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "k1": {
                "type": "Kernel",
                "desc": "The first base-kernel of the sum-kernel",
                "dictKeyOf": "initkargs"
            },
            "k2": {
                "type": "Kernel",
                "desc": "The second base-kernel of the sum-kernel",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.Sum"
        }
    },
    "sklearn.gaussian_process.kernels.WhiteKernel": {
        "cls": "Block",
        "typename": "WhiteKernel",
        "desc": "White kernel.  The main use-case of this kernel is as part of a sum-kernel where it explains the noise of the signal as independently and identically normally-distributed. The parameter noise_level equals the variance of this noise.  .. math::     k(x_1, x_2) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0   Read more in the :ref:`User Guide <gp_kernels>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.gaussian_process.kernels",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "noise_level": {
                "type": "number",
                "desc": "Parameter controlling the noise level (variance)",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "noise_level_bounds": {
                "type": "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)",
                "desc": "The lower and upper bound on 'noise_level'.If set to \"fixed\", 'noise_level' cannot be changed duringhyperparameter tuning.",
                "default": "(1e-5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.gaussian_process.kernels.WhiteKernel"
        }
    },
    "sklearn.impute.KNNImputer": {
        "cls": "Block",
        "typename": "KNNImputer",
        "desc": "Imputation for completing missing values using k-Nearest Neighbors.  Each sample's missing values are imputed using the mean value from `n_neighbors` nearest neighbors found in the training set. Two samples are close if the features that neither is missing are close.  Read more in the :ref:`User Guide <knnimpute>`.  .. versionadded:: 0.22",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.impute",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "missing_values": {
                "type": "number",
                "desc": "The placeholder for the missing values. All occurrences of`missing_values` will be imputed. For pandas' dataframes withnullable integer dtypes with missing values, `missing_values`should be set to np.nan, since `pd.NA` will be converted to np.nan.",
                "default": "np.nan",
                "dictKeyOf": "initkargs"
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of neighboring samples to use for imputation.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "weights": {
                "type": "option(uniform, distance)",
                "desc": "Weight function used in prediction. Possible values:- 'uniform' : uniform weights. All points in each neighborhood areweighted equally.- 'distance' : weight points by the inverse of their distance.in this case, closer neighbors of a query point will have agreater influence than neighbors which are further away.- callable : a user-defined function which accepts anarray of distances, and returns an array of the same shapecontaining the weights.",
                "default": "'uniform'",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "option(nan_euclidean)",
                "desc": "Distance metric for searching neighbors. Possible values:- 'nan_euclidean'- callable : a user-defined function which conforms to the definitionof ``_pairwise_callable(X, Y, metric, **kwds)``. The functionaccepts two arrays, X and Y, and a `missing_values` keyword in`kwds` and returns a scalar distance value.",
                "default": "'nan_euclidean'",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "If True, a copy of X will be created. If False, imputation willbe done in-place whenever possible.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "add_indicator": {
                "type": "boolean",
                "desc": "If True, a :class:`MissingIndicator` transform will stack onto theoutput of the imputer's transform. This allows a predictive estimatorto account for missingness despite imputation. If a feature has nomissing values at fit/train time, the feature won't appear on themissing indicator even if there are missing values at transform/testtime.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.impute.KNNImputer"
        }
    },
    "sklearn.impute.MissingIndicator": {
        "cls": "Block",
        "typename": "MissingIndicator",
        "desc": "Binary indicators for missing values.  Note that this component typically should not be used in a vanilla :class:`Pipeline` consisting of transformers and a classifier, but rather could be added using a :class:`FeatureUnion` or :class:`ColumnTransformer`.  Read more in the :ref:`User Guide <impute>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.impute",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "missing_values": {
                "type": "number",
                "desc": "The placeholder for the missing values. All occurrences of`missing_values` will be imputed. For pandas' dataframes withnullable integer dtypes with missing values, `missing_values`should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.",
                "default": "np.nan",
                "dictKeyOf": "initkargs"
            },
            "features": {
                "type": "option(missing-only, all)",
                "desc": "Whether the imputer mask should represent all or a subset offeatures.- If 'missing-only' (default), the imputer mask will only representfeatures containing missing values during fit time.- If 'all', the imputer mask will represent all features.",
                "default": "'missing-only'",
                "dictKeyOf": "initkargs"
            },
            "sparse": {
                "type": "boolean",
                "desc": "Whether the imputer mask format should be sparse or dense.- If 'auto' (default), the imputer mask will be of same type asinput.- If True, the imputer mask will be a sparse matrix.- If False, the imputer mask will be a numpy array.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "error_on_new": {
                "type": "boolean",
                "desc": "If True, transform will raise an error when there are features withmissing values in transform that have no missing values in fit. This isapplicable only when `features='missing-only'`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.impute.MissingIndicator"
        }
    },
    "sklearn.impute.SimpleImputer": {
        "cls": "Block",
        "typename": "SimpleImputer",
        "desc": "Imputation transformer for completing missing values.  Read more in the :ref:`User Guide <impute>`.  .. versionadded:: 0.20    `SimpleImputer` replaces the previous `sklearn.preprocessing.Imputer`    estimator which is now removed.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.impute",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "missing_values": {
                "type": "number",
                "desc": "The placeholder for the missing values. All occurrences of`missing_values` will be imputed. For pandas' dataframes withnullable integer dtypes with missing values, `missing_values`should be set to `np.nan`, since `pd.NA` will be converted to `np.nan`.",
                "default": "np.nan",
                "dictKeyOf": "initkargs"
            },
            "strategy": {
                "type": "string",
                "desc": "The imputation strategy.- If \"mean\", then replace missing values using the mean alongeach column. Can only be used with numeric data.- If \"median\", then replace missing values using the median alongeach column. Can only be used with numeric data.- If \"most_frequent\", then replace missing using the most frequentvalue along each column. Can be used with strings or numeric data.If there is more than one such value, only the smallest is returned.- If \"constant\", then replace missing values with fill_value. Can beused with strings or numeric data... versionadded:: 0.20strategy=\"constant\" for fixed value imputation.",
                "default": "'mean'",
                "dictKeyOf": "initkargs"
            },
            "fill_value": {
                "type": "string",
                "desc": "When strategy == \"constant\", fill_value is used to replace alloccurrences of missing_values.If left to the default, fill_value will be 0 when imputing numericaldata and \"missing_value\" for strings or object data types.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity of the imputer.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "If True, a copy of X will be created. If False, imputation willbe done in-place whenever possible. Note that, in the following cases,a new copy will always be made, even if `copy=False`:- If X is not an array of floating values;- If X is encoded as a CSR matrix;- If add_indicator=True.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "add_indicator": {
                "type": "boolean",
                "desc": "If True, a :class:`MissingIndicator` transform will stack onto outputof the imputer's transform. This allows a predictive estimatorto account for missingness despite imputation. If a feature has nomissing values at fit/train time, the feature won't appear onthe missing indicator even if there are missing values attransform/test time.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.impute.SimpleImputer"
        }
    },
    "sklearn.inspection.PartialDependenceDisplay": {
        "cls": "Block",
        "typename": "PartialDependenceDisplay",
        "desc": "Partial Dependence Plot (PDP).  This can also display individual partial dependencies which are often referred to as: Individual Condition Expectation (ICE).  It is recommended to use :func:`~sklearn.inspection.plot_partial_dependence` to create a :class:`~sklearn.inspection.PartialDependenceDisplay`. All parameters are stored as attributes.  Read more in :ref:`sphx_glr_auto_examples_miscellaneous_plot_partial_dependence_visualization_api.py` and the :ref:`User Guide <visualizations>`.      .. versionadded:: 0.22",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.inspection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "pd_results": {
                "type": "list of Bunch",
                "desc": "Results of :func:`~sklearn.inspection.partial_dependence` for``features``.",
                "dictKeyOf": "initkargs"
            },
            "features": {
                "type": "list of (int,) or list of (int, int)",
                "desc": "Indices of features for a given plot. A tuple of one integer will plota partial dependence curve of one feature. A tuple of two integers willplot a two-way partial dependence curve as a contour plot.",
                "dictKeyOf": "initkargs"
            },
            "feature_names": {
                "type": "list of str",
                "desc": "Feature names corresponding to the indices in ``features``.",
                "dictKeyOf": "initkargs"
            },
            "target_idx": {
                "type": "number",
                "desc": "- In a multiclass setting, specifies the class for which the PDPsshould be computed. Note that for binary classification, thepositive class (index 1) is always used.- In a multioutput setting, specifies the task for which the PDPsshould be computed.Ignored in binary classification or classical regression settings.",
                "dictKeyOf": "initkargs"
            },
            "pdp_lim": {
                "type": "dict",
                "desc": "Global min and max average predictions, such that all plots will havethe same scale and y limits. `pdp_lim[1]` is the global min and max forsingle partial dependence curves. `pdp_lim[2]` is the global min andmax for two-way partial dependence curves.",
                "dictKeyOf": "initkargs"
            },
            "deciles": {
                "type": "dict",
                "desc": "Deciles for feature indices in ``features``.",
                "dictKeyOf": "initkargs"
            },
            "kind": {
                "type": "option(average, individual, both)",
                "desc": "Whether to plot the partial dependence averaged across all the samplesin the dataset or one line per sample or both.- ``kind='average'`` results in the traditional PD plot;- ``kind='individual'`` results in the ICE plot.Note that the fast ``method='recursion'`` option is only available for``kind='average'``. Plotting individual dependencies requires using theslower ``method='brute'`` option... versionadded:: 0.24",
                "default": "'average'",
                "dictKeyOf": "initkargs"
            },
            "subsample": {
                "type": "number",
                "desc": "Sampling for ICE curves when `kind` is 'individual' or 'both'.If float, should be between 0.0 and 1.0 and represent the proportionof the dataset to be used to plot ICE curves. If int, represents themaximum absolute number of samples to use.Note that the full dataset is still used to calculate partialdependence when `kind='both'`... versionadded:: 0.24",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness of the selected samples when subsamples is not`None`. See :term:`Glossary <random_state>` for details... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.inspection.PartialDependenceDisplay"
        }
    },
    "sklearn.isotonic.IsotonicRegression": {
        "cls": "Block",
        "typename": "IsotonicRegression",
        "desc": "Isotonic regression model.  Read more in the :ref:`User Guide <isotonic>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.isotonic",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "y_min": {
                "type": "number",
                "desc": "Lower bound on the lowest predicted value (the minimum value maystill be higher). If not set, defaults to -inf.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "y_max": {
                "type": "number",
                "desc": "Upper bound on the highest predicted value (the maximum may still belower). If not set, defaults to +inf.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "increasing": {
                "type": "boolean",
                "desc": "Determines whether the predictions should be constrained to increaseor decrease with `X`. 'auto' will decide based on the Spearmancorrelation estimate's sign.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "out_of_bounds": {
                "type": "option(nan, clip, raise)",
                "desc": "Handles how `X` values outside of the training domain are handledduring prediction.- 'nan', predictions will be NaN.- 'clip', predictions will be set to the value corresponding tothe nearest train interval endpoint.- 'raise', a `ValueError` is raised.",
                "default": "'nan'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.isotonic.IsotonicRegression"
        }
    },
    "sklearn.kernel_approximation.AdditiveChi2Sampler": {
        "cls": "Block",
        "typename": "AdditiveChi2Sampler",
        "desc": "Approximate feature map for additive chi2 kernel.  Uses sampling the fourier transform of the kernel characteristic at regular intervals.  Since the kernel that is to be approximated is additive, the components of the input vectors can be treated separately.  Each entry in the original space is transformed into 2*sample_steps+1 features, where sample_steps is a parameter of the method. Typical values of sample_steps include 1, 2 and 3.  Optimal choices for the sampling interval for certain data ranges can be computed (see the reference). The default values should be reasonable.  Read more in the :ref:`User Guide <additive_chi_kernel_approx>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.kernel_approximation",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "sample_steps": {
                "type": "number",
                "desc": "Gives the number of (complex) sampling points.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "sample_interval": {
                "type": "number",
                "desc": "Sampling interval. Must be specified when sample_steps not in {1,2,3}.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.kernel_approximation.AdditiveChi2Sampler"
        }
    },
    "sklearn.kernel_approximation.Nystroem": {
        "cls": "Block",
        "typename": "Nystroem",
        "desc": "Approximate a kernel map using a subset of the training data.  Constructs an approximate feature map for an arbitrary kernel using a subset of the data as basis.  Read more in the :ref:`User Guide <nystroem_kernel_approx>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.kernel_approximation",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "string",
                "desc": "Kernel map to be approximated. A callable should accept two argumentsand the keyword arguments passed to this object as kernel_params, andshould return a floating point number.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "number",
                "desc": "Gamma parameter for the RBF, laplacian, polynomial, exponential chi2and sigmoid kernels. Interpretation of the default value is left tothe kernel; see the documentation for sklearn.metrics.pairwise.Ignored by other kernels.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Zero coefficient for polynomial and sigmoid kernels.Ignored by other kernels.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel. Ignored by other kernels.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "kernel_params": {
                "type": "dict, default=None",
                "desc": "Additional parameters (keyword arguments) for kernel function passedas callable object.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "Number of features to construct.How many data points will be used to construct the mapping.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Pseudo-random number generator to control the uniform sampling withoutreplacement of n_components of the training data to construct the basiskernel.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This works by breakingdown the kernel matrix into n_jobs even slices and computing them inparallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.kernel_approximation.Nystroem"
        }
    },
    "sklearn.kernel_approximation.PolynomialCountSketch": {
        "cls": "Block",
        "typename": "PolynomialCountSketch",
        "desc": "Polynomial kernel approximation via Tensor Sketch.  Implements Tensor Sketch, which approximates the feature map of the polynomial kernel::      K(X, Y) = (gamma * <X, Y> + coef0)^degree  by efficiently computing a Count Sketch of the outer product of a vector with itself using Fast Fourier Transforms (FFT). Read more in the :ref:`User Guide <polynomial_kernel_approx>`.  .. versionadded:: 0.24",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.kernel_approximation",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "gamma": {
                "type": "number",
                "desc": "Parameter of the polynomial kernel whose feature mapwill be approximated.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel whose feature mapwill be approximated.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Constant term of the polynomial kernel whose feature mapwill be approximated.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "Dimensionality of the output feature space. Usually, n_componentsshould be greater than the number of features in input samples inorder to achieve good performance. The optimal score / run timebalance is typically achieved around n_components = 10 * n_features,but this depends on the specific dataset being used.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for indexHash and bitHashinitialization. Pass an int for reproducible results across multiplefunction calls. See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.kernel_approximation.PolynomialCountSketch"
        }
    },
    "sklearn.kernel_approximation.RBFSampler": {
        "cls": "Block",
        "typename": "RBFSampler",
        "desc": "Approximates feature map of an RBF kernel by Monte Carlo approximation of its Fourier transform.  It implements a variant of Random Kitchen Sinks.[1]  Read more in the :ref:`User Guide <rbf_kernel_approx>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.kernel_approximation",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "gamma": {
                "type": "number",
                "desc": "Parameter of RBF kernel: exp(-gamma * x^2)",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "Number of Monte Carlo samples per original feature.Equals the dimensionality of the computed feature space.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Pseudo-random number generator to control the generation of the randomweights and random offset when fitting the training data.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.kernel_approximation.RBFSampler"
        }
    },
    "sklearn.kernel_approximation.SkewedChi2Sampler": {
        "cls": "Block",
        "typename": "SkewedChi2Sampler",
        "desc": "Approximates feature map of the \"skewed chi-squared\" kernel by Monte Carlo approximation of its Fourier transform.  Read more in the :ref:`User Guide <skewed_chi_kernel_approx>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.kernel_approximation",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "skewedness": {
                "type": "number",
                "desc": "\"skewedness\" parameter of the kernel. Needs to be cross-validated.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "number of Monte Carlo samples per original feature.Equals the dimensionality of the computed feature space.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Pseudo-random number generator to control the generation of the randomweights and random offset when fitting the training data.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.kernel_approximation.SkewedChi2Sampler"
        }
    },
    "sklearn.kernel_ridge.KernelRidge": {
        "cls": "Block",
        "typename": "KernelRidge",
        "desc": "Kernel ridge regression.  Kernel ridge regression (KRR) combines ridge regression (linear least squares with l2-norm regularization) with the kernel trick. It thus learns a linear function in the space induced by the respective kernel and the data. For non-linear kernels, this corresponds to a non-linear function in the original space.  The form of the model learned by KRR is identical to support vector regression (SVR). However, different loss functions are used: KRR uses squared error loss while support vector regression uses epsilon-insensitive loss, both combined with l2 regularization. In contrast to SVR, fitting a KRR model can be done in closed-form and is typically faster for medium-sized datasets. On the other hand, the learned model is non-sparse and thus slower than SVR, which learns a sparse model for epsilon > 0, at prediction-time.  This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape [n_samples, n_targets]).  Read more in the :ref:`User Guide <kernel_ridge>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.kernel_ridge",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Regularization strength; must be a positive float. Regularizationimproves the conditioning of the problem and reduces the variance ofthe estimates. Larger values specify stronger regularization.Alpha corresponds to ``1 / (2C)`` in other linear models such as:class:`~sklearn.linear_model.LogisticRegression` or:class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties areassumed to be specific to the targets. Hence they must correspond innumber. See :ref:`ridge_regression` for formula.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "kernel": {
                "type": "string",
                "desc": "Kernel mapping used internally. This parameter is directly passed to:class:`~sklearn.metrics.pairwise.pairwise_kernel`.If `kernel` is a string, it must be one of the metricsin `pairwise.PAIRWISE_KERNEL_FUNCTIONS`.If `kernel` is \"precomputed\", X is assumed to be a kernel matrix.Alternatively, if `kernel` is a callable function, it is called oneach pair of instances (rows) and the resulting value recorded. Thecallable should take two rows from X as input and return thecorresponding kernel value as a single number. This means thatcallables from :mod:`sklearn.metrics.pairwise` are not allowed, asthey operate on matrices, not single samples. Use the stringidentifying the kernel instead.",
                "default": "\"linear\"",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "number",
                "desc": "Gamma parameter for the RBF, laplacian, polynomial, exponential chi2and sigmoid kernels. Interpretation of the default value is left tothe kernel; see the documentation for sklearn.metrics.pairwise.Ignored by other kernels.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel. Ignored by other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Zero coefficient for polynomial and sigmoid kernels.Ignored by other kernels.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "kernel_params": {
                "type": "mapping of string to any, default=None",
                "desc": "Additional parameters (keyword arguments) for kernel function passedas callable object.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.kernel_ridge.KernelRidge"
        }
    },
    "sklearn.kernel_ridge.deprecated": {
        "cls": "Block",
        "typename": "deprecated",
        "desc": "Decorator to mark a function or class as deprecated.  Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.  The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:  >>> from sklearn.utils import deprecated >>> deprecated() <sklearn.utils.deprecation.deprecated object at ...>  >>> @deprecated() ... def some_function(): pass",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.kernel_ridge",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "extra": {
                "type": "string",
                "desc": "",
                "default": "''",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.kernel_ridge.deprecated"
        }
    },
    "sklearn.linear_model.ARDRegression": {
        "cls": "Block",
        "typename": "ARDRegression",
        "desc": "Bayesian ARD regression.  Fit the weights of a regression model, using an ARD prior. The weights of the regression model are assumed to be in Gaussian distributions. Also estimate the parameters lambda (precisions of the distributions of the weights) and alpha (precision of the distribution of the noise). The estimation is done by an iterative procedures (Evidence Maximization)  Read more in the :ref:`User Guide <bayesian_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_iter": {
                "type": "number",
                "desc": "Maximum number of iterations.",
                "default": "300",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Stop the algorithm if w has converged.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "alpha_1": {
                "type": "number",
                "desc": "Hyper-parameter : shape parameter for the Gamma distribution priorover the alpha parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "alpha_2": {
                "type": "number",
                "desc": "Hyper-parameter : inverse scale parameter (rate parameter) for theGamma distribution prior over the alpha parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "lambda_1": {
                "type": "number",
                "desc": "Hyper-parameter : shape parameter for the Gamma distribution priorover the lambda parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "lambda_2": {
                "type": "number",
                "desc": "Hyper-parameter : inverse scale parameter (rate parameter) for theGamma distribution prior over the lambda parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "compute_score": {
                "type": "boolean",
                "desc": "If True, compute the objective function at each step of the model.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "threshold_lambda": {
                "type": "number",
                "desc": "threshold for removing (pruning) weights with high precision fromthe computation.",
                "default": "10 000",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Verbose mode when fitting the model.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.ARDRegression"
        }
    },
    "sklearn.linear_model.BayesianRidge": {
        "cls": "Block",
        "typename": "BayesianRidge",
        "desc": "Bayesian ridge regression.  Fit a Bayesian ridge model. See the Notes section for details on this implementation and the optimization of the regularization parameters lambda (precision of the weights) and alpha (precision of the noise).  Read more in the :ref:`User Guide <bayesian_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_iter": {
                "type": "number",
                "desc": "Maximum number of iterations. Should be greater than or equal to 1.",
                "default": "300",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Stop the algorithm if w has converged.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "alpha_1": {
                "type": "number",
                "desc": "Hyper-parameter : shape parameter for the Gamma distribution priorover the alpha parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "alpha_2": {
                "type": "number",
                "desc": "Hyper-parameter : inverse scale parameter (rate parameter) for theGamma distribution prior over the alpha parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "lambda_1": {
                "type": "number",
                "desc": "Hyper-parameter : shape parameter for the Gamma distribution priorover the lambda parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "lambda_2": {
                "type": "number",
                "desc": "Hyper-parameter : inverse scale parameter (rate parameter) for theGamma distribution prior over the lambda parameter.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "alpha_init": {
                "type": "number",
                "desc": "Initial value for alpha (precision of the noise).If not set, alpha_init is 1/Var(y)... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "lambda_init": {
                "type": "number",
                "desc": "Initial value for lambda (precision of the weights).If not set, lambda_init is 1... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "compute_score": {
                "type": "boolean",
                "desc": "If True, compute the log marginal likelihood at each iteration of theoptimization.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model.The intercept is not treated as a probabilistic parameterand thus has no associated variance. If setto False, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Verbose mode when fitting the model.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.BayesianRidge"
        }
    },
    "sklearn.linear_model.ElasticNet": {
        "cls": "Block",
        "typename": "ElasticNet",
        "desc": "Linear regression with combined L1 and L2 priors as regularizer.  Minimizes the objective function::          1 / (2 * n_samples) * ||y - Xw||^2_2         + alpha * l1_ratio * ||w||_1         + 0.5 * alpha * (1 - l1_ratio) * ||w||^2_2  If you are interested in controlling the L1 and L2 penalty separately, keep in mind that this is equivalent to::          a * ||w||_1 + 0.5 * b * ||w||_2^2  where::          alpha = a + b and l1_ratio = a / (a + b)  The parameter l1_ratio corresponds to alpha in the glmnet R package while alpha corresponds to the lambda parameter in glmnet. Specifically, l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not reliable, unless you supply your own sequence of alpha.  Read more in the :ref:`User Guide <elastic_net>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the penalty terms. Defaults to 1.0.See the notes for the exact mathematical meaning of thisparameter. ``alpha = 0`` is equivalent to an ordinary least square,solved by the :class:`LinearRegression` object. For numericalreasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.Given this, you should use the :class:`LinearRegression` object.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` itis an L1 penalty. For ``0 < l1_ratio < 1``, the penalty is acombination of L1 and L2.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether the intercept should be estimated or not. If ``False``, thedata is assumed to be already centered.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "boolean",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. The Gram matrix can also be passed as argument.For sparse input this option is always ``False`` to preserve sparsity.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "When set to ``True``, forces the coefficients to be positive.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4.",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.ElasticNet"
        }
    },
    "sklearn.linear_model.ElasticNetCV": {
        "cls": "Block",
        "typename": "ElasticNetCV",
        "desc": "Elastic Net model with iterative fitting along a regularization path.  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <elastic_net>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "l1_ratio": {
                "type": "number",
                "desc": "float between 0 and 1 passed to ElasticNet (scaling betweenl1 and l2 penalties). For ``l1_ratio = 0``the penalty is an L2 penalty. For ``l1_ratio = 1`` it is an L1 penalty.For ``0 < l1_ratio < 1``, the penalty is a combination of L1 and L2This parameter can be a list, in which case the differentvalues are tested by cross-validation and the one giving the bestprediction score is used. Note that a good choice of list ofvalues for l1_ratio is often to put more values close to 1(i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,.9, .95, .99, 1]``.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "Length of the path. ``eps=1e-3`` means that``alpha_min / alpha_max = 1e-3``.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "n_alphas": {
                "type": "number",
                "desc": "Number of alphas along the regularization path, used for each l1_ratio.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "alphas": {
                "type": "ndarray, default=None",
                "desc": "List of alphas where to compute the models.If None alphas are set automatically.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "'auto', bool or array-like of shape (n_features, n_features),                 default='auto'",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. If set to ``'auto'`` let us decide. The Grammatrix can also be passed as argument.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- int, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For int/None inputs, :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Amount of verbosity.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "When set to ``True``, forces the coefficients to be positive.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4.",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.ElasticNetCV"
        }
    },
    "sklearn.linear_model.GammaRegressor": {
        "cls": "Block",
        "typename": "GammaRegressor",
        "desc": "Generalized Linear Model with a Gamma distribution.  This regressor uses the 'log' link function.  Read more in the :ref:`User Guide <Generalized_linear_regression>`.  .. versionadded:: 0.23",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the penalty term and thus determines theregularization strength. ``alpha = 0`` is equivalent to unpenalizedGLMs. In this case, the design matrix `X` must have full column rank(no collinearities).",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Specifies if a constant (a.k.a. bias or intercept) should beadded to the linear predictor (X @ coef + intercept).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximal number of iterations for the solver.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Stopping criterion. For the lbfgs solver,the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``where ``g_j`` is the j-th component of the gradient (derivative) ofthe objective function.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "If set to ``True``, reuse the solution of the previous call to ``fit``as initialization for ``coef_`` and ``intercept_`` .",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "For the lbfgs solver set verbose to any positive number for verbosity.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.GammaRegressor"
        }
    },
    "sklearn.linear_model.Hinge": {
        "cls": "Block",
        "typename": "Hinge",
        "desc": "Hinge loss for binary classification tasks with y in {-1,1}",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "threshold": {
                "type": "number",
                "desc": "Margin threshold. When threshold=1.0, one gets the loss used by SVM.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.Hinge"
        }
    },
    "sklearn.linear_model.HuberRegressor": {
        "cls": "Block",
        "typename": "HuberRegressor",
        "desc": "Linear regression model that is robust to outliers.  The Huber Regressor optimizes the squared loss for the samples where ``|(y - X'w) / sigma| < epsilon`` and the absolute loss for the samples where ``|(y - X'w) / sigma| > epsilon``, where w and sigma are parameters to be optimized. The parameter sigma makes sure that if y is scaled up or down by a certain factor, one does not need to rescale epsilon to achieve the same robustness. Note that this does not take into account the fact that the different features of X may be of different scales.  This makes sure that the loss function is not heavily influenced by the outliers while not completely ignoring their effect.  Read more in the :ref:`User Guide <huber_regression>`  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "epsilon": {
                "type": "number",
                "desc": "The parameter epsilon controls the number of samples that should beclassified as outliers. The smaller the epsilon, the more robust it isto outliers.",
                "default": "1.35",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations that``scipy.optimize.minimize(method=\"L-BFGS-B\")`` should run for.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Regularization parameter.",
                "default": "0.0001",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "This is useful if the stored attributes of a previously used modelhas to be reused. If set to False, then the coefficients willbe rewritten for every call to fit.See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether or not to fit the intercept. This can be set to Falseif the data is already centered around the origin.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The iteration will stop when``max{|proj g_i | i = 1, ..., n}`` <= ``tol``where pg_i is the i-th component of the projected gradient.",
                "default": "1e-05",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.HuberRegressor"
        }
    },
    "sklearn.linear_model.Lars": {
        "cls": "Block",
        "typename": "Lars",
        "desc": "Least Angle Regression model a.k.a. LAR  Read more in the :ref:`User Guide <least_angle_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Sets the verbosity amount.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "boolean",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. If set to ``'auto'`` let us decide. The Grammatrix can also be passed as argument.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "n_nonzero_coefs": {
                "type": "number",
                "desc": "Target number of non-zero coefficients. Use ``np.inf`` for no limit.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "The machine-precision regularization in the computation of theCholesky diagonal factors. Increase this for very ill-conditionedsystems. Unlike the ``tol`` parameter in some iterativeoptimization-based algorithms, this parameter does not controlthe tolerance of the optimization.",
                "default": "np.finfo(float).eps",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "fit_path": {
                "type": "boolean",
                "desc": "If True the full path is stored in the ``coef_path_`` attribute.If you compute the solution for a large problem or many targets,setting ``fit_path`` to ``False`` will lead to a speedup, especiallywith a small alpha.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "jitter": {
                "type": "number",
                "desc": "Upper bound on a uniform noise parameter to be added to the`y` values, to satisfy the model's assumption ofone-at-a-time computations. Might help with stability... versionadded:: 0.23",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for jittering. Pass an intfor reproducible output across multiple function calls.See :term:`Glossary <random_state>`. Ignored if `jitter` is None... versionadded:: 0.23",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.Lars"
        }
    },
    "sklearn.linear_model.LarsCV": {
        "cls": "Block",
        "typename": "LarsCV",
        "desc": "Cross-validated Least Angle Regression model.  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <least_angle_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Sets the verbosity amount.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "boolean",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. If set to ``'auto'`` let us decide. The Gram matrixcannot be passed as argument since we will use only subsets of X.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- integer, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_n_alphas": {
                "type": "number",
                "desc": "The maximum number of points on the path used to compute theresiduals in the cross-validation",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "The machine-precision regularization in the computation of theCholesky diagonal factors. Increase this for very ill-conditionedsystems. Unlike the ``tol`` parameter in some iterativeoptimization-based algorithms, this parameter does not controlthe tolerance of the optimization.",
                "default": "np.finfo(float).eps",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LarsCV"
        }
    },
    "sklearn.linear_model.Lasso": {
        "cls": "Block",
        "typename": "Lasso",
        "desc": "Linear Model trained with L1 prior as regularizer (aka the Lasso)  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Technically the Lasso model is optimizing the same objective function as the Elastic Net with ``l1_ratio=1.0`` (no L2 penalty).  Read more in the :ref:`User Guide <lasso>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the L1 term. Defaults to 1.0.``alpha = 0`` is equivalent to an ordinary least square, solvedby the :class:`LinearRegression` object. For numericalreasons, using ``alpha = 0`` with the ``Lasso`` object is not advised.Given this, you should use the :class:`LinearRegression` object.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto False, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "boolean",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. The Gram matrix can also be passed as argument.For sparse input this option is always ``False`` to preserve sparsity.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "When set to ``True``, forces the coefficients to be positive.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4.",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.Lasso"
        }
    },
    "sklearn.linear_model.LassoCV": {
        "cls": "Block",
        "typename": "LassoCV",
        "desc": "Lasso linear model with iterative fitting along a regularization path.  See glossary entry for :term:`cross-validation estimator`.  The best model is selected by cross-validation.  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <lasso>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "eps": {
                "type": "number",
                "desc": "Length of the path. ``eps=1e-3`` means that``alpha_min / alpha_max = 1e-3``.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "n_alphas": {
                "type": "number",
                "desc": "Number of alphas along the regularization path.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "alphas": {
                "type": "ndarray, default=None",
                "desc": "List of alphas where to compute the models.If ``None`` alphas are set automatically.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "'auto', bool or array-like of shape (n_features, n_features),                 default='auto'",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. If set to ``'auto'`` let us decide. The Grammatrix can also be passed as argument.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- int, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For int/None inputs, :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Amount of verbosity.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "If positive, restrict regression coefficients to be positive.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4.",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LassoCV"
        }
    },
    "sklearn.linear_model.LassoLars": {
        "cls": "Block",
        "typename": "LassoLars",
        "desc": "Lasso model fit with Least Angle Regression a.k.a. Lars  It is a Linear Model trained with an L1 prior as regularizer.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the penalty term. Defaults to 1.0.``alpha = 0`` is equivalent to an ordinary least square, solvedby :class:`LinearRegression`. For numerical reasons, using``alpha = 0`` with the LassoLars object is not advised and youshould prefer the LinearRegression object.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Sets the verbosity amount.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "boolean",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. If set to ``'auto'`` let us decide. The Grammatrix can also be passed as argument.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "The machine-precision regularization in the computation of theCholesky diagonal factors. Increase this for very ill-conditionedsystems. Unlike the ``tol`` parameter in some iterativeoptimization-based algorithms, this parameter does not controlthe tolerance of the optimization.",
                "default": "np.finfo(float).eps",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "fit_path": {
                "type": "boolean",
                "desc": "If ``True`` the full path is stored in the ``coef_path_`` attribute.If you compute the solution for a large problem or many targets,setting ``fit_path`` to ``False`` will lead to a speedup, especiallywith a small alpha.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "Restrict coefficients to be >= 0. Be aware that you might want toremove fit_intercept which is set True by default.Under the positive restriction the model coefficients will not convergeto the ordinary-least-squares solution for small values of alpha.Only coefficients up to the smallest alpha value (``alphas_[alphas_ >0.].min()`` when fit_path=True) reached by the stepwise Lars-Lassoalgorithm are typically in congruence with the solution of thecoordinate descent Lasso estimator.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "jitter": {
                "type": "number",
                "desc": "Upper bound on a uniform noise parameter to be added to the`y` values, to satisfy the model's assumption ofone-at-a-time computations. Might help with stability... versionadded:: 0.23",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for jittering. Pass an intfor reproducible output across multiple function calls.See :term:`Glossary <random_state>`. Ignored if `jitter` is None... versionadded:: 0.23",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LassoLars"
        }
    },
    "sklearn.linear_model.LassoLarsCV": {
        "cls": "Block",
        "typename": "LassoLarsCV",
        "desc": "Cross-validated Lasso, using the LARS algorithm.  See glossary entry for :term:`cross-validation estimator`.  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  Read more in the :ref:`User Guide <least_angle_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Sets the verbosity amount.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "boolean",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. If set to ``'auto'`` let us decide. The Gram matrixcannot be passed as argument since we will use only subsets of X.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- integer, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_n_alphas": {
                "type": "number",
                "desc": "The maximum number of points on the path used to compute theresiduals in the cross-validation",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "The machine-precision regularization in the computation of theCholesky diagonal factors. Increase this for very ill-conditionedsystems. Unlike the ``tol`` parameter in some iterativeoptimization-based algorithms, this parameter does not controlthe tolerance of the optimization.",
                "default": "np.finfo(float).eps",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "Restrict coefficients to be >= 0. Be aware that you might want toremove fit_intercept which is set True by default.Under the positive restriction the model coefficients do not convergeto the ordinary-least-squares solution for small values of alpha.Only coefficients up to the smallest alpha value (``alphas_[alphas_ >0.].min()`` when fit_path=True) reached by the stepwise Lars-Lassoalgorithm are typically in congruence with the solution of thecoordinate descent Lasso estimator.As a consequence using LassoLarsCV only makes sense for problems wherea sparse solution is expected and/or reached.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LassoLarsCV"
        }
    },
    "sklearn.linear_model.LassoLarsIC": {
        "cls": "Block",
        "typename": "LassoLarsIC",
        "desc": "Lasso model fit with Lars using BIC or AIC for model selection  The optimization objective for Lasso is::  (1 / (2 * n_samples)) * ||y - Xw||^2_2 + alpha * ||w||_1  AIC is the Akaike information criterion and BIC is the Bayes Information criterion. Such criteria are useful to select the value of the regularization parameter by making a trade-off between the goodness of fit and the complexity of the model. A good model should explain well the data while being simple.  Read more in the :ref:`User Guide <least_angle_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "criterion": {
                "type": "option(bic , aic)",
                "desc": "The type of criterion to use.",
                "default": "'aic'",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Sets the verbosity amount.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "boolean",
                "desc": "Whether to use a precomputed Gram matrix to speed upcalculations. If set to ``'auto'`` let us decide. The Grammatrix can also be passed as argument.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations to perform. Can be used forearly stopping.",
                "default": "500",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "The machine-precision regularization in the computation of theCholesky diagonal factors. Increase this for very ill-conditionedsystems. Unlike the ``tol`` parameter in some iterativeoptimization-based algorithms, this parameter does not controlthe tolerance of the optimization.",
                "default": "np.finfo(float).eps",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "Restrict coefficients to be >= 0. Be aware that you might want toremove fit_intercept which is set True by default.Under the positive restriction the model coefficients do not convergeto the ordinary-least-squares solution for small values of alpha.Only coefficients up to the smallest alpha value (``alphas_[alphas_ >0.].min()`` when fit_path=True) reached by the stepwise Lars-Lassoalgorithm are typically in congruence with the solution of thecoordinate descent Lasso estimator.As a consequence using LassoLarsIC only makes sense for problems wherea sparse solution is expected and/or reached.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LassoLarsIC"
        }
    },
    "sklearn.linear_model.LinearRegression": {
        "cls": "Block",
        "typename": "LinearRegression",
        "desc": "Ordinary least squares Linear Regression.  LinearRegression fits a linear model with coefficients w = (w1, ..., wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto False, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This will only providespeedup for n_targets > 1 and sufficient large problems.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "positive": {
                "type": "boolean",
                "desc": "When set to ``True``, forces the coefficients to be positive. Thisoption is only supported for dense arrays... versionadded:: 0.24",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LinearRegression"
        }
    },
    "sklearn.linear_model.LogisticRegression": {
        "cls": "Block",
        "typename": "LogisticRegression",
        "desc": "Logistic Regression (aka logit, MaxEnt) classifier.  In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the 'multi_class' option is set to 'ovr', and uses the cross-entropy loss if the 'multi_class' option is set to 'multinomial'. (Currently the 'multinomial' option is supported only by the 'lbfgs', 'sag', 'saga' and 'newton-cg' solvers.)  This class implements regularized logistic regression using the 'liblinear' library, 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers. **Note that regularization is applied by default**. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).  The 'newton-cg', 'sag', and 'lbfgs' solvers support only L2 regularization with primal formulation, or no regularization. The 'liblinear' solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the 'saga' solver.  Read more in the :ref:`User Guide <logistic_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "penalty": {
                "type": "option(l1, l2, elasticnet, none)",
                "desc": "Used to specify the norm used in the penalization. The 'newton-cg','sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' isonly supported by the 'saga' solver. If 'none' (not supported by theliblinear solver), no regularization is applied... versionadded:: 0.19l1 penalty with SAGA solver (allowing 'multinomial' + L1)",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "dual": {
                "type": "boolean",
                "desc": "Dual or primal formulation. Dual formulation is only implemented forl2 penalty with liblinear solver. Prefer dual=False whenn_samples > n_features.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criteria.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "C": {
                "type": "number",
                "desc": "Inverse of regularization strength; must be a positive float.Like in support vector machines, smaller values specify strongerregularization.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Specifies if a constant (a.k.a. bias or intercept) should beadded to the decision function.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "intercept_scaling": {
                "type": "number",
                "desc": "Useful only when the solver 'liblinear' is usedand self.fit_intercept is set to True. In this case, x becomes[x, self.intercept_scaling],i.e. a \"synthetic\" feature with constant value equal tointercept_scaling is appended to the instance vector.The intercept becomes ``intercept_scaling * synthetic_feature_weight``.Note! the synthetic feature weight is subject to l1/l2 regularizationas all other features.To lessen the effect of regularization on synthetic feature weight(and therefore on the intercept) intercept_scaling has to be increased.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict or 'balanced', default=None",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If not given, all classes are supposed to have weight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified... versionadded:: 0.17*class_weight='balanced'*",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used when ``solver`` == 'sag', 'saga' or 'liblinear' to shuffle thedata. See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "solver": {
                "type": "option(newton-cg, lbfgs, liblinear, sag, saga)",
                "desc": "Algorithm to use in the optimization problem.- For small datasets, 'liblinear' is a good choice, whereas 'sag' and'saga' are faster for large ones.- For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'handle multinomial loss; 'liblinear' is limited to one-versus-restschemes.- 'newton-cg', 'lbfgs', 'sag' and 'saga' handle L2 or no penalty- 'liblinear' and 'saga' also handle L1 penalty- 'saga' also supports 'elasticnet' penalty- 'liblinear' does not support setting ``penalty='none'``Note that 'sag' and 'saga' fast convergence is only guaranteed onfeatures with approximately the same scale. You canpreprocess the data with a scaler from sklearn.preprocessing... versionadded:: 0.17Stochastic Average Gradient descent solver... versionadded:: 0.19SAGA solver... versionchanged:: 0.22The default solver changed from 'liblinear' to 'lbfgs' in 0.22.",
                "default": "'lbfgs'",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations taken for the solvers to converge.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "multi_class": {
                "type": "option(auto, ovr, multinomial)",
                "desc": "If the option chosen is 'ovr', then a binary problem is fit for eachlabel. For 'multinomial' the loss minimised is the multinomial loss fitacross the entire probability distribution, *even when the data isbinary*. 'multinomial' is unavailable when solver='liblinear'.'auto' selects 'ovr' if the data is binary, or if solver='liblinear',and otherwise selects 'multinomial'... versionadded:: 0.18Stochastic Average Gradient descent solver for 'multinomial' case... versionchanged:: 0.22Default changed from 'ovr' to 'auto' in 0.22.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "For the liblinear and lbfgs solvers set verbose to any positivenumber for verbosity.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.Useless for liblinear solver. See :term:`the Glossary <warm_start>`... versionadded:: 0.17*warm_start* to support *lbfgs*, *newton-cg*, *sag*, *saga* solvers.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPU cores used when parallelizing over classes ifmulti_class='ovr'\". This parameter is ignored when the ``solver`` isset to 'liblinear' regardless of whether 'multi_class' is specified ornot. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`context. ``-1`` means using all processors.See :term:`Glossary <n_jobs>` for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``. Onlyused if ``penalty='elasticnet'``. Setting ``l1_ratio=0`` is equivalentto using ``penalty='l2'``, while setting ``l1_ratio=1`` is equivalentto using ``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is acombination of L1 and L2.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LogisticRegression"
        }
    },
    "sklearn.linear_model.LogisticRegressionCV": {
        "cls": "Block",
        "typename": "LogisticRegressionCV",
        "desc": "Logistic Regression CV (aka logit, MaxEnt) classifier.  See glossary entry for :term:`cross-validation estimator`.  This class implements logistic regression using liblinear, newton-cg, sag of lbfgs optimizer. The newton-cg, sag and lbfgs solvers support only L2 regularization with primal formulation. The liblinear solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. Elastic-Net penalty is only supported by the saga solver.  For the grid of `Cs` values and `l1_ratios` values, the best hyperparameter is selected by the cross-validator :class:`~sklearn.model_selection.StratifiedKFold`, but it can be changed using the :term:`cv` parameter. The 'newton-cg', 'sag', 'saga' and 'lbfgs' solvers can warm-start the coefficients (see :term:`Glossary<warm_start>`).  Read more in the :ref:`User Guide <logistic_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "Cs": {
                "type": "number",
                "desc": "Each of the values in Cs describes the inverse of regularizationstrength. If Cs is as an int, then a grid of Cs values are chosenin a logarithmic scale between 1e-4 and 1e4.Like in support vector machines, smaller values specify strongerregularization.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Specifies if a constant (a.k.a. bias or intercept) should beadded to the decision function.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "The default cross-validation generator used is Stratified K-Folds.If an integer is provided, then it is the number of folds used.See the module :mod:`sklearn.model_selection` module for thelist of possible cross-validation objects... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "dual": {
                "type": "boolean",
                "desc": "Dual or primal formulation. Dual formulation is only implemented forl2 penalty with liblinear solver. Prefer dual=False whenn_samples > n_features.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "penalty": {
                "type": "option(l1, l2, elasticnet)",
                "desc": "Used to specify the norm used in the penalization. The 'newton-cg','sag' and 'lbfgs' solvers support only l2 penalties. 'elasticnet' isonly supported by the 'saga' solver.",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "scoring": {
                "type": "string",
                "desc": "A string (see model evaluation documentation) ora scorer callable object / function with signature``scorer(estimator, X, y)``. For a list of scoring functionsthat can be used, look at :mod:`sklearn.metrics`. Thedefault scoring option used is 'accuracy'.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "solver": {
                "type": "option(newton-cg, lbfgs, liblinear, sag, saga)",
                "desc": "Algorithm to use in the optimization problem.- For small datasets, 'liblinear' is a good choice, whereas 'sag' and'saga' are faster for large ones.- For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs'handle multinomial loss; 'liblinear' is limited to one-versus-restschemes.- 'newton-cg', 'lbfgs' and 'sag' only handle L2 penalty, whereas'liblinear' and 'saga' handle L1 penalty.- 'liblinear' might be slower in LogisticRegressionCV because it doesnot handle warm-starting.Note that 'sag' and 'saga' fast convergence is only guaranteed onfeatures with approximately the same scale. You can preprocess the datawith a scaler from sklearn.preprocessing... versionadded:: 0.17Stochastic Average Gradient descent solver... versionadded:: 0.19SAGA solver.",
                "default": "'lbfgs'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criteria.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations of the optimization algorithm.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict or 'balanced', default=None",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If not given, all classes are supposed to have weight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified... versionadded:: 0.17class_weight == 'balanced'",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPU cores used during the cross-validation loop.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "For the 'liblinear', 'sag' and 'lbfgs' solvers set verbose to anypositive number for verbosity.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "refit": {
                "type": "boolean",
                "desc": "If set to True, the scores are averaged across all folds, and thecoefs and the C that corresponds to the best score is taken, and afinal refit is done using these parameters.Otherwise the coefs, intercepts and C that correspond to thebest scores across folds are averaged.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "intercept_scaling": {
                "type": "number",
                "desc": "Useful only when the solver 'liblinear' is usedand self.fit_intercept is set to True. In this case, x becomes[x, self.intercept_scaling],i.e. a \"synthetic\" feature with constant value equal tointercept_scaling is appended to the instance vector.The intercept becomes ``intercept_scaling * synthetic_feature_weight``.Note! the synthetic feature weight is subject to l1/l2 regularizationas all other features.To lessen the effect of regularization on synthetic feature weight(and therefore on the intercept) intercept_scaling has to be increased.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "multi_class": {
                "type": "option(auto, ovr, multinomial)",
                "desc": "If the option chosen is 'ovr', then a binary problem is fit for eachlabel. For 'multinomial' the loss minimised is the multinomial loss fitacross the entire probability distribution, *even when the data isbinary*. 'multinomial' is unavailable when solver='liblinear'.'auto' selects 'ovr' if the data is binary, or if solver='liblinear',and otherwise selects 'multinomial'... versionadded:: 0.18Stochastic Average Gradient descent solver for 'multinomial' case... versionchanged:: 0.22Default changed from 'ovr' to 'auto' in 0.22.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used when `solver='sag'`, 'saga' or 'liblinear' to shuffle the data.Note that this only applies to the solver and not the cross-validationgenerator. See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "l1_ratios": {
                "type": "list of float, default=None",
                "desc": "The list of Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``.Only used if ``penalty='elasticnet'``. A value of 0 is equivalent tousing ``penalty='l2'``, while 1 is equivalent to using``penalty='l1'``. For ``0 < l1_ratio <1``, the penalty is a combinationof L1 and L2.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.LogisticRegressionCV"
        }
    },
    "sklearn.linear_model.MultiTaskElasticNet": {
        "cls": "Block",
        "typename": "MultiTaskElasticNet",
        "desc": "Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer.  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||_Fro^2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = sum_i sqrt(sum_j W_ij ^ 2)  i.e. the sum of norms of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the L1/L2 term. Defaults to 1.0.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 itis an L2 penalty.For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4.",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.MultiTaskElasticNet"
        }
    },
    "sklearn.linear_model.MultiTaskElasticNetCV": {
        "cls": "Block",
        "typename": "MultiTaskElasticNetCV",
        "desc": "Multi-task L1/L2 ElasticNet with built-in cross-validation.  See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskElasticNet is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2     + alpha * l1_ratio * ||W||_21     + 0.5 * alpha * (1 - l1_ratio) * ||W||_Fro^2  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_elastic_net>`.  .. versionadded:: 0.15",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The ElasticNet mixing parameter, with 0 < l1_ratio <= 1.For l1_ratio = 1 the penalty is an L1/L2 penalty. For l1_ratio = 0 itis an L2 penalty.For ``0 < l1_ratio < 1``, the penalty is a combination of L1/L2 and L2.This parameter can be a list, in which case the differentvalues are tested by cross-validation and the one giving the bestprediction score is used. Note that a good choice of list ofvalues for l1_ratio is often to put more values close to 1(i.e. Lasso) and less close to 0 (i.e. Ridge), as in ``[.1, .5, .7,.9, .95, .99, 1]``",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "Length of the path. ``eps=1e-3`` means that``alpha_min / alpha_max = 1e-3``.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "n_alphas": {
                "type": "number",
                "desc": "Number of alphas along the regularization path.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "alphas": {
                "type": "array-like, default=None",
                "desc": "List of alphas where to compute the models.If not provided, set automatically.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- int, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For int/None inputs, :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Amount of verbosity.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation. Note that this isused only if multiple values for l1_ratio are given.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4.",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.MultiTaskElasticNetCV"
        }
    },
    "sklearn.linear_model.MultiTaskLasso": {
        "cls": "Block",
        "typename": "MultiTaskLasso",
        "desc": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.  The optimization objective for Lasso is::      (1 / (2 * n_samples)) * ||Y - XW||^2_Fro + alpha * ||W||_21  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the L1/L2 term. Defaults to 1.0.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to ``True``, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.MultiTaskLasso"
        }
    },
    "sklearn.linear_model.MultiTaskLassoCV": {
        "cls": "Block",
        "typename": "MultiTaskLassoCV",
        "desc": "Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer.  See glossary entry for :term:`cross-validation estimator`.  The optimization objective for MultiTaskLasso is::      (1 / (2 * n_samples)) * ||Y - XW||^Fro_2 + alpha * ||W||_21  Where::      ||W||_21 = \\sum_i \\sqrt{\\sum_j w_{ij}^2}  i.e. the sum of norm of each row.  Read more in the :ref:`User Guide <multi_task_lasso>`.  .. versionadded:: 0.15",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "eps": {
                "type": "number",
                "desc": "Length of the path. ``eps=1e-3`` means that``alpha_min / alpha_max = 1e-3``.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "n_alphas": {
                "type": "number",
                "desc": "Number of alphas along the regularization path.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "alphas": {
                "type": "array-like, default=None",
                "desc": "List of alphas where to compute the models.If not provided, set automatically.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The tolerance for the optimization: if the updates aresmaller than ``tol``, the optimization code checks thedual gap for optimality and continues until it is smallerthan ``tol``.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If ``True``, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- int, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For int/None inputs, :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Amount of verbosity.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation. Note that this isused only if multiple values for l1_ratio are given.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The seed of the pseudo random number generator that selects a randomfeature to update. Used when ``selection`` == 'random'.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "selection": {
                "type": "option(cyclic, random)",
                "desc": "If set to 'random', a random coefficient is updated every iterationrather than looping over features sequentially by default. This(setting to 'random') often leads to significantly faster convergenceespecially when tol is higher than 1e-4.",
                "default": "'cyclic'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.MultiTaskLassoCV"
        }
    },
    "sklearn.linear_model.OrthogonalMatchingPursuit": {
        "cls": "Block",
        "typename": "OrthogonalMatchingPursuit",
        "desc": "Orthogonal Matching Pursuit model (OMP).  Read more in the :ref:`User Guide <omp>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_nonzero_coefs": {
                "type": "number",
                "desc": "Desired number of non-zero entries in the solution. If None (bydefault) this value is set to 10% of n_features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Maximum norm of the residual. If not None, overrides n_nonzero_coefs.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "precompute": {
                "type": "'auto' or bool, default='auto'",
                "desc": "Whether to use a precomputed Gram and Xy matrix to speed upcalculations. Improves performance when :term:`n_targets` or:term:`n_samples` is very large. Note that if you already have suchmatrices, you can pass them directly to the fit method.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.OrthogonalMatchingPursuit"
        }
    },
    "sklearn.linear_model.OrthogonalMatchingPursuitCV": {
        "cls": "Block",
        "typename": "OrthogonalMatchingPursuitCV",
        "desc": "Cross-validated Orthogonal Matching Pursuit model (OMP).  See glossary entry for :term:`cross-validation estimator`.  Read more in the :ref:`User Guide <omp>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "copy": {
                "type": "boolean",
                "desc": "Whether the design matrix X must be copied by the algorithm. A falsevalue is only helpful if X is already Fortran-ordered, otherwise acopy is made anyway.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum numbers of iterations to perform, therefore maximum featuresto include. 10% of ``n_features`` but at least 5 if available.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross-validation,- integer, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, :class:`KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Sets the verbosity amount.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.OrthogonalMatchingPursuitCV"
        }
    },
    "sklearn.linear_model.PassiveAggressiveClassifier": {
        "cls": "Block",
        "typename": "PassiveAggressiveClassifier",
        "desc": "Passive Aggressive Classifier  Read more in the :ref:`User Guide <passive_aggressive>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "C": {
                "type": "number",
                "desc": "Maximum step size (regularization). Defaults to 1.0.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether the intercept should be estimated or not. If False, thedata is assumed to be already centered.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of passes over the training data (aka epochs).It only impacts the behavior in the ``fit`` method, and not the:meth:`partial_fit` method... versionadded:: 0.19",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The stopping criterion. If it is not None, the iterations will stopwhen (loss > previous_loss - tol)... versionadded:: 0.19",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "early_stopping": {
                "type": "boolean",
                "desc": "Whether to use early stopping to terminate training when validation.score is not improving. If set to True, it will automatically set asidea stratified fraction of training data as validation and terminatetraining when validation score is not improving by at least tol forn_iter_no_change consecutive epochs... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if early_stopping is True... versionadded:: 0.20",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "Number of iterations with no improvement to wait before early stopping... versionadded:: 0.20",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether or not the training data should be shuffled after each epoch.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "The verbosity level",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "loss": {
                "type": "string",
                "desc": "The loss function to be used:hinge: equivalent to PA-I in the reference paper.squared_hinge: equivalent to PA-II in the reference paper.",
                "default": "\"hinge\"",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of CPUs to use to do the OVA (One Versus All, formulti-class problems) computation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used to shuffle the training data, when ``shuffle`` is set to``True``. Pass an int for reproducible output across multiplefunction calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.Repeatedly calling fit or partial_fit when warm_start is True canresult in a different solution than when calling fit a single timebecause of the way the data is shuffled.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict, {class_label: weight} or \"balanced\" or None,             default=None",
                "desc": "Preset for the class_weight fit parameter.Weights associated with classes. If not given, all classesare supposed to have weight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``.. versionadded:: 0.17parameter *class_weight* to automatically weight samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "average": {
                "type": "boolean",
                "desc": "When set to True, computes the averaged SGD weights and stores theresult in the ``coef_`` attribute. If set to an int greater than 1,averaging will begin once the total number of samples seen reachesaverage. So average=10 will begin averaging after seeing 10 samples... versionadded:: 0.19parameter *average* to use weights averaging in SGD",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.PassiveAggressiveClassifier"
        }
    },
    "sklearn.linear_model.PassiveAggressiveRegressor": {
        "cls": "Block",
        "typename": "PassiveAggressiveRegressor",
        "desc": "Passive Aggressive Regressor  Read more in the :ref:`User Guide <passive_aggressive>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "C": {
                "type": "number",
                "desc": "Maximum step size (regularization). Defaults to 1.0.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether the intercept should be estimated or not. If False, thedata is assumed to be already centered. Defaults to True.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of passes over the training data (aka epochs).It only impacts the behavior in the ``fit`` method, and not the:meth:`partial_fit` method... versionadded:: 0.19",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The stopping criterion. If it is not None, the iterations will stopwhen (loss > previous_loss - tol)... versionadded:: 0.19",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "early_stopping": {
                "type": "boolean",
                "desc": "Whether to use early stopping to terminate training when validation.score is not improving. If set to True, it will automatically set asidea fraction of training data as validation and terminatetraining when validation score is not improving by at least tol forn_iter_no_change consecutive epochs... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if early_stopping is True... versionadded:: 0.20",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "Number of iterations with no improvement to wait before early stopping... versionadded:: 0.20",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether or not the training data should be shuffled after each epoch.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "The verbosity level",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "loss": {
                "type": "string",
                "desc": "The loss function to be used:epsilon_insensitive: equivalent to PA-I in the reference paper.squared_epsilon_insensitive: equivalent to PA-II in the referencepaper.",
                "default": "\"epsilon_insensitive\"",
                "dictKeyOf": "initkargs"
            },
            "epsilon": {
                "type": "number",
                "desc": "If the difference between the current prediction and the correct labelis below this threshold, the model is not updated.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used to shuffle the training data, when ``shuffle`` is set to``True``. Pass an int for reproducible output across multiplefunction calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.Repeatedly calling fit or partial_fit when warm_start is True canresult in a different solution than when calling fit a single timebecause of the way the data is shuffled.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "average": {
                "type": "boolean",
                "desc": "When set to True, computes the averaged SGD weights and stores theresult in the ``coef_`` attribute. If set to an int greater than 1,averaging will begin once the total number of samples seen reachesaverage. So average=10 will begin averaging after seeing 10 samples... versionadded:: 0.19parameter *average* to use weights averaging in SGD",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.PassiveAggressiveRegressor"
        }
    },
    "sklearn.linear_model.Perceptron": {
        "cls": "Block",
        "typename": "Perceptron",
        "desc": "Perceptron  Read more in the :ref:`User Guide <perceptron>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "penalty": {
                "type": "option(l2,l1,elasticnet)",
                "desc": "The penalty (aka regularization term) to be used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the regularization term if regularization isused.",
                "default": "0.0001",
                "dictKeyOf": "initkargs"
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.`l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.Only used if `penalty='elasticnet'`... versionadded:: 0.24",
                "default": "0.15",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether the intercept should be estimated or not. If False, thedata is assumed to be already centered.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of passes over the training data (aka epochs).It only impacts the behavior in the ``fit`` method, and not the:meth:`partial_fit` method... versionadded:: 0.19",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The stopping criterion. If it is not None, the iterations will stopwhen (loss > previous_loss - tol)... versionadded:: 0.19",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether or not the training data should be shuffled after each epoch.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "The verbosity level",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "eta0": {
                "type": "double, default=1",
                "desc": "Constant by which the updates are multiplied.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of CPUs to use to do the OVA (One Versus All, formulti-class problems) computation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used to shuffle the training data, when ``shuffle`` is set to``True``. Pass an int for reproducible output across multiplefunction calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "early_stopping": {
                "type": "boolean",
                "desc": "Whether to use early stopping to terminate training when validation.score is not improving. If set to True, it will automatically set asidea stratified fraction of training data as validation and terminatetraining when validation score is not improving by at least tol forn_iter_no_change consecutive epochs... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if early_stopping is True... versionadded:: 0.20",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "Number of iterations with no improvement to wait before early stopping... versionadded:: 0.20",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict, {class_label: weight} or \"balanced\", default=None",
                "desc": "Preset for the class_weight fit parameter.Weights associated with classes. If not given, all classesare supposed to have weight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution. See:term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.Perceptron"
        }
    },
    "sklearn.linear_model.PoissonRegressor": {
        "cls": "Block",
        "typename": "PoissonRegressor",
        "desc": "Generalized Linear Model with a Poisson distribution.  This regressor uses the 'log' link function.  Read more in the :ref:`User Guide <Generalized_linear_regression>`.  .. versionadded:: 0.23",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the penalty term and thus determines theregularization strength. ``alpha = 0`` is equivalent to unpenalizedGLMs. In this case, the design matrix `X` must have full column rank(no collinearities).",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Specifies if a constant (a.k.a. bias or intercept) should beadded to the linear predictor (X @ coef + intercept).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximal number of iterations for the solver.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Stopping criterion. For the lbfgs solver,the iteration will stop when ``max{|g_j|, j = 1, ..., d} <= tol``where ``g_j`` is the j-th component of the gradient (derivative) ofthe objective function.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "If set to ``True``, reuse the solution of the previous call to ``fit``as initialization for ``coef_`` and ``intercept_`` .",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "For the lbfgs solver set verbose to any positive number for verbosity.",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.PoissonRegressor"
        }
    },
    "sklearn.linear_model.RANSACRegressor": {
        "cls": "Parent",
        "typename": "RANSACRegressor",
        "desc": "RANSAC (RANdom SAmple Consensus) algorithm.  RANSAC is an iterative algorithm for the robust estimation of parameters from a subset of inliers from the complete data set.  Read more in the :ref:`User Guide <ransac_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "min_samples": {
                "type": "number",
                "desc": "Minimum number of samples chosen randomly from original data. Treatedas an absolute number of samples for `min_samples >= 1`, treated as arelative number `ceil(min_samples * X.shape[0]`) for`min_samples < 1`. This is typically chosen as the minimal number ofsamples necessary to estimate the given `base_estimator`. By default a``sklearn.linear_model.LinearRegression()`` estimator is assumed and`min_samples` is chosen as ``X.shape[1] + 1``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "residual_threshold": {
                "type": "number",
                "desc": "Maximum residual for a data sample to be classified as an inlier.By default the threshold is chosen as the MAD (median absolutedeviation) of the target values `y`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "is_data_valid": {
                "type": "callable, default=None",
                "desc": "This function is called with the randomly selected data before themodel is fitted to it: `is_data_valid(X, y)`. If its return value isFalse the current randomly chosen sub-sample is skipped.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "is_model_valid": {
                "type": "callable, default=None",
                "desc": "This function is called with the estimated model and the randomlyselected data: `is_model_valid(model, X, y)`. If its return value isFalse the current randomly chosen sub-sample is skipped.Rejecting samples with this function is computationally costlier thanwith `is_data_valid`. `is_model_valid` should therefore only be used ifthe estimated model is needed for making the rejection decision.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_trials": {
                "type": "number",
                "desc": "Maximum number of iterations for random sample selection.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "max_skips": {
                "type": "number",
                "desc": "Maximum number of iterations that can be skipped due to finding zeroinliers or invalid data defined by ``is_data_valid`` or invalid modelsdefined by ``is_model_valid``... versionadded:: 0.19",
                "default": "np.inf",
                "dictKeyOf": "initkargs"
            },
            "stop_n_inliers": {
                "type": "number",
                "desc": "Stop iteration if at least this number of inliers are found.",
                "default": "np.inf",
                "dictKeyOf": "initkargs"
            },
            "stop_score": {
                "type": "number",
                "desc": "Stop iteration if score is greater equal than this threshold.",
                "default": "np.inf",
                "dictKeyOf": "initkargs"
            },
            "stop_probability": {
                "type": "number",
                "desc": "RANSAC iteration stops if at least one outlier-free set of the trainingdata is sampled in RANSAC. This requires to generate at least Nsamples (iterations)::N >= log(1 - probability) / log(1 - e**m)where the probability (confidence) is typically set to high value suchas 0.99 (the default) and e is the current fraction of inliers w.r.t.the total number of samples.",
                "default": "0.99",
                "dictKeyOf": "initkargs"
            },
            "loss": {
                "type": "string",
                "desc": "String inputs, \"absolute_loss\" and \"squared_loss\" are supported whichfind the absolute loss and squared loss per samplerespectively.If ``loss`` is a callable, then it should be a function that takestwo arrays as inputs, the true and predicted value and returns a 1-Darray with the i-th value of the array corresponding to the losson ``X[i]``.If the loss on a sample is greater than the ``residual_threshold``,then this sample is classified as an outlier... versionadded:: 0.18",
                "default": "'absolute_loss'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The generator used to initialize the centers.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.RANSACRegressor",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.linear_model.Ridge": {
        "cls": "Block",
        "typename": "Ridge",
        "desc": "Linear least squares with l2 regularization.  Minimizes the objective function::  ||y - Xw||^2_2 + alpha * ||w||^2_2  This model solves a regression model where the loss function is the linear least squares function and regularization is given by the l2-norm. Also known as Ridge Regression or Tikhonov regularization. This estimator has built-in support for multi-variate regression (i.e., when y is a 2d-array of shape (n_samples, n_targets)).  Read more in the :ref:`User Guide <ridge_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "option(float, ndarray of shape (n_targets,))",
                "desc": "Regularization strength; must be a positive float. Regularizationimproves the conditioning of the problem and reduces the variance ofthe estimates. Larger values specify stronger regularization.Alpha corresponds to ``1 / (2C)`` in other linear models such as:class:`~sklearn.linear_model.LogisticRegression` or:class:`~sklearn.svm.LinearSVC`. If an array is passed, penalties areassumed to be specific to the targets. Hence they must correspond innumber.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to fit the intercept for this model. If setto false, no intercept will be used in calculations(i.e. ``X`` and ``y`` are expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations for conjugate gradient solver.For 'sparse_cg' and 'lsqr' solvers, the default value is determinedby scipy.sparse.linalg. For 'sag' solver, the default value is 1000.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Precision of the solution.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "solver": {
                "type": "option(auto, svd, cholesky, lsqr, sparse_cg, sag, saga)",
                "desc": "Solver to use in the computational routines:- 'auto' chooses the solver automatically based on the type of data.- 'svd' uses a Singular Value Decomposition of X to compute the Ridgecoefficients. More stable for singular matrices than 'cholesky'.- 'cholesky' uses the standard scipy.linalg.solve function toobtain a closed-form solution.- 'sparse_cg' uses the conjugate gradient solver as found inscipy.sparse.linalg.cg. As an iterative algorithm, this solver ismore appropriate than 'cholesky' for large-scale data(possibility to set `tol` and `max_iter`).- 'lsqr' uses the dedicated regularized least-squares routinescipy.sparse.linalg.lsqr. It is the fastest and uses an iterativeprocedure.- 'sag' uses a Stochastic Average Gradient descent, and 'saga' usesits improved, unbiased version named SAGA. Both methods also use aniterative procedure, and are often faster than other solvers whenboth n_samples and n_features are large. Note that 'sag' and'saga' fast convergence is only guaranteed on features withapproximately the same scale. You can preprocess the data with ascaler from sklearn.preprocessing.All last five solvers support both dense and sparse data. However, only'sag' and 'sparse_cg' supports sparse input when `fit_intercept` isTrue... versionadded:: 0.17Stochastic Average Gradient descent solver... versionadded:: 0.19SAGA solver.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used when ``solver`` == 'sag' or 'saga' to shuffle the data.See :term:`Glossary <random_state>` for details... versionadded:: 0.17`random_state` to support Stochastic Average Gradient.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.Ridge"
        }
    },
    "sklearn.linear_model.RidgeCV": {
        "cls": "Block",
        "typename": "RidgeCV",
        "desc": "Ridge regression with built-in cross-validation.  See glossary entry for :term:`cross-validation estimator`.  By default, it performs efficient Leave-One-Out Cross-Validation.  Read more in the :ref:`User Guide <ridge_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alphas": {
                "type": "ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)",
                "desc": "Array of alpha values to try.Regularization strength; must be a positive float. Regularizationimproves the conditioning of the problem and reduces the variance ofthe estimates. Larger values specify stronger regularization.Alpha corresponds to ``1 / (2C)`` in other linear models such as:class:`~sklearn.linear_model.LogisticRegression` or:class:`~sklearn.svm.LinearSVC`.If using Leave-One-Out cross-validation, alphas must be positive.",
                "default": "(0.1",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "scoring": {
                "type": "string",
                "desc": "A string (see model evaluation documentation) ora scorer callable object / function with signature``scorer(estimator, X, y)``.If None, the negative mean squared error if cv is 'auto' or None(i.e. when using leave-one-out cross-validation), and r2 scoreotherwise.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the efficient Leave-One-Out cross-validation- integer, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, if ``y`` is binary or multiclass,:class:`~sklearn.model_selection.StratifiedKFold` is used, else,:class:`~sklearn.model_selection.KFold` is used.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "gcv_mode": {
                "type": "option(auto, svd, eigen)",
                "desc": "Flag indicating which strategy to use when performingLeave-One-Out Cross-Validation. Options are::'auto' : use 'svd' if n_samples > n_features, otherwise use 'eigen''svd' : force use of singular value decomposition of X when X isdense, eigenvalue decomposition of X^T.X when X is sparse.'eigen' : force computation via eigendecomposition of X.X^TThe 'auto' mode is the default and is intended to pick the cheaperoption of the two depending on the shape of the training data.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "store_cv_values": {
                "type": "boolean",
                "desc": "Flag indicating if the cross-validation values corresponding toeach alpha should be stored in the ``cv_values_`` attribute (seebelow). This flag is only compatible with ``cv=None`` (i.e. usingLeave-One-Out Cross-Validation).",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "alpha_per_target": {
                "type": "boolean",
                "desc": "Flag indicating whether to optimize the alpha value (picked from the`alphas` parameter list) for each target separately (for multi-outputsettings: multiple prediction targets). When set to `True`, afterfitting, the `alpha_` attribute will contain a value for each target.When set to `False`, a single alpha is used for all targets... versionadded:: 0.24",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.RidgeCV"
        }
    },
    "sklearn.linear_model.RidgeClassifier": {
        "cls": "Block",
        "typename": "RidgeClassifier",
        "desc": "Classifier using Ridge regression.  This classifier first converts the target values into ``{-1, 1}`` and then treats the problem as a regression task (multi-output regression in the multiclass case).  Read more in the :ref:`User Guide <ridge_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Regularization strength; must be a positive float. Regularizationimproves the conditioning of the problem and reduces the variance ofthe estimates. Larger values specify stronger regularization.Alpha corresponds to ``1 / (2C)`` in other linear models such as:class:`~sklearn.linear_model.LogisticRegression` or:class:`~sklearn.svm.LinearSVC`.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If set to false, nointercept will be used in calculations (e.g. data is expected to bealready centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations for conjugate gradient solver.The default value is determined by scipy.sparse.linalg.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Precision of the solution.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict or 'balanced', default=None",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If not given, all classes are supposed to have weight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "solver": {
                "type": "option(auto, svd, cholesky, lsqr, sparse_cg, sag, saga)",
                "desc": "Solver to use in the computational routines:- 'auto' chooses the solver automatically based on the type of data.- 'svd' uses a Singular Value Decomposition of X to compute the Ridgecoefficients. More stable for singular matrices than 'cholesky'.- 'cholesky' uses the standard scipy.linalg.solve function toobtain a closed-form solution.- 'sparse_cg' uses the conjugate gradient solver as found inscipy.sparse.linalg.cg. As an iterative algorithm, this solver ismore appropriate than 'cholesky' for large-scale data(possibility to set `tol` and `max_iter`).- 'lsqr' uses the dedicated regularized least-squares routinescipy.sparse.linalg.lsqr. It is the fastest and uses an iterativeprocedure.- 'sag' uses a Stochastic Average Gradient descent, and 'saga' usesits unbiased and more flexible version named SAGA. Both methodsuse an iterative procedure, and are often faster than other solverswhen both n_samples and n_features are large. Note that 'sag' and'saga' fast convergence is only guaranteed on features withapproximately the same scale. You can preprocess the data with ascaler from sklearn.preprocessing... versionadded:: 0.17Stochastic Average Gradient descent solver... versionadded:: 0.19SAGA solver.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used when ``solver`` == 'sag' or 'saga' to shuffle the data.See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.RidgeClassifier"
        }
    },
    "sklearn.linear_model.RidgeClassifierCV": {
        "cls": "Block",
        "typename": "RidgeClassifierCV",
        "desc": "Ridge classifier with built-in cross-validation.  See glossary entry for :term:`cross-validation estimator`.  By default, it performs Leave-One-Out Cross-Validation. Currently, only the n_features > n_samples case is handled efficiently.  Read more in the :ref:`User Guide <ridge_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alphas": {
                "type": "ndarray of shape (n_alphas,), default=(0.1, 1.0, 10.0)",
                "desc": "Array of alpha values to try.Regularization strength; must be a positive float. Regularizationimproves the conditioning of the problem and reduces the variance ofthe estimates. Larger values specify stronger regularization.Alpha corresponds to ``1 / (2C)`` in other linear models such as:class:`~sklearn.linear_model.LogisticRegression` or:class:`~sklearn.svm.LinearSVC`.",
                "default": "(0.1",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "This parameter is ignored when ``fit_intercept`` is set to False.If True, the regressors X will be normalized before regression bysubtracting the mean and dividing by the l2-norm.If you wish to standardize, please use:class:`~sklearn.preprocessing.StandardScaler` before calling ``fit``on an estimator with ``normalize=False``.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "scoring": {
                "type": "string",
                "desc": "A string (see model evaluation documentation) ora scorer callable object / function with signature``scorer(estimator, X, y)``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the efficient Leave-One-Out cross-validation- integer, to specify the number of folds.- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict or 'balanced', default=None",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If not given, all classes are supposed to have weight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "store_cv_values": {
                "type": "boolean",
                "desc": "Flag indicating if the cross-validation values corresponding toeach alpha should be stored in the ``cv_values_`` attribute (seebelow). This flag is only compatible with ``cv=None`` (i.e. usingLeave-One-Out Cross-Validation).",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.RidgeClassifierCV"
        }
    },
    "sklearn.linear_model.SGDClassifier": {
        "cls": "Block",
        "typename": "SGDClassifier",
        "desc": "Linear classifiers (SVM, logistic regression, etc.) with SGD training.  This estimator implements regularized linear models with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate). SGD allows minibatch (online/out-of-core) learning via the `partial_fit` method. For best results using the default learning rate schedule, the data should have zero mean and unit variance.  This implementation works with data represented as dense or sparse arrays of floating point values for the features. The model it fits can be controlled with the loss parameter; by default, it fits a linear support vector machine (SVM).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  Read more in the :ref:`User Guide <sgd>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "loss": {
                "type": "string",
                "desc": "The loss function to be used. Defaults to 'hinge', which gives alinear SVM.The possible options are 'hinge', 'log', 'modified_huber','squared_hinge', 'perceptron', or a regression loss: 'squared_loss','huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.The 'log' loss gives logistic regression, a probabilistic classifier.'modified_huber' is another smooth loss that brings tolerance tooutliers as well as probability estimates.'squared_hinge' is like hinge but is quadratically penalized.'perceptron' is the linear loss used by the perceptron algorithm.The other losses are designed for regression but can be useful inclassification as well; see:class:`~sklearn.linear_model.SGDRegressor` for a description.More details about the losses formulas can be found in the:ref:`User Guide <sgd_mathematical_formulation>`.",
                "default": "'hinge'",
                "dictKeyOf": "initkargs"
            },
            "penalty": {
                "type": "option(l2, l1, elasticnet)",
                "desc": "The penalty (aka regularization term) to be used. Defaults to 'l2'which is the standard regularizer for linear SVM models. 'l1' and'elasticnet' might bring sparsity to the model (feature selection)not achievable with 'l2'.",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the regularization term. The higher thevalue, the stronger the regularization.Also used to compute the learning rate when set to `learning_rate` isset to 'optimal'.",
                "default": "0.0001",
                "dictKeyOf": "initkargs"
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.Only used if `penalty` is 'elasticnet'.",
                "default": "0.15",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether the intercept should be estimated or not. If False, thedata is assumed to be already centered.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of passes over the training data (aka epochs).It only impacts the behavior in the ``fit`` method, and not the:meth:`partial_fit` method... versionadded:: 0.19",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The stopping criterion. If it is not None, training will stopwhen (loss > best_loss - tol) for ``n_iter_no_change`` consecutiveepochs.Convergence is checked against the training loss or thevalidation loss depending on the `early_stopping` parameter... versionadded:: 0.19",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether or not the training data should be shuffled after each epoch.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "The verbosity level.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "epsilon": {
                "type": "number",
                "desc": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.For 'huber', determines the threshold at which it becomes lessimportant to get the prediction exactly right.For epsilon-insensitive, any differences between the current predictionand the correct label are ignored if they are less than this threshold.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of CPUs to use to do the OVA (One Versus All, formulti-class problems) computation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for shuffling the data, when ``shuffle`` is set to ``True``.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "string",
                "desc": "The learning rate schedule:- 'constant': `eta = eta0`- 'optimal': `eta = 1.0 / (alpha * (t + t0))`where t0 is chosen by a heuristic proposed by Leon Bottou.- 'invscaling': `eta = eta0 / pow(t, power_t)`- 'adaptive': eta = eta0, as long as the training keeps decreasing.Each time n_iter_no_change consecutive epochs fail to decrease thetraining loss by tol or fail to increase validation score by tol ifearly_stopping is True, the current learning rate is divided by 5... versionadded:: 0.20Added 'adaptive' option",
                "default": "'optimal'",
                "dictKeyOf": "initkargs"
            },
            "eta0": {
                "type": "double, default=0.0",
                "desc": "The initial learning rate for the 'constant', 'invscaling' or'adaptive' schedules. The default value is 0.0 as eta0 is not used bythe default schedule 'optimal'.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "power_t": {
                "type": "double, default=0.5",
                "desc": "The exponent for inverse scaling learning rate [default 0.5].",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "early_stopping": {
                "type": "boolean",
                "desc": "Whether to use early stopping to terminate training when validationscore is not improving. If set to True, it will automatically set asidea stratified fraction of training data as validation and terminatetraining when validation score returned by the `score` method is notimproving by at least tol for n_iter_no_change consecutive epochs... versionadded:: 0.20Added 'early_stopping' option",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if `early_stopping` is True... versionadded:: 0.20Added 'validation_fraction' option",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "Number of iterations with no improvement to wait before stoppingfitting.Convergence is checked against the training loss or thevalidation loss depending on the `early_stopping` parameter... versionadded:: 0.20Added 'n_iter_no_change' option",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict, {class_label: weight} or \"balanced\", default=None",
                "desc": "Preset for the class_weight fit parameter.Weights associated with classes. If not given, all classesare supposed to have weight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.Repeatedly calling fit or partial_fit when warm_start is True canresult in a different solution than when calling fit a single timebecause of the way the data is shuffled.If a dynamic learning rate is used, the learning rate is adapteddepending on the number of samples already seen. Calling ``fit`` resetsthis counter, while ``partial_fit`` will result in increasing theexisting counter.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "average": {
                "type": "boolean",
                "desc": "When set to True, computes the averaged SGD weights accross allupdates and stores the result in the ``coef_`` attribute. If set toan int greater than 1, averaging will begin once the total number ofsamples seen reaches `average`. So ``average=10`` will beginaveraging after seeing 10 samples.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.SGDClassifier"
        }
    },
    "sklearn.linear_model.SGDRegressor": {
        "cls": "Block",
        "typename": "SGDRegressor",
        "desc": "Linear model fitted by minimizing a regularized empirical loss with SGD  SGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).  The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.  This implementation works with data represented as dense numpy arrays of floating point values for the features.  Read more in the :ref:`User Guide <sgd>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "loss": {
                "type": "string",
                "desc": "The loss function to be used. The possible values are 'squared_loss','huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'The 'squared_loss' refers to the ordinary least squares fit.'huber' modifies 'squared_loss' to focus less on getting outlierscorrect by switching from squared to linear loss past a distance ofepsilon. 'epsilon_insensitive' ignores errors less than epsilon and islinear past that; this is the loss function used in SVR.'squared_epsilon_insensitive' is the same but becomes squared loss pasta tolerance of epsilon.More details about the losses formulas can be found in the:ref:`User Guide <sgd_mathematical_formulation>`.",
                "default": "'squared_loss'",
                "dictKeyOf": "initkargs"
            },
            "penalty": {
                "type": "option(l2, l1, elasticnet)",
                "desc": "The penalty (aka regularization term) to be used. Defaults to 'l2'which is the standard regularizer for linear SVM models. 'l1' and'elasticnet' might bring sparsity to the model (feature selection)not achievable with 'l2'.",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Constant that multiplies the regularization term. The higher thevalue, the stronger the regularization.Also used to compute the learning rate when set to `learning_rate` isset to 'optimal'.",
                "default": "0.0001",
                "dictKeyOf": "initkargs"
            },
            "l1_ratio": {
                "type": "number",
                "desc": "The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1.l1_ratio=0 corresponds to L2 penalty, l1_ratio=1 to L1.Only used if `penalty` is 'elasticnet'.",
                "default": "0.15",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether the intercept should be estimated or not. If False, thedata is assumed to be already centered.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of passes over the training data (aka epochs).It only impacts the behavior in the ``fit`` method, and not the:meth:`partial_fit` method... versionadded:: 0.19",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The stopping criterion. If it is not None, training will stopwhen (loss > best_loss - tol) for ``n_iter_no_change`` consecutiveepochs.Convergence is checked against the training loss or thevalidation loss depending on the `early_stopping` parameter... versionadded:: 0.19",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether or not the training data should be shuffled after each epoch.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "The verbosity level.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "epsilon": {
                "type": "number",
                "desc": "Epsilon in the epsilon-insensitive loss functions; only if `loss` is'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'.For 'huber', determines the threshold at which it becomes lessimportant to get the prediction exactly right.For epsilon-insensitive, any differences between the current predictionand the correct label are ignored if they are less than this threshold.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used for shuffling the data, when ``shuffle`` is set to ``True``.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "string",
                "desc": "The learning rate schedule:- 'constant': `eta = eta0`- 'optimal': `eta = 1.0 / (alpha * (t + t0))`where t0 is chosen by a heuristic proposed by Leon Bottou.- 'invscaling': `eta = eta0 / pow(t, power_t)`- 'adaptive': eta = eta0, as long as the training keeps decreasing.Each time n_iter_no_change consecutive epochs fail to decrease thetraining loss by tol or fail to increase validation score by tol ifearly_stopping is True, the current learning rate is divided by 5... versionadded:: 0.20Added 'adaptive' option",
                "default": "'invscaling'",
                "dictKeyOf": "initkargs"
            },
            "eta0": {
                "type": "double, default=0.01",
                "desc": "The initial learning rate for the 'constant', 'invscaling' or'adaptive' schedules. The default value is 0.01.",
                "default": "0.01",
                "dictKeyOf": "initkargs"
            },
            "power_t": {
                "type": "double, default=0.25",
                "desc": "The exponent for inverse scaling learning rate.",
                "default": "0.25",
                "dictKeyOf": "initkargs"
            },
            "early_stopping": {
                "type": "boolean",
                "desc": "Whether to use early stopping to terminate training when validationscore is not improving. If set to True, it will automatically set asidea fraction of training data as validation and terminatetraining when validation score returned by the `score` method is notimproving by at least `tol` for `n_iter_no_change` consecutiveepochs... versionadded:: 0.20Added 'early_stopping' option",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if `early_stopping` is True... versionadded:: 0.20Added 'validation_fraction' option",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "Number of iterations with no improvement to wait before stoppingfitting.Convergence is checked against the training loss or thevalidation loss depending on the `early_stopping` parameter... versionadded:: 0.20Added 'n_iter_no_change' option",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previous call to fit asinitialization, otherwise, just erase the previous solution.See :term:`the Glossary <warm_start>`.Repeatedly calling fit or partial_fit when warm_start is True canresult in a different solution than when calling fit a single timebecause of the way the data is shuffled.If a dynamic learning rate is used, the learning rate is adapteddepending on the number of samples already seen. Calling ``fit`` resetsthis counter, while ``partial_fit`` will result in increasing theexisting counter.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "average": {
                "type": "boolean",
                "desc": "When set to True, computes the averaged SGD weights accross allupdates and stores the result in the ``coef_`` attribute. If set toan int greater than 1, averaging will begin once the total number ofsamples seen reaches `average`. So ``average=10`` will beginaveraging after seeing 10 samples.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.SGDRegressor"
        }
    },
    "sklearn.linear_model.TheilSenRegressor": {
        "cls": "Block",
        "typename": "TheilSenRegressor",
        "desc": "Theil-Sen Estimator: robust multivariate regression model.  The algorithm calculates least square solutions on subsets with size n_subsamples of the samples in X. Any value of n_subsamples between the number of features and samples leads to an estimator with a compromise between robustness and efficiency. Since the number of least square solutions is \"n_samples choose n_subsamples\", it can be extremely large and can therefore be limited with max_subpopulation. If this limit is reached, the subsets are chosen randomly. In a final step, the spatial median (or L1 median) is calculated of all least square solutions.  Read more in the :ref:`User Guide <theil_sen_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "copy_X": {
                "type": "boolean",
                "desc": "If True, X will be copied; else, it may be overwritten.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "max_subpopulation": {
                "type": "number",
                "desc": "Instead of computing with a set of cardinality 'n choose k', where n isthe number of samples and k is the number of subsamples (at leastnumber of features), consider only a stochastic subpopulation of agiven maximal size if 'n choose k' is larger than max_subpopulation.For other than small problem sizes this parameter will determinememory usage and runtime if n_subsamples is not changed.",
                "default": "1e4",
                "dictKeyOf": "initkargs"
            },
            "n_subsamples": {
                "type": "number",
                "desc": "Number of samples to calculate the parameters. This is at least thenumber of features (plus 1 if fit_intercept=True) and the number ofsamples as a maximum. A lower number leads to a higher breakdownpoint and a low efficiency while a high number leads to a lowbreakdown point and a high efficiency. If None, take theminimum number of subsamples leading to maximal robustness.If n_subsamples is set to n_samples, Theil-Sen is identical to leastsquares.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations for the calculation of spatial median.",
                "default": "300",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance when calculating spatial median.",
                "default": "1.e-3",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "A random number generator instance to define the state of the randompermutations generator. Pass an int for reproducible output acrossmultiple function calls.See :term:`Glossary <random_state>`",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of CPUs to use during the cross validation.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Verbose mode when fitting the model.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.TheilSenRegressor"
        }
    },
    "sklearn.linear_model.TweedieRegressor": {
        "cls": "Block",
        "typename": "TweedieRegressor",
        "desc": "Generalized Linear Model with a Tweedie distribution.  This estimator can be used to model different GLMs depending on the ``power`` parameter, which determines the underlying distribution.  Read more in the :ref:`User Guide <Generalized_linear_regression>`.  .. versionadded:: 0.23",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.linear_model",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "power": {
                "type": "number",
                "desc": "The power determines the underlying target distribution accordingto the following table:",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.linear_model.TweedieRegressor"
        }
    },
    "sklearn.manifold.Isomap": {
        "cls": "Block",
        "typename": "Isomap",
        "desc": "Isomap Embedding  Non-linear dimensionality reduction through Isometric Mapping  Read more in the :ref:`User Guide <isomap>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.manifold",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_neighbors": {
                "type": "number",
                "desc": "number of neighbors to consider for each point.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "number of coordinates for the manifold",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "eigen_solver": {
                "type": "option(auto, arpack, dense)",
                "desc": "'auto' : Attempt to choose the most efficient solverfor the given problem.'arpack' : Use Arnoldi decomposition to find the eigenvaluesand eigenvectors.'dense' : Use a direct solver (i.e. LAPACK)for the eigenvalue decomposition.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Convergence tolerance passed to arpack or lobpcg.not used if eigen_solver == 'dense'.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations for the arpack solver.not used if eigen_solver == 'dense'.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "path_method": {
                "type": "option(auto, FW, D)",
                "desc": "Method to use in finding shortest path.'auto' : attempt to choose the best algorithm automatically.'FW' : Floyd-Warshall algorithm.'D' : Dijkstra's algorithm.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "neighbors_algorithm": {
                "type": "option(auto, brute, kd_tree, ball_tree)",
                "desc": "Algorithm to use for nearest neighbors search,passed to neighbors.NearestNeighbors instance.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string or callable, it must be one ofthe options allowed by :func:`sklearn.metrics.pairwise_distances` forits metric parameter.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square. X may be a :term:`Glossary <sparse graph>`... versionadded:: 0.22",
                "default": "\"minkowski\"",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Parameter for the Minkowski metric fromsklearn.metrics.pairwise.pairwise_distances. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used... versionadded:: 0.22",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.manifold.Isomap"
        }
    },
    "sklearn.manifold.LocallyLinearEmbedding": {
        "cls": "Block",
        "typename": "LocallyLinearEmbedding",
        "desc": "Locally Linear Embedding  Read more in the :ref:`User Guide <locally_linear_embedding>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.manifold",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_neighbors": {
                "type": "number",
                "desc": "number of neighbors to consider for each point.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "n_components": {
                "type": "number",
                "desc": "number of coordinates for the manifold",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "reg": {
                "type": "number",
                "desc": "regularization constant, multiplies the trace of the local covariancematrix of the distances.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "eigen_solver": {
                "type": "option(auto, arpack, dense)",
                "desc": "",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "auto": {
                "type": "algorithm will attempt to choose the best method for input data",
                "desc": "",
                "dictKeyOf": "initkargs"
            },
            "arpack": {
                "type": "use arnoldi iteration in shift-invert mode.",
                "desc": "For this method, M may be a dense matrix, sparse matrix,or general linear operator.Warning: ARPACK can be unstable for some problems. It isbest to try several random seeds in order to check results.dense : use standard dense matrix operations for the eigenvaluedecomposition. For this method, M must be an arrayor matrix type. This method should be avoided forlarge problems.",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for 'arpack' methodNot used if eigen_solver=='dense'.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "maximum number of iterations for the arpack solver.Not used if eigen_solver=='dense'.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "method": {
                "type": "option(standard, hessian, modified, ltsa)",
                "desc": "- `standard`: use the standard locally linear embedding algorithm. seereference [1]_- `hessian`: use the Hessian eigenmap method. This method requires``n_neighbors > n_components * (1 + (n_components + 1) / 2``. seereference [2]_- `modified`: use the modified locally linear embedding algorithm.see reference [3]_- `ltsa`: use local tangent space alignment algorithm. seereference [4]_",
                "default": "'standard'",
                "dictKeyOf": "initkargs"
            },
            "hessian_tol": {
                "type": "number",
                "desc": "Tolerance for Hessian eigenmapping method.Only used if ``method == 'hessian'``",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "modified_tol": {
                "type": "number",
                "desc": "Tolerance for modified LLE method.Only used if ``method == 'modified'``",
                "default": "1e-12",
                "dictKeyOf": "initkargs"
            },
            "neighbors_algorithm": {
                "type": "option(auto, brute, kd_tree, ball_tree)",
                "desc": "algorithm to use for nearest neighbors search,passed to neighbors.NearestNeighbors instance",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines the random number generator when``eigen_solver`` == 'arpack'. Pass an int for reproducible resultsacross multiple function calls. See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.manifold.LocallyLinearEmbedding"
        }
    },
    "sklearn.manifold.MDS": {
        "cls": "Block",
        "typename": "MDS",
        "desc": "Multidimensional scaling.  Read more in the :ref:`User Guide <multidimensional_scaling>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.manifold",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of dimensions in which to immerse the dissimilarities.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "boolean",
                "desc": "If ``True``, perform metric MDS; otherwise, perform nonmetric MDS.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "Number of times the SMACOF algorithm will be run with differentinitializations. The final results will be the best output of the runs,determined by the run with the smallest final stress.",
                "default": "4",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations of the SMACOF algorithm for a single run.",
                "default": "300",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Level of verbosity.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "Relative tolerance with respect to stress at which to declareconvergence.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. If multipleinitializations are used (``n_init``), each run of the algorithm iscomputed in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines the random number generator used to initialize the centers.Pass an int for reproducible results across multiple function calls.See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "dissimilarity": {
                "type": "option(euclidean, precomputed)",
                "desc": "Dissimilarity measure to use:- 'euclidean':Pairwise Euclidean distances between points in the dataset.- 'precomputed':Pre-computed dissimilarities are passed directly to ``fit`` and``fit_transform``.",
                "default": "'euclidean'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.manifold.MDS"
        }
    },
    "sklearn.manifold.SpectralEmbedding": {
        "cls": "Block",
        "typename": "SpectralEmbedding",
        "desc": "Spectral embedding for non-linear dimensionality reduction.  Forms an affinity matrix given by the specified function and applies spectral decomposition to the corresponding graph laplacian. The resulting transformation is given by the value of the eigenvectors for each data point.  Note : Laplacian Eigenmaps is the actual algorithm implemented here.  Read more in the :ref:`User Guide <spectral_embedding>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.manifold",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "The dimension of the projected subspace.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "affinity": {
                "type": "option(nearest_neighbors, rbf, precomputed,                 precomputed_nearest_neighbors)",
                "desc": "How to construct the affinity matrix.- 'nearest_neighbors' : construct the affinity matrix by computing agraph of nearest neighbors.- 'rbf' : construct the affinity matrix by computing a radial basisfunction (RBF) kernel.- 'precomputed' : interpret ``X`` as a precomputed affinity matrix.- 'precomputed_nearest_neighbors' : interpret ``X`` as a sparse graphof precomputed nearest neighbors, and constructs the affinity matrixby selecting the ``n_neighbors`` nearest neighbors.- callable : use passed in function as affinitythe function takes in data matrix (n_samples, n_features)and return affinity matrix (n_samples, n_samples).",
                "default": "'nearest_neighbors'",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "number",
                "desc": "Kernel coefficient for rbf kernel. If None, gamma will be set to1/n_features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines the random number generator used for the initialization ofthe lobpcg eigenvectors when ``solver`` == 'amg'. Pass an int forreproducible results across multiple function calls.See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "eigen_solver": {
                "type": "option(arpack, lobpcg, amg)",
                "desc": "The eigenvalue decomposition strategy to use. AMG requires pyamgto be installed. It can be faster on very large, sparse problems.If None, then ``'arpack'`` is used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of nearest neighbors for nearest_neighbors graph building.If None, n_neighbors will be set to max(n_samples/10, 1).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.manifold.SpectralEmbedding"
        }
    },
    "sklearn.manifold.TSNE": {
        "cls": "Block",
        "typename": "TSNE",
        "desc": "t-distributed Stochastic Neighbor Embedding.  t-SNE [1] is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.  It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. For more tips see Laurens van der Maaten's FAQ [2].  Read more in the :ref:`User Guide <t_sne>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.manifold",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Dimension of the embedded space.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "perplexity": {
                "type": "number",
                "desc": "The perplexity is related to the number of nearest neighbors thatis used in other manifold learning algorithms. Larger datasetsusually require a larger perplexity. Consider selecting a valuebetween 5 and 50. Different values can result in significantlydifferent results.",
                "default": "30.0",
                "dictKeyOf": "initkargs"
            },
            "early_exaggeration": {
                "type": "number",
                "desc": "Controls how tight natural clusters in the original space are inthe embedded space and how much space will be between them. Forlarger values, the space between natural clusters will be largerin the embedded space. Again, the choice of this parameter is notvery critical. If the cost function increases during initialoptimization, the early exaggeration factor or the learning ratemight be too high.",
                "default": "12.0",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "number",
                "desc": "The learning rate for t-SNE is usually in the range [10.0, 1000.0]. Ifthe learning rate is too high, the data may look like a 'ball' with anypoint approximately equidistant from its nearest neighbours. If thelearning rate is too low, most points may look compressed in a densecloud with few outliers. If the cost function gets stuck in a bad localminimum increasing the learning rate may help.",
                "default": "200.0",
                "dictKeyOf": "initkargs"
            },
            "n_iter": {
                "type": "number",
                "desc": "Maximum number of iterations for the optimization. Should be atleast 250.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "n_iter_without_progress": {
                "type": "number",
                "desc": "Maximum number of iterations without progress before we abort theoptimization, used after 250 initial iterations with earlyexaggeration. Note that progress is only checked every 50 iterations sothis value is rounded to the next multiple of 50... versionadded:: 0.17parameter *n_iter_without_progress* to control stopping criteria.",
                "default": "300",
                "dictKeyOf": "initkargs"
            },
            "min_grad_norm": {
                "type": "number",
                "desc": "If the gradient norm is below this threshold, the optimization willbe stopped.",
                "default": "1e-7",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string, it must be one of the optionsallowed by scipy.spatial.distance.pdist for its metric parameter, ora metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.If metric is \"precomputed\", X is assumed to be a distance matrix.Alternatively, if metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays from X as input and return a value indicatingthe distance between them. The default is \"euclidean\" which isinterpreted as squared euclidean distance.",
                "default": "'euclidean'",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "option(random, pca)",
                "desc": "Initialization of embedding. Possible options are 'random', 'pca',and a numpy array of shape (n_samples, n_components).PCA initialization cannot be used with precomputed distances and isusually more globally stable than random initialization.",
                "default": "'random'",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Verbosity level.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines the random number generator. Pass an int for reproducibleresults across multiple function calls. Note that differentinitializations might result in different local minima of the costfunction. See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "method": {
                "type": "string",
                "desc": "By default the gradient calculation algorithm uses Barnes-Hutapproximation running in O(NlogN) time. method='exact'will run on the slower, but exact, algorithm in O(N^2) time. Theexact algorithm should be used when nearest-neighbor errors needto be better than 3%. However, the exact method cannot scale tomillions of examples... versionadded:: 0.17Approximate optimization *method* via the Barnes-Hut.",
                "default": "'barnes_hut'",
                "dictKeyOf": "initkargs"
            },
            "angle": {
                "type": "number",
                "desc": "Only used if method='barnes_hut'This is the trade-off between speed and accuracy for Barnes-Hut T-SNE.'angle' is the angular size (referred to as theta in [3]) of a distantnode as measured from a point. If this size is below 'angle' then it isused as a summary node of all points contained within it.This method is not very sensitive to changes in this parameterin the range of 0.2 - 0.8. Angle less than 0.2 has quickly increasingcomputation time and angle greater 0.8 has quickly increasing error.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search. This parameterhas no impact when ``metric=\"precomputed\"`` or(``metric=\"euclidean\"`` and ``method=\"exact\"``).``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionadded:: 0.22",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "square_distances": {
                "type": "True or 'legacy', default='legacy'",
                "desc": "Whether TSNE should square the distance values. ``'legacy'`` meansthat distance values are squared only when ``metric=\"euclidean\"``.``True`` means that distance values are squared for all metrics... versionadded:: 0.24Added to provide backward compatibility during deprecation oflegacy squaring behavior... deprecated:: 0.24Legacy squaring behavior was deprecated in 0.24. The ``'legacy'``value will be removed in 1.1 (renaming of 0.26), at which point thedefault value will change to ``True``.",
                "default": "'legacy'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.manifold.TSNE"
        }
    },
    "sklearn.metrics.ConfusionMatrixDisplay": {
        "cls": "Block",
        "typename": "ConfusionMatrixDisplay",
        "desc": "Confusion Matrix visualization.  It is recommend to use :func:`~sklearn.metrics.plot_confusion_matrix` to create a :class:`ConfusionMatrixDisplay`. All parameters are stored as attributes.  Read more in the :ref:`User Guide <visualizations>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.metrics",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "confusion_matrix": {
                "type": "ndarray of shape (n_classes, n_classes)",
                "desc": "Confusion matrix.",
                "dictKeyOf": "initkargs"
            },
            "display_labels": {
                "type": "ndarray of shape (n_classes,), default=None",
                "desc": "Display labels for plot. If None, display labels are set from 0 to`n_classes - 1`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.metrics.ConfusionMatrixDisplay"
        }
    },
    "sklearn.metrics.DetCurveDisplay": {
        "cls": "Block",
        "typename": "DetCurveDisplay",
        "desc": "DET curve visualization.  It is recommend to use :func:`~sklearn.metrics.plot_det_curve` to create a visualizer. All parameters are stored as attributes.  Read more in the :ref:`User Guide <visualizations>`.  .. versionadded:: 0.24",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.metrics",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "fpr": {
                "type": "ndarray",
                "desc": "False positive rate.",
                "dictKeyOf": "initkargs"
            },
            "fnr": {
                "type": "ndarray",
                "desc": "False negative rate.",
                "dictKeyOf": "initkargs"
            },
            "estimator_name": {
                "type": "string",
                "desc": "Name of estimator. If None, the estimator name is not shown.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The label of the positive class.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.metrics.DetCurveDisplay"
        }
    },
    "sklearn.metrics.PrecisionRecallDisplay": {
        "cls": "Block",
        "typename": "PrecisionRecallDisplay",
        "desc": "Precision Recall visualization.  It is recommend to use :func:`~sklearn.metrics.plot_precision_recall_curve` to create a visualizer. All parameters are stored as attributes.  Read more in the :ref:`User Guide <visualizations>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.metrics",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "precision": {
                "type": "ndarray",
                "desc": "Precision values.",
                "dictKeyOf": "initkargs"
            },
            "recall": {
                "type": "ndarray",
                "desc": "Recall values.",
                "dictKeyOf": "initkargs"
            },
            "average_precision": {
                "type": "number",
                "desc": "Average precision. If None, the average precision is not shown.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "estimator_name": {
                "type": "string",
                "desc": "Name of estimator. If None, then the estimator name is not shown.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The class considered as the positive class. If None, the class will notbe shown in the legend... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.metrics.PrecisionRecallDisplay"
        }
    },
    "sklearn.metrics.RocCurveDisplay": {
        "cls": "Block",
        "typename": "RocCurveDisplay",
        "desc": "ROC Curve visualization.  It is recommend to use :func:`~sklearn.metrics.plot_roc_curve` to create a visualizer. All parameters are stored as attributes.  Read more in the :ref:`User Guide <visualizations>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.metrics",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "fpr": {
                "type": "ndarray",
                "desc": "False positive rate.",
                "dictKeyOf": "initkargs"
            },
            "tpr": {
                "type": "ndarray",
                "desc": "True positive rate.",
                "dictKeyOf": "initkargs"
            },
            "roc_auc": {
                "type": "number",
                "desc": "Area under ROC curve. If None, the roc_auc score is not shown.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "estimator_name": {
                "type": "string",
                "desc": "Name of estimator. If None, the estimator name is not shown.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The class considered as the positive class when computing the roc aucmetrics. By default, `estimators.classes_[1]` is consideredas the positive class... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.metrics.RocCurveDisplay"
        }
    },
    "sklearn.metrics.pairwise.Parallel": {
        "cls": "Block",
        "typename": "Parallel",
        "desc": "Helper class for readable parallel mapping.  Read more in the :ref:`User Guide <parallel>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics.pairwise",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.pairwise.Parallel"
        }
    },
    "sklearn.mixture.BayesianGaussianMixture": {
        "cls": "Block",
        "typename": "BayesianGaussianMixture",
        "desc": "Variational Bayesian estimation of a Gaussian mixture.  This class allows to infer an approximate posterior distribution over the parameters of a Gaussian mixture distribution. The effective number of components can be inferred from the data.  This class implements two types of prior for the weights distribution: a finite mixture model with Dirichlet distribution and an infinite mixture model with the Dirichlet Process. In practice Dirichlet Process inference algorithm is approximated and uses a truncated distribution with a fixed maximum number of components (called the Stick-breaking representation). The number of components actually used almost always depends on the data.  .. versionadded:: 0.18  Read more in the :ref:`User Guide <bgmm>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.mixture",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "The number of mixture components. Depending on the data and the valueof the `weight_concentration_prior` the model can decide to not useall the components by setting some component `weights_` to values veryclose to zero. The number of effective components is therefore smallerthan n_components.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "covariance_type": {
                "type": "option(full, tied, diag, spherical)",
                "desc": "String describing the type of covariance parameters to use.Must be one of::'full' (each component has its own general covariance matrix),'tied' (all components share the same general covariance matrix),'diag' (each component has its own diagonal covariance matrix),'spherical' (each component has its own single variance).",
                "default": "'full'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The convergence threshold. EM iterations will stop when thelower bound average gain on the likelihood (of the training data withrespect to the model) is below this threshold.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "reg_covar": {
                "type": "number",
                "desc": "Non-negative regularization added to the diagonal of covariance.Allows to assure that the covariance matrices are all positive.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The number of EM iterations to perform.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "The number of initializations to perform. The result with the highestlower bound value on the likelihood is kept.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "init_params": {
                "type": "option(kmeans, random)",
                "desc": "The method used to initialize the weights, the means and thecovariances.Must be one of::'kmeans' : responsibilities are initialized using kmeans.'random' : responsibilities are initialized randomly.",
                "default": "'kmeans'",
                "dictKeyOf": "initkargs"
            },
            "weight_concentration_prior_type": {
                "type": "string",
                "desc": "String describing the type of the weight concentration prior.Must be one of::'dirichlet_process' (using the Stick-breaking representation),'dirichlet_distribution' (can favor more uniform weights).",
                "default": "'dirichlet_process'",
                "dictKeyOf": "initkargs"
            },
            "weight_concentration_prior": {
                "type": "number",
                "desc": "The dirichlet concentration of each component on the weightdistribution (Dirichlet). This is commonly called gamma in theliterature. The higher concentration puts more mass inthe center and will lead to more components being active, while a lowerconcentration parameter will lead to more mass at the edge of themixture weights simplex. The value of the parameter must be greaterthan 0. If it is None, it's set to ``1. / n_components``.",
                "default": "None.",
                "dictKeyOf": "initkargs"
            },
            "mean_precision_prior": {
                "type": "number",
                "desc": "The precision prior on the mean distribution (Gaussian).Controls the extent of where means can be placed. Largervalues concentrate the cluster means around `mean_prior`.The value of the parameter must be greater than 0.If it is None, it is set to 1.",
                "default": "None.",
                "dictKeyOf": "initkargs"
            },
            "mean_prior": {
                "type": "array-like, shape (n_features,), default=None.",
                "desc": "The prior on the mean distribution (Gaussian).If it is None, it is set to the mean of X.",
                "default": "None.",
                "dictKeyOf": "initkargs"
            },
            "degrees_of_freedom_prior": {
                "type": "number",
                "desc": "The prior of the number of degrees of freedom on the covariancedistributions (Wishart). If it is None, it's set to `n_features`.",
                "default": "None.",
                "dictKeyOf": "initkargs"
            },
            "covariance_prior": {
                "type": "number",
                "desc": "The prior on the covariance distribution (Wishart).If it is None, the emiprical covariance prior is initialized using thecovariance of X. The shape depends on `covariance_type`::(n_features, n_features) if 'full',(n_features, n_features) if 'tied',(n_features)       if 'diag',float          if 'spherical'",
                "default": "None.",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random seed given to the method chosen to initialize theparameters (see `init_params`).In addition, it controls the generation of random samples from thefitted distribution (see the method `sample`).Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "If 'warm_start' is True, the solution of the last fitting is used asinitialization for the next call of fit(). This can speed upconvergence when fit is called several times on similar problems.See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Enable verbose output. If 1 then it prints the currentinitialization and each iteration step. If greater than 1 thenit prints also the log probability and the time neededfor each step.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "verbose_interval": {
                "type": "number",
                "desc": "Number of iteration done before the next print.",
                "default": "10",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.mixture.BayesianGaussianMixture"
        }
    },
    "sklearn.mixture.GaussianMixture": {
        "cls": "Block",
        "typename": "GaussianMixture",
        "desc": "Gaussian Mixture.  Representation of a Gaussian mixture model probability distribution. This class allows to estimate the parameters of a Gaussian mixture distribution.  Read more in the :ref:`User Guide <gmm>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.mixture",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "The number of mixture components.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "covariance_type": {
                "type": "option(full, tied, diag, spherical)",
                "desc": "String describing the type of covariance parameters to use.Must be one of:'full'each component has its own general covariance matrix'tied'all components share the same general covariance matrix'diag'each component has its own diagonal covariance matrix'spherical'each component has its own single variance",
                "default": "'full'",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "The convergence threshold. EM iterations will stop when thelower bound average gain is below this threshold.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "reg_covar": {
                "type": "number",
                "desc": "Non-negative regularization added to the diagonal of covariance.Allows to assure that the covariance matrices are all positive.",
                "default": "1e-6",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The number of EM iterations to perform.",
                "default": "100",
                "dictKeyOf": "initkargs"
            },
            "n_init": {
                "type": "number",
                "desc": "The number of initializations to perform. The best results are kept.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "init_params": {
                "type": "option(kmeans, random)",
                "desc": "The method used to initialize the weights, the means and theprecisions.Must be one of::'kmeans' : responsibilities are initialized using kmeans.'random' : responsibilities are initialized randomly.",
                "default": "'kmeans'",
                "dictKeyOf": "initkargs"
            },
            "weights_init": {
                "type": "array-like of shape (n_components, ), default=None",
                "desc": "The user-provided initial weights.If it is None, weights are initialized using the `init_params` method.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "means_init": {
                "type": "array-like of shape (n_components, n_features), default=None",
                "desc": "The user-provided initial means,If it is None, means are initialized using the `init_params` method.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "precisions_init": {
                "type": "array-like, default=None",
                "desc": "The user-provided initial precisions (inverse of the covariancematrices).If it is None, precisions are initialized using the 'init_params'method.The shape depends on 'covariance_type'::(n_components,)            if 'spherical',(n_features, n_features)        if 'tied',(n_components, n_features)       if 'diag',(n_components, n_features, n_features) if 'full'",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the random seed given to the method chosen to initialize theparameters (see `init_params`).In addition, it controls the generation of random samples from thefitted distribution (see the method `sample`).Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "If 'warm_start' is True, the solution of the last fitting is used asinitialization for the next call of fit(). This can speed upconvergence when fit is called several times on similar problems.In that case, 'n_init' is ignored and only a single initializationoccurs upon the first call.See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Enable verbose output. If 1 then it prints the currentinitialization and each iteration step. If greater than 1 thenit prints also the log probability and the time neededfor each step.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "verbose_interval": {
                "type": "number",
                "desc": "Number of iteration done before the next print.",
                "default": "10",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.mixture.GaussianMixture"
        }
    },
    "sklearn.model_selection.GridSearchCV": {
        "cls": "Parent",
        "typename": "GridSearchCV",
        "desc": "Exhaustive search over specified parameter values for an estimator.  Important members are fit, predict.  GridSearchCV implements a \"fit\" and a \"score\" method. It also implements \"score_samples\", \"predict\", \"predict_proba\", \"decision_function\", \"transform\" and \"inverse_transform\" if they are implemented in the estimator used.  The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.  Read more in the :ref:`User Guide <grid_search>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "param_grid": {
                "type": "dict or list of dictionaries",
                "desc": "Dictionary with parameters names (`str`) as keys and lists ofparameter settings to try as values, or a list of suchdictionaries, in which case the grids spanned by each dictionaryin the list are explored. This enables searching over any sequenceof parameter settings.",
                "dictKeyOf": "initkargs"
            },
            "scoring": {
                "type": "string",
                "desc": "Strategy to evaluate the performance of the cross-validated model onthe test set.If `scoring` represents a single score, one can use:- a single string (see :ref:`scoring_parameter`);- a callable (see :ref:`scoring`) that returns a single value.If `scoring` represents multiple scores, one can use:- a list or tuple of unique strings;- a callable returning a dictionary where the keys are the metricnames and the values are the metric scores;- a dictionary with metric names as keys and callables a values.See :ref:`multimetric_grid_search` for an example.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of jobs to run in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionchanged:: v0.20`n_jobs` default changed from 1 to None",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "refit": {
                "type": "boolean",
                "desc": "Refit an estimator using the best found parameters on the wholedataset.For multiple metric evaluation, this needs to be a `str` denoting thescorer that would be used to find the best parameters for refittingthe estimator at the end.Where there are considerations other than maximum score inchoosing a best estimator, ``refit`` can be set to a function whichreturns the selected ``best_index_`` given ``cv_results_``. In thatcase, the ``best_estimator_`` and ``best_params_`` will be setaccording to the returned ``best_index_`` while the ``best_score_``attribute will not be available.The refitted estimator is made available at the ``best_estimator_``attribute and permits using ``predict`` directly on this``GridSearchCV`` instance.Also for multiple metric evaluation, the attributes ``best_index_``,``best_score_`` and ``best_params_`` will only be available if``refit`` is set and all of them will be determined w.r.t this specificscorer.See ``scoring`` parameter to know more about multiple metricevaluation... versionchanged:: 0.20Support for callable added.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross validation,- integer, to specify the number of folds in a `(Stratified)KFold`,- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, if the estimator is a classifier and ``y`` iseither binary or multiclass, :class:`StratifiedKFold` is used. In allother cases, :class:`KFold` is used. These splitters are instantiatedwith `shuffle=False` so the splits will be the same across calls.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity: the higher, the more messages.- >1 : the computation time for each fold and parameter candidate isdisplayed;- >2 : the score is also displayed;- >3 : the fold and candidate parameter indexes are also displayedtogether with the starting time of the computation.",
                "dictKeyOf": "initkargs"
            },
            "pre_dispatch": {
                "type": "number",
                "desc": "Controls the number of jobs that get dispatched during parallelexecution. Reducing this number can be useful to avoid anexplosion of memory consumption when more jobs get dispatchedthan CPUs can process. This parameter can be:- None, in which case all the jobs are immediatelycreated and spawned. Use this for lightweight andfast-running jobs, to avoid delays due to on-demandspawning of the jobs- An int, giving the exact number of total jobs that arespawned- A str, giving an expression as a function of n_jobs,as in '2*n_jobs'",
                "default": "n_jobs",
                "dictKeyOf": "initkargs"
            },
            "error_score": {
                "type": "'raise' or numeric, default=np.nan",
                "desc": "Value to assign to the score if an error occurs in estimator fitting.If set to 'raise', the error is raised. If a numeric value is given,FitFailedWarning is raised. This parameter does not affect the refitstep, which will always raise the error.",
                "default": "np.nan",
                "dictKeyOf": "initkargs"
            },
            "return_train_score": {
                "type": "boolean",
                "desc": "If ``False``, the ``cv_results_`` attribute will not include trainingscores.Computing training scores is used to get insights on how differentparameter settings impact the overfitting/underfitting trade-off.However computing the scores on the training set can be computationallyexpensive and is not strictly required to select the parameters thatyield the best generalization performance... versionadded:: 0.19.. versionchanged:: 0.21Default value was changed from ``True`` to ``False``",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.GridSearchCV",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.model_selection.GroupKFold": {
        "cls": "Parent",
        "typename": "GroupKFold",
        "desc": "K-fold iterator variant with non-overlapping groups.  The same group will not appear in two different folds (the number of distinct groups has to be at least equal to the number of folds).  The folds are approximately balanced in the sense that the number of distinct groups is approximately the same in each fold.  Read more in the :ref:`User Guide <group_k_fold>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of folds. Must be at least 2... versionchanged:: 0.22``n_splits`` default value changed from 3 to 5.",
                "default": "5",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.GroupKFold"
        }
    },
    "sklearn.model_selection.GroupShuffleSplit": {
        "cls": "Parent",
        "typename": "GroupShuffleSplit",
        "desc": "Shuffle-Group(s)-Out cross-validation iterator  Provides randomized train/test indices to split data according to a third-party provided group. This group information can be used to encode arbitrary domain specific stratifications of the samples as integers.  For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.  The difference between LeavePGroupsOut and GroupShuffleSplit is that the former generates splits using all subsets of size ``p`` unique groups, whereas GroupShuffleSplit generates a user-determined number of random test splits, each with a user-determined fraction of unique groups.  For example, a less computationally intensive alternative to ``LeavePGroupsOut(p=10)`` would be ``GroupShuffleSplit(test_size=10, n_splits=100)``.  Note: The parameters ``test_size`` and ``train_size`` refer to groups, and not to samples, as in ShuffleSplit.  Read more in the :ref:`User Guide <group_shuffle_split>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of re-shuffling & splitting iterations.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "test_size": {
                "type": "number",
                "desc": "If float, should be between 0.0 and 1.0 and represent the proportionof groups to include in the test split (rounded up). If int,represents the absolute number of test groups. If None, the value isset to the complement of the train size.The default will change in version 0.21. It will remain 0.2 onlyif ``train_size`` is unspecified, otherwise it will complementthe specified ``train_size``.",
                "default": "0.2",
                "dictKeyOf": "initkargs"
            },
            "train_size": {
                "type": "number",
                "desc": "If float, should be between 0.0 and 1.0 and represent theproportion of the groups to include in the train split. Ifint, represents the absolute number of train groups. If None,the value is automatically set to the complement of the test size.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness of the training and testing indices produced.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.GroupShuffleSplit"
        }
    },
    "sklearn.model_selection.KFold": {
        "cls": "Parent",
        "typename": "KFold",
        "desc": "K-Folds cross-validator  Provides train/test indices to split data in train/test sets. Split dataset into k consecutive folds (without shuffling by default).  Each fold is then used once as a validation while the k - 1 remaining folds form the training set.  Read more in the :ref:`User Guide <k_fold>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of folds. Must be at least 2... versionchanged:: 0.22``n_splits`` default value changed from 3 to 5.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether to shuffle the data before splitting into batches.Note that the samples within each split will not be shuffled.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "When `shuffle` is True, `random_state` affects the ordering of theindices, which controls the randomness of each fold. Otherwise, thisparameter has no effect.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.KFold"
        }
    },
    "sklearn.model_selection.LeavePGroupsOut": {
        "cls": "Block",
        "typename": "LeavePGroupsOut",
        "desc": "Leave P Group(s) Out cross-validator  Provides train/test indices to split data according to a third-party provided group. This group information can be used to encode arbitrary domain specific stratifications of the samples as integers.  For instance the groups could be the year of collection of the samples and thus allow for cross-validation against time-based splits.  The difference between LeavePGroupsOut and LeaveOneGroupOut is that the former builds the test sets with all the samples assigned to ``p`` different values of the groups while the latter uses samples all assigned the same groups.  Read more in the :ref:`User Guide <leave_p_groups_out>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_groups": {
                "type": "number",
                "desc": "Number of groups (``p``) to leave out in the test split.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.LeavePGroupsOut"
        }
    },
    "sklearn.model_selection.LeavePOut": {
        "cls": "Block",
        "typename": "LeavePOut",
        "desc": "Leave-P-Out cross-validator  Provides train/test indices to split data in train/test sets. This results in testing on all distinct samples of size p, while the remaining n - p samples form the training set in each iteration.  Note: ``LeavePOut(p)`` is NOT equivalent to ``KFold(n_splits=n_samples // p)`` which creates non-overlapping test sets.  Due to the high number of iterations which grows combinatorically with the number of samples this cross-validation method can be very costly. For large datasets one should favor :class:`KFold`, :class:`StratifiedKFold` or :class:`ShuffleSplit`.  Read more in the :ref:`User Guide <leave_p_out>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "p": {
                "type": "number",
                "desc": "Size of the test sets. Must be strictly less than the number ofsamples.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.LeavePOut"
        }
    },
    "sklearn.model_selection.ParameterGrid": {
        "cls": "Block",
        "typename": "ParameterGrid",
        "desc": "Grid of parameters with a discrete number of values for each.  Can be used to iterate over parameter value combinations with the Python built-in function iter. The order of the generated parameter combinations is deterministic.  Read more in the :ref:`User Guide <grid_search>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "param_grid": {
                "type": "dict of str to sequence, or sequence of such",
                "desc": "The parameter grid to explore, as a dictionary mapping estimatorparameters to sequences of allowed values.An empty dict signifies default parameters.A sequence of dicts signifies a sequence of grids to search, and isuseful to avoid exploring parameter combinations that make no senseor have no effect. See the examples below.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.ParameterGrid"
        }
    },
    "sklearn.model_selection.ParameterSampler": {
        "cls": "Block",
        "typename": "ParameterSampler",
        "desc": "Generator on parameters sampled from given distributions.  Non-deterministic iterable over random candidate combinations for hyper- parameter search. If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.  Read more in the :ref:`User Guide <grid_search>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "param_distributions": {
                "type": "dict",
                "desc": "Dictionary with parameters names (`str`) as keys and distributionsor lists of parameters to try. Distributions must provide a ``rvs``method for sampling (such as those from scipy.stats.distributions).If a list is given, it is sampled uniformly.If a list of dicts is given, first a dict is sampled uniformly, andthen a parameter is sampled using that dict as above.",
                "dictKeyOf": "initkargs"
            },
            "n_iter": {
                "type": "number",
                "desc": "Number of parameter settings that are produced.",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Pseudo random number generator state used for random uniform samplingfrom lists of possible values instead of scipy.stats distributions.Pass an int for reproducible output across multiplefunction calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.ParameterSampler"
        }
    },
    "sklearn.model_selection.RandomizedSearchCV": {
        "cls": "Parent",
        "typename": "RandomizedSearchCV",
        "desc": "Randomized search on hyper parameters.  RandomizedSearchCV implements a \"fit\" and a \"score\" method. It also implements \"score_samples\", \"predict\", \"predict_proba\", \"decision_function\", \"transform\" and \"inverse_transform\" if they are implemented in the estimator used.  The parameters of the estimator used to apply these methods are optimized by cross-validated search over parameter settings.  In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter.  If all parameters are presented as a list, sampling without replacement is performed. If at least one parameter is given as a distribution, sampling with replacement is used. It is highly recommended to use continuous distributions for continuous parameters.  Read more in the :ref:`User Guide <randomized_parameter_search>`.  .. versionadded:: 0.14",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "param_distributions": {
                "type": "dict or list of dicts",
                "desc": "Dictionary with parameters names (`str`) as keys and distributionsor lists of parameters to try. Distributions must provide a ``rvs``method for sampling (such as those from scipy.stats.distributions).If a list is given, it is sampled uniformly.If a list of dicts is given, first a dict is sampled uniformly, andthen a parameter is sampled using that dict as above.",
                "dictKeyOf": "initkargs"
            },
            "n_iter": {
                "type": "number",
                "desc": "Number of parameter settings that are sampled. n_iter tradesoff runtime vs quality of the solution.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "scoring": {
                "type": "string",
                "desc": "Strategy to evaluate the performance of the cross-validated model onthe test set.If `scoring` represents a single score, one can use:- a single string (see :ref:`scoring_parameter`);- a callable (see :ref:`scoring`) that returns a single value.If `scoring` represents multiple scores, one can use:- a list or tuple of unique strings;- a callable returning a dictionary where the keys are the metricnames and the values are the metric scores;- a dictionary with metric names as keys and callables a values.See :ref:`multimetric_grid_search` for an example.If None, the estimator's score method is used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of jobs to run in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionchanged:: v0.20`n_jobs` default changed from 1 to None",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "refit": {
                "type": "boolean",
                "desc": "Refit an estimator using the best found parameters on the wholedataset.For multiple metric evaluation, this needs to be a `str` denoting thescorer that would be used to find the best parameters for refittingthe estimator at the end.Where there are considerations other than maximum score inchoosing a best estimator, ``refit`` can be set to a function whichreturns the selected ``best_index_`` given the ``cv_results``. In thatcase, the ``best_estimator_`` and ``best_params_`` will be setaccording to the returned ``best_index_`` while the ``best_score_``attribute will not be available.The refitted estimator is made available at the ``best_estimator_``attribute and permits using ``predict`` directly on this``RandomizedSearchCV`` instance.Also for multiple metric evaluation, the attributes ``best_index_``,``best_score_`` and ``best_params_`` will only be available if``refit`` is set and all of them will be determined w.r.t this specificscorer.See ``scoring`` parameter to know more about multiple metricevaluation... versionchanged:: 0.20Support for callable added.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines the cross-validation splitting strategy.Possible inputs for cv are:- None, to use the default 5-fold cross validation,- integer, to specify the number of folds in a `(Stratified)KFold`,- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.For integer/None inputs, if the estimator is a classifier and ``y`` iseither binary or multiclass, :class:`StratifiedKFold` is used. In allother cases, :class:`KFold` is used. These splitters are instantiatedwith `shuffle=False` so the splits will be the same across calls.Refer :ref:`User Guide <cross_validation>` for the variouscross-validation strategies that can be used here... versionchanged:: 0.22``cv`` default value if None changed from 3-fold to 5-fold.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Controls the verbosity: the higher, the more messages.",
                "dictKeyOf": "initkargs"
            },
            "pre_dispatch": {
                "type": "number",
                "desc": "Controls the number of jobs that get dispatched during parallelexecution. Reducing this number can be useful to avoid anexplosion of memory consumption when more jobs get dispatchedthan CPUs can process. This parameter can be:- None, in which case all the jobs are immediatelycreated and spawned. Use this for lightweight andfast-running jobs, to avoid delays due to on-demandspawning of the jobs- An int, giving the exact number of total jobs that arespawned- A str, giving an expression as a function of n_jobs,as in '2*n_jobs'",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Pseudo random number generator state used for random uniform samplingfrom lists of possible values instead of scipy.stats distributions.Pass an int for reproducible output across multiplefunction calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "error_score": {
                "type": "'raise' or numeric, default=np.nan",
                "desc": "Value to assign to the score if an error occurs in estimator fitting.If set to 'raise', the error is raised. If a numeric value is given,FitFailedWarning is raised. This parameter does not affect the refitstep, which will always raise the error.",
                "default": "np.nan",
                "dictKeyOf": "initkargs"
            },
            "return_train_score": {
                "type": "boolean",
                "desc": "If ``False``, the ``cv_results_`` attribute will not include trainingscores.Computing training scores is used to get insights on how differentparameter settings impact the overfitting/underfitting trade-off.However computing the scores on the training set can be computationallyexpensive and is not strictly required to select the parameters thatyield the best generalization performance... versionadded:: 0.19.. versionchanged:: 0.21Default value was changed from ``True`` to ``False``",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.RandomizedSearchCV",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.model_selection.RepeatedKFold": {
        "cls": "Parent",
        "typename": "RepeatedKFold",
        "desc": "Repeated K-Fold cross validator.  Repeats K-Fold n times with different randomization in each repetition.  Read more in the :ref:`User Guide <repeated_k_fold>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of folds. Must be at least 2.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "n_repeats": {
                "type": "number",
                "desc": "Number of times cross-validator needs to be repeated.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness of each repeated cross-validation instance.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.RepeatedKFold"
        }
    },
    "sklearn.model_selection.RepeatedStratifiedKFold": {
        "cls": "Parent",
        "typename": "RepeatedStratifiedKFold",
        "desc": "Repeated Stratified K-Fold cross validator.  Repeats Stratified K-Fold n times with different randomization in each repetition.  Read more in the :ref:`User Guide <repeated_k_fold>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of folds. Must be at least 2.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "n_repeats": {
                "type": "number",
                "desc": "Number of times cross-validator needs to be repeated.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the generation of the random states for each repetition.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.RepeatedStratifiedKFold"
        }
    },
    "sklearn.model_selection.ShuffleSplit": {
        "cls": "Parent",
        "typename": "ShuffleSplit",
        "desc": "Random permutation cross-validator  Yields indices to split data into training and test sets.  Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.  Read more in the :ref:`User Guide <ShuffleSplit>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of re-shuffling & splitting iterations.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "test_size": {
                "type": "number",
                "desc": "If float, should be between 0.0 and 1.0 and represent the proportionof the dataset to include in the test split. If int, represents theabsolute number of test samples. If None, the value is set to thecomplement of the train size. If ``train_size`` is also None, it willbe set to 0.1.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "train_size": {
                "type": "number",
                "desc": "If float, should be between 0.0 and 1.0 and represent theproportion of the dataset to include in the train split. Ifint, represents the absolute number of train samples. If None,the value is automatically set to the complement of the test size.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness of the training and testing indices produced.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.ShuffleSplit"
        }
    },
    "sklearn.model_selection.StratifiedKFold": {
        "cls": "Parent",
        "typename": "StratifiedKFold",
        "desc": "Stratified K-Folds cross-validator.  Provides train/test indices to split data in train/test sets.  This cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.  Read more in the :ref:`User Guide <stratified_k_fold>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of folds. Must be at least 2... versionchanged:: 0.22``n_splits`` default value changed from 3 to 5.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether to shuffle each class's samples before splitting into batches.Note that the samples within each split will not be shuffled.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "When `shuffle` is True, `random_state` affects the ordering of theindices, which controls the randomness of each fold for each class.Otherwise, leave `random_state` as `None`.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.StratifiedKFold"
        }
    },
    "sklearn.model_selection.StratifiedShuffleSplit": {
        "cls": "Parent",
        "typename": "StratifiedShuffleSplit",
        "desc": "Stratified ShuffleSplit cross-validator  Provides train/test indices to split data in train/test sets.  This cross-validation object is a merge of StratifiedKFold and ShuffleSplit, which returns stratified randomized folds. The folds are made by preserving the percentage of samples for each class.  Note: like the ShuffleSplit strategy, stratified random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.  Read more in the :ref:`User Guide <stratified_shuffle_split>`.",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of re-shuffling & splitting iterations.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "test_size": {
                "type": "number",
                "desc": "If float, should be between 0.0 and 1.0 and represent the proportionof the dataset to include in the test split. If int, represents theabsolute number of test samples. If None, the value is set to thecomplement of the train size. If ``train_size`` is also None, it willbe set to 0.1.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "train_size": {
                "type": "number",
                "desc": "If float, should be between 0.0 and 1.0 and represent theproportion of the dataset to include in the train split. Ifint, represents the absolute number of train samples. If None,the value is automatically set to the complement of the test size.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness of the training and testing indices produced.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.StratifiedShuffleSplit"
        }
    },
    "sklearn.model_selection.TimeSeriesSplit": {
        "cls": "Parent",
        "typename": "TimeSeriesSplit",
        "desc": "Time Series cross-validator  Provides train/test indices to split time series data samples that are observed at fixed time intervals, in train/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate.  This cross-validation object is a variation of :class:`KFold`. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.  Note that unlike standard cross-validation methods, successive training sets are supersets of those that come before them.  Read more in the :ref:`User Guide <time_series_split>`.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklSplitter",
        "pytype": "skll.plugin.sklearn.block.SklSplitter",
        "group": "sklearn.model_selection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_splits": {
                "type": "number",
                "desc": "Number of splits. Must be at least 2... versionchanged:: 0.22``n_splits`` default value changed from 3 to 5.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "max_train_size": {
                "type": "number",
                "desc": "Maximum size for a single training set.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "test_size": {
                "type": "number",
                "desc": "Used to limit the size of the test set. Defaults to``n_samples // (n_splits + 1)``, which is the maximum allowed valuewith ``gap=0``... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "gap": {
                "type": "number",
                "desc": "Number of samples to exclude from the end of each train set beforethe test set... versionadded:: 0.24",
                "default": "0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.model_selection.TimeSeriesSplit"
        }
    },
    "sklearn.multiclass.LabelBinarizer": {
        "cls": "Block",
        "typename": "LabelBinarizer",
        "desc": "Binarize labels in a one-vs-all fashion.  Several regression and binary classification algorithms are available in scikit-learn. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.  At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.  At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. LabelBinarizer makes this easy with the inverse_transform method.  Read more in the :ref:`User Guide <preprocessing_targets>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.multiclass",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "neg_label": {
                "type": "number",
                "desc": "Value with which negative labels must be encoded.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "pos_label": {
                "type": "number",
                "desc": "Value with which positive labels must be encoded.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "sparse_output": {
                "type": "boolean",
                "desc": "True if the returned array from transform is desired to be in sparseCSR format.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multiclass.LabelBinarizer"
        }
    },
    "sklearn.multiclass.OneVsOneClassifier": {
        "cls": "Parent",
        "typename": "OneVsOneClassifier",
        "desc": "One-vs-one multiclass strategy  This strategy consists in fitting one classifier per class pair. At prediction time, the class which received the most votes is selected. Since it requires to fit `n_classes * (n_classes - 1) / 2` classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. However, this method may be advantageous for algorithms such as kernel algorithms which don't scale well with `n_samples`. This is because each individual learning problem only involves a small subset of the data whereas, with one-vs-the-rest, the complete dataset is used `n_classes` times.  Read more in the :ref:`User Guide <ovo_classification>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.multiclass",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation: the `n_classes * (n_classes - 1) / 2` OVO problems are computed in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multiclass.OneVsOneClassifier",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.multiclass.OneVsRestClassifier": {
        "cls": "Parent",
        "typename": "OneVsRestClassifier",
        "desc": "One-vs-the-rest (OvR) multiclass strategy.  Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only `n_classes` classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.  OneVsRestClassifier can also be used for multilabel classification. To use this feature, provide an indicator matrix for the target `y` when calling `.fit`. In other words, the target labels should be formatted as a 2D binary (0/1) matrix, where [i, j] == 1 indicates the presence of label j in sample i. This estimator uses the binary relevance method to perform multilabel classification, which involves training one binary classifier independently for each label.  Read more in the :ref:`User Guide <ovr_classification>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.multiclass",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation: the `n_classes`one-vs-rest problems are computed in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionchanged:: v0.20`n_jobs` default changed from 1 to None",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multiclass.OneVsRestClassifier",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.multiclass.OutputCodeClassifier": {
        "cls": "Parent",
        "typename": "OutputCodeClassifier",
        "desc": "(Error-Correcting) Output-Code multiclass strategy  Output-code based strategies consist in representing each class with a binary code (an array of 0s and 1s). At fitting time, one binary classifier per bit in the code book is fitted.  At prediction time, the classifiers are used to project new points in the class space and the class closest to the points is chosen. The main advantage of these strategies is that the number of classifiers used can be controlled by the user, either for compressing the model (0 < code_size < 1) or for making the model more robust to errors (code_size > 1). See the documentation for more details.  Read more in the :ref:`User Guide <ecoc>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.multiclass",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "code_size": {
                "type": "number",
                "desc": "Percentage of the number of classes to be used to create the code book.A number between 0 and 1 will require fewer classifiers thanone-vs-the-rest. A number greater than 1 will require more classifiersthan one-vs-the-rest.",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "The generator used to initialize the codebook.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation: the multiclass problemsare computed in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multiclass.OutputCodeClassifier",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.multiclass.Parallel": {
        "cls": "Block",
        "typename": "Parallel",
        "desc": "Helper class for readable parallel mapping.  Read more in the :ref:`User Guide <parallel>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.multiclass",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            }
        },
        "defaults": {
            "cls": "sklearn.multiclass.Parallel"
        }
    },
    "sklearn.multiclass.deprecated": {
        "cls": "Block",
        "typename": "deprecated",
        "desc": "Decorator to mark a function or class as deprecated.  Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.  The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:  >>> from sklearn.utils import deprecated >>> deprecated() <sklearn.utils.deprecation.deprecated object at ...>  >>> @deprecated() ... def some_function(): pass",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.multiclass",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "extra": {
                "type": "string",
                "desc": "",
                "default": "''",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multiclass.deprecated"
        }
    },
    "sklearn.multioutput.ClassifierChain": {
        "cls": "Parent",
        "typename": "ClassifierChain",
        "desc": "A multi-label model that arranges binary classifiers into a chain.  Each model makes a prediction in the order specified by the chain using all of the available features provided to the model plus the predictions of models that are earlier in the chain.  Read more in the :ref:`User Guide <classifierchain>`.  .. versionadded:: 0.19",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.multioutput",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "order": {
                "type": "array-like of shape (n_outputs,) or 'random', default=None",
                "desc": "If None, the order will be determined by the order of columns inthe label matrix Y.::order = [0, 1, 2, ..., Y.shape[1] - 1]The order of the chain can be explicitly set by providing a list ofintegers. For example, for a chain of length 5.::order = [1, 3, 2, 4, 0]means that the first model in the chain will make predictions forcolumn 1 in the Y matrix, the second model will make predictionsfor column 3, etc.If order is 'random' a random ordering will be used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines whether to use cross validated predictions or truelabels for the results of previous estimators in the chain.Possible inputs for cv are:- None, to use true labels when fitting,- integer, to specify the number of folds in a (Stratified)KFold,- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "If ``order='random'``, determines random number generation for thechain order.In addition, it controls the random seed given at each `base_estimator`at each chaining iteration. Thus, it is only used when `base_estimator`exposes a `random_state`.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None)",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multioutput.ClassifierChain",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.multioutput.MultiOutputClassifier": {
        "cls": "Parent",
        "typename": "MultiOutputClassifier",
        "desc": "Multi target classification  This strategy consists of fitting one classifier per target. This is a simple strategy for extending classifiers that do not natively support multi-target classification",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.multioutput",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel.:meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supportedby the passed estimator) will be parallelized for each target.When individual estimators are fast to train or predict,using ``n_jobs > 1`` can result in slower performance dueto the parallelism overhead.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all available processes / threads.See :term:`Glossary <n_jobs>` for more details... versionchanged:: 0.20`n_jobs` default changed from 1 to None",
                "default": "None)",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multioutput.MultiOutputClassifier",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.multioutput.MultiOutputRegressor": {
        "cls": "Parent",
        "typename": "MultiOutputRegressor",
        "desc": "Multi target regression  This strategy consists of fitting one regressor per target. This is a simple strategy for extending regressors that do not natively support multi-target regression.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.multioutput",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to run in parallel.:meth:`fit`, :meth:`predict` and :meth:`partial_fit` (if supportedby the passed estimator) will be parallelized for each target.When individual estimators are fast to train or predict,using ``n_jobs > 1`` can result in slower performance dueto the parallelism overhead.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all available processes / threads.See :term:`Glossary <n_jobs>` for more details... versionchanged:: 0.20`n_jobs` default changed from 1 to None",
                "default": "None)",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multioutput.MultiOutputRegressor",
            "estname": "estimator",
            "multiple": false
        }
    },
    "sklearn.multioutput.Parallel": {
        "cls": "Block",
        "typename": "Parallel",
        "desc": "Helper class for readable parallel mapping.  Read more in the :ref:`User Guide <parallel>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.multioutput",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            }
        },
        "defaults": {
            "cls": "sklearn.multioutput.Parallel"
        }
    },
    "sklearn.multioutput.RegressorChain": {
        "cls": "Parent",
        "typename": "RegressorChain",
        "desc": "A multi-label model that arranges regressions into a chain.  Each model makes a prediction in the order specified by the chain using all of the available features provided to the model plus the predictions of models that are earlier in the chain.  Read more in the :ref:`User Guide <regressorchain>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.multioutput",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "order": {
                "type": "array-like of shape (n_outputs,) or 'random', default=None",
                "desc": "If None, the order will be determined by the order of columns inthe label matrix Y.::order = [0, 1, 2, ..., Y.shape[1] - 1]The order of the chain can be explicitly set by providing a list ofintegers. For example, for a chain of length 5.::order = [1, 3, 2, 4, 0]means that the first model in the chain will make predictions forcolumn 1 in the Y matrix, the second model will make predictionsfor column 3, etc.If order is 'random' a random ordering will be used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "cv": {
                "type": "number",
                "desc": "Determines whether to use cross validated predictions or truelabels for the results of previous estimators in the chain.Possible inputs for cv are:- None, to use true labels when fitting,- integer, to specify the number of folds in a (Stratified)KFold,- :term:`CV splitter`,- An iterable yielding (train, test) splits as arrays of indices.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "If ``order='random'``, determines random number generation for thechain order.In addition, it controls the random seed given at each `base_estimator`at each chaining iteration. Thus, it is only used when `base_estimator`exposes a `random_state`.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None)",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.multioutput.RegressorChain",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.naive_bayes.BernoulliNB": {
        "cls": "Block",
        "typename": "BernoulliNB",
        "desc": "Naive Bayes classifier for multivariate Bernoulli models.  Like MultinomialNB, this classifier is suitable for discrete data. The difference is that while MultinomialNB works with occurrence counts, BernoulliNB is designed for binary/boolean features.  Read more in the :ref:`User Guide <bernoulli_naive_bayes>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.naive_bayes",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Additive (Laplace/Lidstone) smoothing parameter(0 for no smoothing).",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "binarize": {
                "type": "number",
                "desc": "Threshold for binarizing (mapping to booleans) of sample features.If None, input is presumed to already consist of binary vectors.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "fit_prior": {
                "type": "boolean",
                "desc": "Whether to learn class prior probabilities or not.If false, a uniform prior will be used.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "class_prior": {
                "type": "array-like of shape (n_classes,), default=None",
                "desc": "Prior probabilities of the classes. If specified the priors are notadjusted according to the data.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.naive_bayes.BernoulliNB"
        }
    },
    "sklearn.naive_bayes.CategoricalNB": {
        "cls": "Block",
        "typename": "CategoricalNB",
        "desc": "Naive Bayes classifier for categorical features  The categorical Naive Bayes classifier is suitable for classification with discrete features that are categorically distributed. The categories of each feature are drawn from a categorical distribution.  Read more in the :ref:`User Guide <categorical_naive_bayes>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.naive_bayes",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Additive (Laplace/Lidstone) smoothing parameter(0 for no smoothing).",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_prior": {
                "type": "boolean",
                "desc": "Whether to learn class prior probabilities or not.If false, a uniform prior will be used.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "class_prior": {
                "type": "array-like of shape (n_classes,), default=None",
                "desc": "Prior probabilities of the classes. If specified the priors are notadjusted according to the data.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_categories": {
                "type": "number",
                "desc": "Minimum number of categories per feature.- integer: Sets the minimum number of categories per feature to`n_categories` for each features.- array-like: shape (n_features,) where `n_categories[i]` holds theminimum number of categories for the ith column of the input.- None (default): Determines the number of categories automaticallyfrom the training data... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.naive_bayes.CategoricalNB"
        }
    },
    "sklearn.naive_bayes.ComplementNB": {
        "cls": "Block",
        "typename": "ComplementNB",
        "desc": "The Complement Naive Bayes classifier described in Rennie et al. (2003).  The Complement Naive Bayes classifier was designed to correct the \"severe assumptions\" made by the standard Multinomial Naive Bayes classifier. It is particularly suited for imbalanced data sets.  Read more in the :ref:`User Guide <complement_naive_bayes>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.naive_bayes",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing).",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_prior": {
                "type": "boolean",
                "desc": "Only used in edge case with a single class in the training set.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "class_prior": {
                "type": "array-like of shape (n_classes,), default=None",
                "desc": "Prior probabilities of the classes. Not used.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "norm": {
                "type": "boolean",
                "desc": "Whether or not a second normalization of the weights is performed. Thedefault behavior mirrors the implementations found in Mahout and Weka,which do not follow the full algorithm described in Table 9 of thepaper.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.naive_bayes.ComplementNB"
        }
    },
    "sklearn.naive_bayes.GaussianNB": {
        "cls": "Block",
        "typename": "GaussianNB",
        "desc": "Gaussian Naive Bayes (GaussianNB)  Can perform online updates to model parameters via :meth:`partial_fit`. For details on algorithm used to update feature means and variance online, see Stanford CS tech report STAN-CS-79-773 by Chan, Golub, and LeVeque:      http://i.stanford.edu/pub/cstr/reports/cs/tr/79/773/CS-TR-79-773.pdf  Read more in the :ref:`User Guide <gaussian_naive_bayes>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.naive_bayes",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "priors": {
                "type": "array-like of shape (n_classes,)",
                "desc": "Prior probabilities of the classes. If specified the priors are notadjusted according to the data.",
                "dictKeyOf": "initkargs"
            },
            "var_smoothing": {
                "type": "number",
                "desc": "Portion of the largest variance of all features that is added tovariances for calculation stability... versionadded:: 0.20",
                "default": "1e-9",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.naive_bayes.GaussianNB"
        }
    },
    "sklearn.naive_bayes.LabelBinarizer": {
        "cls": "Block",
        "typename": "LabelBinarizer",
        "desc": "Binarize labels in a one-vs-all fashion.  Several regression and binary classification algorithms are available in scikit-learn. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.  At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.  At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. LabelBinarizer makes this easy with the inverse_transform method.  Read more in the :ref:`User Guide <preprocessing_targets>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.naive_bayes",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "neg_label": {
                "type": "number",
                "desc": "Value with which negative labels must be encoded.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "pos_label": {
                "type": "number",
                "desc": "Value with which positive labels must be encoded.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "sparse_output": {
                "type": "boolean",
                "desc": "True if the returned array from transform is desired to be in sparseCSR format.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.naive_bayes.LabelBinarizer"
        }
    },
    "sklearn.naive_bayes.MultinomialNB": {
        "cls": "Block",
        "typename": "MultinomialNB",
        "desc": "Naive Bayes classifier for multinomial models  The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.  Read more in the :ref:`User Guide <multinomial_naive_bayes>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.naive_bayes",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "alpha": {
                "type": "number",
                "desc": "Additive (Laplace/Lidstone) smoothing parameter(0 for no smoothing).",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "fit_prior": {
                "type": "boolean",
                "desc": "Whether to learn class prior probabilities or not.If false, a uniform prior will be used.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "class_prior": {
                "type": "array-like of shape (n_classes,), default=None",
                "desc": "Prior probabilities of the classes. If specified the priors are notadjusted according to the data.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.naive_bayes.MultinomialNB"
        }
    },
    "sklearn.naive_bayes.deprecated": {
        "cls": "Block",
        "typename": "deprecated",
        "desc": "Decorator to mark a function or class as deprecated.  Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.  The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:  >>> from sklearn.utils import deprecated >>> deprecated() <sklearn.utils.deprecation.deprecated object at ...>  >>> @deprecated() ... def some_function(): pass",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.naive_bayes",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "extra": {
                "type": "string",
                "desc": "",
                "default": "''",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.naive_bayes.deprecated"
        }
    },
    "sklearn.neighbors.BallTree": {
        "cls": "Block",
        "typename": "BallTree",
        "desc": "BallTree(X, leaf_size=40, metric='minkowski', **kwargs)  BallTree for fast generalized N-point problems  Read more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "desc": "n_samples is the number of points in the data set, andn_features is the dimension of the parameter space.Note: if X is a C-contiguous array of doubles then data willnot be copied. Otherwise, an internal copy will be made.",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "positive int, default=40",
                "desc": "Number of points at which to switch to brute-force. Changingleaf_size will not affect the results of a query, but cansignificantly impact the speed of a query and the memory requiredto store the constructed tree. The amount of memory needed tostore the tree scales as approximately n_samples / leaf_size.For a specified ``leaf_size``, a leaf node is guaranteed tosatisfy ``leaf_size <= n_points <= 2 * leaf_size``, except inthe case that ``n_samples < leaf_size``.",
                "default": "40",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "the distance metric to use for the tree. Default='minkowski'with p=2 (that is, a euclidean metric). See the documentationof the DistanceMetric class for a list of available metrics.ball_tree.valid_metrics gives a list of the metrics whichare valid for BallTree.Additional keywords are passed to the distance metric class.Note: Callable functions in the metric parameter are NOT supported for KDTreeand Ball Tree. Function call overhead will result in very poor performance.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.BallTree"
        }
    },
    "sklearn.neighbors.KDTree": {
        "cls": "Block",
        "typename": "KDTree",
        "desc": "KDTree(X, leaf_size=40, metric='minkowski', **kwargs)  KDTree for fast generalized N-point problems  Read more in the :ref:`User Guide <unsupervised_neighbors>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "X": {
                "type": "array-like of shape (n_samples, n_features)",
                "desc": "n_samples is the number of points in the data set, andn_features is the dimension of the parameter space.Note: if X is a C-contiguous array of doubles then data willnot be copied. Otherwise, an internal copy will be made.",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "positive int, default=40",
                "desc": "Number of points at which to switch to brute-force. Changingleaf_size will not affect the results of a query, but cansignificantly impact the speed of a query and the memory requiredto store the constructed tree. The amount of memory needed tostore the tree scales as approximately n_samples / leaf_size.For a specified ``leaf_size``, a leaf node is guaranteed tosatisfy ``leaf_size <= n_points <= 2 * leaf_size``, except inthe case that ``n_samples < leaf_size``.",
                "default": "40",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "the distance metric to use for the tree. Default='minkowski'with p=2 (that is, a euclidean metric). See the documentationof the DistanceMetric class for a list of available metrics.kd_tree.valid_metrics gives a list of the metrics whichare valid for KDTree.Additional keywords are passed to the distance metric class.Note: Callable functions in the metric parameter are NOT supported for KDTreeand Ball Tree. Function call overhead will result in very poor performance.",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.KDTree"
        }
    },
    "sklearn.neighbors.KNeighborsClassifier": {
        "cls": "Block",
        "typename": "KNeighborsClassifier",
        "desc": "Classifier implementing the k-nearest neighbors vote.  Read more in the :ref:`User Guide <classification>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of neighbors to use by default for :meth:`kneighbors` queries.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "weights": {
                "type": "option(uniform, distance)",
                "desc": "weight function used in prediction. Possible values:- 'uniform' : uniform weights. All points in each neighborhoodare weighted equally.- 'distance' : weight points by the inverse of their distance.in this case, closer neighbors of a query point will have agreater influence than neighbors which are further away.- [callable] : a user-defined function which accepts anarray of distances, and returns an array of the same shapecontaining the weights.",
                "default": "'uniform'",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or KDTree. This can affect thespeed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Power parameter for the Minkowski metric. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "the distance metric to use for the tree. The default metric isminkowski, and with p=2 is equivalent to the standard Euclideanmetric. See the documentation of :class:`DistanceMetric` for alist of available metrics.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square during fit. X may be a :term:`sparse graph`,in which case only \"nonzero\" elements may be considered neighbors.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.Doesn't affect :meth:`fit` method.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.KNeighborsClassifier"
        }
    },
    "sklearn.neighbors.KNeighborsRegressor": {
        "cls": "Block",
        "typename": "KNeighborsRegressor",
        "desc": "Regression based on k-nearest neighbors.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.  .. versionadded:: 0.9",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of neighbors to use by default for :meth:`kneighbors` queries.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "weights": {
                "type": "option(uniform, distance)",
                "desc": "weight function used in prediction. Possible values:- 'uniform' : uniform weights. All points in each neighborhoodare weighted equally.- 'distance' : weight points by the inverse of their distance.in this case, closer neighbors of a query point will have agreater influence than neighbors which are further away.- [callable] : a user-defined function which accepts anarray of distances, and returns an array of the same shapecontaining the weights.Uniform weights are used by default.",
                "default": "'uniform'",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or KDTree. This can affect thespeed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Power parameter for the Minkowski metric. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "the distance metric to use for the tree. The default metric isminkowski, and with p=2 is equivalent to the standard Euclideanmetric. See the documentation of :class:`DistanceMetric` for alist of available metrics.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square during fit. X may be a :term:`sparse graph`,in which case only \"nonzero\" elements may be considered neighbors.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.Doesn't affect :meth:`fit` method.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.KNeighborsRegressor"
        }
    },
    "sklearn.neighbors.KNeighborsTransformer": {
        "cls": "Block",
        "typename": "KNeighborsTransformer",
        "desc": "Transform X into a (weighted) graph of k nearest neighbors  The transformed data is a sparse graph as returned by kneighbors_graph.  Read more in the :ref:`User Guide <neighbors_transformer>`.  .. versionadded:: 0.22",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "mode": {
                "type": "option(distance, connectivity)",
                "desc": "Type of returned matrix: 'connectivity' will return the connectivitymatrix with ones and zeros, and 'distance' will return the distancesbetween neighbors according to the given metric.",
                "default": "'distance'",
                "dictKeyOf": "initkargs"
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of neighbors for each sample in the transformed sparse graph.For compatibility reasons, as each sample is considered as its ownneighbor, one extra neighbor will be computed when mode == 'distance'.In this case, the sparse graph contains (n_neighbors + 1) neighbors.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or KDTree. This can affect thespeed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "metric to use for distance computation. Any metric from scikit-learnor scipy.spatial.distance can be used.If metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays as input and return one value indicating thedistance between them. This works for Scipy's metrics, but is lessefficient than passing the metric name as a string.Distance matrices are not supported.Valid values for metric are:- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2','manhattan']- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev','correlation', 'dice', 'hamming', 'jaccard', 'kulsinski','mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao','seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean','yule']See the documentation for scipy.spatial.distance for details on thesemetrics.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Parameter for the Minkowski metric fromsklearn.metrics.pairwise.pairwise_distances. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.If ``-1``, then the number of jobs is set to the number of CPU cores.",
                "default": "1",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.KNeighborsTransformer"
        }
    },
    "sklearn.neighbors.KernelDensity": {
        "cls": "Block",
        "typename": "KernelDensity",
        "desc": "Kernel Density Estimation.  Read more in the :ref:`User Guide <kernel_density>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "bandwidth": {
                "type": "number",
                "desc": "The bandwidth of the kernel.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(kd_tree, ball_tree, auto)",
                "desc": "The tree algorithm to use.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "kernel": {
                "type": "option(gaussian, tophat, epanechnikov, exponential, linear,                  cosine)",
                "desc": "The kernel to use.",
                "default": "'gaussian'",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "The distance metric to use. Note that not all metrics arevalid with all algorithms. Refer to the documentation of:class:`BallTree` and :class:`KDTree` for a description ofavailable algorithms. Note that the normalization of the densityoutput is correct only for the Euclidean distance metric. Defaultis 'euclidean'.",
                "default": "'euclidean'",
                "dictKeyOf": "initkargs"
            },
            "atol": {
                "type": "number",
                "desc": "The desired absolute tolerance of the result. A larger tolerance willgenerally lead to faster execution.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "rtol": {
                "type": "number",
                "desc": "The desired relative tolerance of the result. A larger tolerance willgenerally lead to faster execution.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "breadth_first": {
                "type": "boolean",
                "desc": "If true (default), use a breadth-first approach to the problem.Otherwise use a depth-first approach.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Specify the leaf size of the underlying tree. See :class:`BallTree`or :class:`KDTree` for details.",
                "default": "40",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional parameters to be passed to the tree for use with themetric. For more information, see the documentation of:class:`BallTree` or :class:`KDTree`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.KernelDensity"
        }
    },
    "sklearn.neighbors.LocalOutlierFactor": {
        "cls": "Block",
        "typename": "LocalOutlierFactor",
        "desc": "Unsupervised Outlier Detection using Local Outlier Factor (LOF)  The anomaly score of each sample is called Local Outlier Factor. It measures the local deviation of density of a given sample with respect to its neighbors. It is local in that the anomaly score depends on how isolated the object is with respect to the surrounding neighborhood. More precisely, locality is given by k-nearest neighbors, whose distance is used to estimate the local density. By comparing the local density of a sample to the local densities of its neighbors, one can identify samples that have a substantially lower density than their neighbors. These are considered outliers.  .. versionadded:: 0.19",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of neighbors to use by default for :meth:`kneighbors` queries.If n_neighbors is larger than the number of samples provided,all samples will be used.",
                "default": "20",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to :class:`BallTree` or :class:`KDTree`. This canaffect the speed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "metric used for the distance computation. Any metric from scikit-learnor scipy.spatial.distance can be used.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square. X may be a sparse matrix, in which case only \"nonzero\"elements may be considered neighbors.If metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays as input and return one value indicating thedistance between them. This works for Scipy's metrics, but is lessefficient than passing the metric name as a string.Valid values for metric are:- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2','manhattan']- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev','correlation', 'dice', 'hamming', 'jaccard', 'kulsinski','mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao','seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean','yule']See the documentation for scipy.spatial.distance for details on thesemetrics:https://docs.scipy.org/doc/scipy/reference/spatial.distance.html",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Parameter for the Minkowski metric from:func:`sklearn.metrics.pairwise.pairwise_distances`. When p = 1, thisis equivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "contamination": {
                "type": "'auto' or float, default='auto'",
                "desc": "The amount of contamination of the data set, i.e. the proportionof outliers in the data set. When fitting this is used to define thethreshold on the scores of the samples.- if 'auto', the threshold is determined as in theoriginal paper,- if a float, the contamination should be in the range [0, 0.5]... versionchanged:: 0.22The default value of ``contamination`` changed from 0.1to ``'auto'``.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "novelty": {
                "type": "boolean",
                "desc": "By default, LocalOutlierFactor is only meant to be used for outlierdetection (novelty=False). Set novelty to True if you want to useLocalOutlierFactor for novelty detection. In this case be aware thatyou should only use predict, decision_function and score_sampleson new unseen data and not on the training set... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.LocalOutlierFactor"
        }
    },
    "sklearn.neighbors.NearestCentroid": {
        "cls": "Block",
        "typename": "NearestCentroid",
        "desc": "Nearest centroid classifier.  Each class is represented by its centroid, with test samples classified to the class with the nearest centroid.  Read more in the :ref:`User Guide <nearest_centroid_classifier>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string or callable, it must be one ofthe options allowed by metrics.pairwise.pairwise_distances for itsmetric parameter.The centroids for the samples corresponding to each class is the pointfrom which the sum of the distances (according to the metric) of allsamples that belong to that particular class are minimized.If the \"manhattan\" metric is provided, this centroid is the median andfor all other metrics, the centroid is now set to be the mean... versionchanged:: 0.19``metric='precomputed'`` was deprecated and now raises an error",
                "dictKeyOf": "initkargs"
            },
            "shrink_threshold": {
                "type": "number",
                "desc": "Threshold for shrinking centroids to remove features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.NearestCentroid"
        }
    },
    "sklearn.neighbors.NearestNeighbors": {
        "cls": "Block",
        "typename": "NearestNeighbors",
        "desc": "Unsupervised learner for implementing neighbor searches.  Read more in the :ref:`User Guide <unsupervised_neighbors>`.  .. versionadded:: 0.9",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Number of neighbors to use by default for :meth:`kneighbors` queries.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "radius": {
                "type": "number",
                "desc": "Range of parameter space to use by default for :meth:`radius_neighbors`queries.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or KDTree. This can affect thespeed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "the distance metric to use for the tree. The default metric isminkowski, and with p=2 is equivalent to the standard Euclideanmetric. See the documentation of :class:`DistanceMetric` for alist of available metrics.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square during fit. X may be a :term:`sparse graph`,in which case only \"nonzero\" elements may be considered neighbors.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Parameter for the Minkowski metric fromsklearn.metrics.pairwise.pairwise_distances. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.NearestNeighbors"
        }
    },
    "sklearn.neighbors.NeighborhoodComponentsAnalysis": {
        "cls": "Block",
        "typename": "NeighborhoodComponentsAnalysis",
        "desc": "Neighborhood Components Analysis  Neighborhood Component Analysis (NCA) is a machine learning algorithm for metric learning. It learns a linear transformation in a supervised fashion to improve the classification accuracy of a stochastic nearest neighbors rule in the transformed space.  Read more in the :ref:`User Guide <nca>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Preferred dimensionality of the projected space.If None it will be set to ``n_features``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "init": {
                "type": "option(auto, pca, lda, identity, random)",
                "desc": "Initialization of the linear transformation. Possible options are'auto', 'pca', 'lda', 'identity', 'random', and a numpy array of shape(n_features_a, n_features_b).'auto'Depending on ``n_components``, the most reasonable initializationwill be chosen. If ``n_components <= n_classes`` we use 'lda', asit uses labels information. If not, but``n_components < min(n_features, n_samples)``, we use 'pca', asit projects data in meaningful directions (those of highervariance). Otherwise, we just use 'identity'.'pca'``n_components`` principal components of the inputs passedto :meth:`fit` will be used to initialize the transformation.(See :class:`~sklearn.decomposition.PCA`)'lda'``min(n_components, n_classes)`` most discriminativecomponents of the inputs passed to :meth:`fit` will be used toinitialize the transformation. (If ``n_components > n_classes``,the rest of the components will be zero.) (See:class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`)'identity'If ``n_components`` is strictly smaller than thedimensionality of the inputs passed to :meth:`fit`, the identitymatrix will be truncated to the first ``n_components`` rows.'random'The initial transformation will be a random array of shape`(n_components, n_features)`. Each value is sampled from thestandard normal distribution.numpy arrayn_features_b must match the dimensionality of the inputs passed to:meth:`fit` and n_features_a must be less than or equal to that.If ``n_components`` is not None, n_features_a must match it.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "If True and :meth:`fit` has been called before, the solution of theprevious call to :meth:`fit` is used as the initial lineartransformation (``n_components`` and ``init`` will be ignored).",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations in the optimization.",
                "default": "50",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Convergence tolerance for the optimization.",
                "default": "1e-5",
                "dictKeyOf": "initkargs"
            },
            "callback": {
                "type": "callable, default=None",
                "desc": "If not None, this function is called after every iteration of theoptimizer, taking as arguments the current solution (flattenedtransformation matrix) and the number of iterations. This might beuseful in case one wants to examine or store the transformationfound after each iteration.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "If 0, no progress messages will be printed.If 1, progress messages will be printed to stdout.If > 1, progress messages will be printed and the ``disp``parameter of :func:`scipy.optimize.minimize` will be set to``verbose - 2``.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "A pseudo random number generator object or a seed for it if int. If``init='random'``, ``random_state`` is used to initialize the randomtransformation. If ``init='pca'``, ``random_state`` is passed as anargument to PCA when initializing the transformation. Pass an intfor reproducible results across multiple function calls.See :term: `Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.NeighborhoodComponentsAnalysis"
        }
    },
    "sklearn.neighbors.RadiusNeighborsClassifier": {
        "cls": "Block",
        "typename": "RadiusNeighborsClassifier",
        "desc": "Classifier implementing a vote among neighbors within a given radius  Read more in the :ref:`User Guide <classification>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "radius": {
                "type": "number",
                "desc": "Range of parameter space to use by default for :meth:`radius_neighbors`queries.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "weights": {
                "type": "option(uniform, distance)",
                "desc": "weight function used in prediction. Possible values:- 'uniform' : uniform weights. All points in each neighborhoodare weighted equally.- 'distance' : weight points by the inverse of their distance.in this case, closer neighbors of a query point will have agreater influence than neighbors which are further away.- [callable] : a user-defined function which accepts anarray of distances, and returns an array of the same shapecontaining the weights.Uniform weights are used by default.",
                "default": "'uniform'",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or KDTree. This can affect thespeed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Power parameter for the Minkowski metric. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "the distance metric to use for the tree. The default metric isminkowski, and with p=2 is equivalent to the standard Euclideanmetric. See the documentation of :class:`DistanceMetric` for alist of available metrics.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square during fit. X may be a :term:`sparse graph`,in which case only \"nonzero\" elements may be considered neighbors.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "outlier_label": {
                "type": "option(manual label, most_frequent)",
                "desc": "label for outlier samples (samples with no neighbors in given radius).- manual label: str or int label (should be the same type as y)or list of manual labels if multi-output is used.- 'most_frequent' : assign the most frequent label of y to outliers.- None : when any outlier is detected, ValueError will be raised.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.RadiusNeighborsClassifier"
        }
    },
    "sklearn.neighbors.RadiusNeighborsRegressor": {
        "cls": "Block",
        "typename": "RadiusNeighborsRegressor",
        "desc": "Regression based on neighbors within a fixed radius.  The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.  Read more in the :ref:`User Guide <regression>`.  .. versionadded:: 0.9",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "radius": {
                "type": "number",
                "desc": "Range of parameter space to use by default for :meth:`radius_neighbors`queries.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "weights": {
                "type": "option(uniform, distance)",
                "desc": "weight function used in prediction. Possible values:- 'uniform' : uniform weights. All points in each neighborhoodare weighted equally.- 'distance' : weight points by the inverse of their distance.in this case, closer neighbors of a query point will have agreater influence than neighbors which are further away.- [callable] : a user-defined function which accepts anarray of distances, and returns an array of the same shapecontaining the weights.Uniform weights are used by default.",
                "default": "'uniform'",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or KDTree. This can affect thespeed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Power parameter for the Minkowski metric. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "the distance metric to use for the tree. The default metric isminkowski, and with p=2 is equivalent to the standard Euclideanmetric. See the documentation of :class:`DistanceMetric` for alist of available metrics.If metric is \"precomputed\", X is assumed to be a distance matrix andmust be square during fit. X may be a :term:`sparse graph`,in which case only \"nonzero\" elements may be considered neighbors.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.RadiusNeighborsRegressor"
        }
    },
    "sklearn.neighbors.RadiusNeighborsTransformer": {
        "cls": "Block",
        "typename": "RadiusNeighborsTransformer",
        "desc": "Transform X into a (weighted) graph of neighbors nearer than a radius  The transformed data is a sparse graph as returned by radius_neighbors_graph.  Read more in the :ref:`User Guide <neighbors_transformer>`.  .. versionadded:: 0.22",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neighbors",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "mode": {
                "type": "option(distance, connectivity)",
                "desc": "Type of returned matrix: 'connectivity' will return the connectivitymatrix with ones and zeros, and 'distance' will return the distancesbetween neighbors according to the given metric.",
                "default": "'distance'",
                "dictKeyOf": "initkargs"
            },
            "radius": {
                "type": "number",
                "desc": "Radius of neighborhood in the transformed sparse graph.",
                "default": "1.",
                "dictKeyOf": "initkargs"
            },
            "algorithm": {
                "type": "option(auto, ball_tree, kd_tree, brute)",
                "desc": "Algorithm used to compute the nearest neighbors:- 'ball_tree' will use :class:`BallTree`- 'kd_tree' will use :class:`KDTree`- 'brute' will use a brute-force search.- 'auto' will attempt to decide the most appropriate algorithmbased on the values passed to :meth:`fit` method.Note: fitting on sparse input will override the setting ofthis parameter, using brute force.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "leaf_size": {
                "type": "number",
                "desc": "Leaf size passed to BallTree or KDTree. This can affect thespeed of the construction and query, as well as the memoryrequired to store the tree. The optimal value depends on thenature of the problem.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "metric": {
                "type": "string",
                "desc": "metric to use for distance computation. Any metric from scikit-learnor scipy.spatial.distance can be used.If metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays as input and return one value indicating thedistance between them. This works for Scipy's metrics, but is lessefficient than passing the metric name as a string.Distance matrices are not supported.Valid values for metric are:- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2','manhattan']- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev','correlation', 'dice', 'hamming', 'jaccard', 'kulsinski','mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao','seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean','yule']See the documentation for scipy.spatial.distance for details on thesemetrics.",
                "default": "'minkowski'",
                "dictKeyOf": "initkargs"
            },
            "p": {
                "type": "number",
                "desc": "Parameter for the Minkowski metric fromsklearn.metrics.pairwise.pairwise_distances. When p = 1, this isequivalent to using manhattan_distance (l1), and euclidean_distance(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "metric_params": {
                "type": "dict, default=None",
                "desc": "Additional keyword arguments for the metric function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run for neighbors search.If ``-1``, then the number of jobs is set to the number of CPU cores.",
                "default": "1",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neighbors.RadiusNeighborsTransformer"
        }
    },
    "sklearn.neural_network.BernoulliRBM": {
        "cls": "Block",
        "typename": "BernoulliRBM",
        "desc": "Bernoulli Restricted Boltzmann Machine (RBM).  A Restricted Boltzmann Machine with binary visible units and binary hidden units. Parameters are estimated using Stochastic Maximum Likelihood (SML), also known as Persistent Contrastive Divergence (PCD) [2].  The time complexity of this implementation is ``O(d ** 2)`` assuming d ~ n_features ~ n_components.  Read more in the :ref:`User Guide <rbm>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neural_network",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Number of binary hidden units.",
                "default": "256",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "number",
                "desc": "The learning rate for weight updates. It is *highly* recommendedto tune this hyper-parameter. Reasonable values are in the10**[0., -3.] range.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "Number of examples per minibatch.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "n_iter": {
                "type": "number",
                "desc": "Number of iterations/sweeps over the training dataset to performduring training.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "The verbosity level. The default, zero, means silent mode.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for:- Gibbs sampling from visible and hidden layers.- Initializing components, sampling from layers during fit.- Corrupting the data when scoring samples.Pass an int for reproducible results across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neural_network.BernoulliRBM"
        }
    },
    "sklearn.neural_network.MLPClassifier": {
        "cls": "Block",
        "typename": "MLPClassifier",
        "desc": "Multi-layer Perceptron classifier.  This model optimizes the log-loss function using LBFGS or stochastic gradient descent.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neural_network",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "hidden_layer_sizes": {
                "type": "tuple, length = n_layers - 2, default=(100,)",
                "desc": "The ith element represents the number of neurons in the ithhidden layer.",
                "default": "(100",
                "dictKeyOf": "initkargs"
            },
            "activation": {
                "type": "option(identity, logistic, tanh, relu)",
                "desc": "Activation function for the hidden layer.- 'identity', no-op activation, useful to implement linear bottleneck,returns f(x) = x- 'logistic', the logistic sigmoid function,returns f(x) = 1 / (1 + exp(-x)).- 'tanh', the hyperbolic tan function,returns f(x) = tanh(x).- 'relu', the rectified linear unit function,returns f(x) = max(0, x)",
                "default": "'relu'",
                "dictKeyOf": "initkargs"
            },
            "solver": {
                "type": "option(lbfgs, sgd, adam)",
                "desc": "The solver for weight optimization.- 'lbfgs' is an optimizer in the family of quasi-Newton methods.- 'sgd' refers to stochastic gradient descent.- 'adam' refers to a stochastic gradient-based optimizer proposedby Kingma, Diederik, and Jimmy BaNote: The default solver 'adam' works pretty well on relativelylarge datasets (with thousands of training samples or more) in terms ofboth training time and validation score.For small datasets, however, 'lbfgs' can converge faster and performbetter.",
                "default": "'adam'",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "L2 penalty (regularization term) parameter.",
                "default": "0.0001",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "Size of minibatches for stochastic optimizers.If the solver is 'lbfgs', the classifier will not use minibatch.When set to \"auto\", `batch_size=min(200, n_samples)`",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "option(constant, invscaling, adaptive)",
                "desc": "Learning rate schedule for weight updates.- 'constant' is a constant learning rate given by'learning_rate_init'.- 'invscaling' gradually decreases the learning rate at eachtime step 't' using an inverse scaling exponent of 'power_t'.effective_learning_rate = learning_rate_init / pow(t, power_t)- 'adaptive' keeps the learning rate constant to'learning_rate_init' as long as training loss keeps decreasing.Each time two consecutive epochs fail to decrease training loss by atleast tol, or fail to increase validation score by at least tol if'early_stopping' is on, the current learning rate is divided by 5.Only used when ``solver='sgd'``.",
                "default": "'constant'",
                "dictKeyOf": "initkargs"
            },
            "learning_rate_init": {
                "type": "double, default=0.001",
                "desc": "The initial learning rate used. It controls the step-sizein updating the weights. Only used when solver='sgd' or 'adam'.",
                "default": "0.001",
                "dictKeyOf": "initkargs"
            },
            "power_t": {
                "type": "double, default=0.5",
                "desc": "The exponent for inverse scaling learning rate.It is used in updating effective learning rate when the learning_rateis set to 'invscaling'. Only used when solver='sgd'.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations. The solver iterates until convergence(determined by 'tol') or this number of iterations. For stochasticsolvers ('sgd', 'adam'), note that this determines the number of epochs(how many times each data point will be used), not the number ofgradient steps.",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether to shuffle samples in each iteration. Only used whensolver='sgd' or 'adam'.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for weights and biasinitialization, train-test split if early stopping is used, and batchsampling when solver='sgd' or 'adam'.Pass an int for reproducible results across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for the optimization. When the loss or score is not improvingby at least ``tol`` for ``n_iter_no_change`` consecutive iterations,unless ``learning_rate`` is set to 'adaptive', convergence isconsidered to be reached and training stops.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Whether to print progress messages to stdout.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previouscall to fit as initialization, otherwise, just erase theprevious solution. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "momentum": {
                "type": "number",
                "desc": "Momentum for gradient descent update. Should be between 0 and 1. Onlyused when solver='sgd'.",
                "default": "0.9",
                "dictKeyOf": "initkargs"
            },
            "nesterovs_momentum": {
                "type": "boolean",
                "desc": "Whether to use Nesterov's momentum. Only used when solver='sgd' andmomentum > 0.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "early_stopping": {
                "type": "boolean",
                "desc": "Whether to use early stopping to terminate training when validationscore is not improving. If set to true, it will automatically setaside 10% of training data as validation and terminate training whenvalidation score is not improving by at least tol for``n_iter_no_change`` consecutive epochs. The split is stratified,except in a multilabel setting.If early stopping is False, then the training stops when the trainingloss does not improve by more than tol for n_iter_no_change consecutivepasses over the training set.Only effective when solver='sgd' or 'adam'",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if early_stopping is True",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "beta_1": {
                "type": "number",
                "desc": "Exponential decay rate for estimates of first moment vector in adam,should be in [0, 1). Only used when solver='adam'",
                "default": "0.9",
                "dictKeyOf": "initkargs"
            },
            "beta_2": {
                "type": "number",
                "desc": "Exponential decay rate for estimates of second moment vector in adam,should be in [0, 1). Only used when solver='adam'",
                "default": "0.999",
                "dictKeyOf": "initkargs"
            },
            "epsilon": {
                "type": "number",
                "desc": "Value for numerical stability in adam. Only used when solver='adam'",
                "default": "1e-8",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "Maximum number of epochs to not meet ``tol`` improvement.Only effective when solver='sgd' or 'adam'.. versionadded:: 0.20",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "max_fun": {
                "type": "number",
                "desc": "Only used when solver='lbfgs'. Maximum number of loss function calls.The solver iterates until convergence (determined by 'tol'), numberof iterations reaches max_iter, or this number of loss function calls.Note that number of loss function calls will be greater than or equalto the number of iterations for the `MLPClassifier`... versionadded:: 0.22",
                "default": "15000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neural_network.MLPClassifier"
        }
    },
    "sklearn.neural_network.MLPRegressor": {
        "cls": "Block",
        "typename": "MLPRegressor",
        "desc": "Multi-layer Perceptron regressor.  This model optimizes the squared-loss using LBFGS or stochastic gradient descent.  .. versionadded:: 0.18",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.neural_network",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "hidden_layer_sizes": {
                "type": "tuple, length = n_layers - 2, default=(100,)",
                "desc": "The ith element represents the number of neurons in the ithhidden layer.",
                "default": "(100",
                "dictKeyOf": "initkargs"
            },
            "activation": {
                "type": "option(identity, logistic, tanh, relu)",
                "desc": "Activation function for the hidden layer.- 'identity', no-op activation, useful to implement linear bottleneck,returns f(x) = x- 'logistic', the logistic sigmoid function,returns f(x) = 1 / (1 + exp(-x)).- 'tanh', the hyperbolic tan function,returns f(x) = tanh(x).- 'relu', the rectified linear unit function,returns f(x) = max(0, x)",
                "default": "'relu'",
                "dictKeyOf": "initkargs"
            },
            "solver": {
                "type": "option(lbfgs, sgd, adam)",
                "desc": "The solver for weight optimization.- 'lbfgs' is an optimizer in the family of quasi-Newton methods.- 'sgd' refers to stochastic gradient descent.- 'adam' refers to a stochastic gradient-based optimizer proposed byKingma, Diederik, and Jimmy BaNote: The default solver 'adam' works pretty well on relativelylarge datasets (with thousands of training samples or more) in terms ofboth training time and validation score.For small datasets, however, 'lbfgs' can converge faster and performbetter.",
                "default": "'adam'",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "L2 penalty (regularization term) parameter.",
                "default": "0.0001",
                "dictKeyOf": "initkargs"
            },
            "batch_size": {
                "type": "number",
                "desc": "Size of minibatches for stochastic optimizers.If the solver is 'lbfgs', the classifier will not use minibatch.When set to \"auto\", `batch_size=min(200, n_samples)`",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "learning_rate": {
                "type": "option(constant, invscaling, adaptive)",
                "desc": "Learning rate schedule for weight updates.- 'constant' is a constant learning rate given by'learning_rate_init'.- 'invscaling' gradually decreases the learning rate ``learning_rate_``at each time step 't' using an inverse scaling exponent of 'power_t'.effective_learning_rate = learning_rate_init / pow(t, power_t)- 'adaptive' keeps the learning rate constant to'learning_rate_init' as long as training loss keeps decreasing.Each time two consecutive epochs fail to decrease training loss by atleast tol, or fail to increase validation score by at least tol if'early_stopping' is on, the current learning rate is divided by 5.Only used when solver='sgd'.",
                "default": "'constant'",
                "dictKeyOf": "initkargs"
            },
            "learning_rate_init": {
                "type": "double, default=0.001",
                "desc": "The initial learning rate used. It controls the step-sizein updating the weights. Only used when solver='sgd' or 'adam'.",
                "default": "0.001",
                "dictKeyOf": "initkargs"
            },
            "power_t": {
                "type": "double, default=0.5",
                "desc": "The exponent for inverse scaling learning rate.It is used in updating effective learning rate when the learning_rateis set to 'invscaling'. Only used when solver='sgd'.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations. The solver iterates until convergence(determined by 'tol') or this number of iterations. For stochasticsolvers ('sgd', 'adam'), note that this determines the number of epochs(how many times each data point will be used), not the number ofgradient steps.",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "shuffle": {
                "type": "boolean",
                "desc": "Whether to shuffle samples in each iteration. Only used whensolver='sgd' or 'adam'.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for weights and biasinitialization, train-test split if early stopping is used, and batchsampling when solver='sgd' or 'adam'.Pass an int for reproducible results across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for the optimization. When the loss or score is not improvingby at least ``tol`` for ``n_iter_no_change`` consecutive iterations,unless ``learning_rate`` is set to 'adaptive', convergence isconsidered to be reached and training stops.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Whether to print progress messages to stdout.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "warm_start": {
                "type": "boolean",
                "desc": "When set to True, reuse the solution of the previouscall to fit as initialization, otherwise, just erase theprevious solution. See :term:`the Glossary <warm_start>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "momentum": {
                "type": "number",
                "desc": "Momentum for gradient descent update. Should be between 0 and 1. Onlyused when solver='sgd'.",
                "default": "0.9",
                "dictKeyOf": "initkargs"
            },
            "nesterovs_momentum": {
                "type": "boolean",
                "desc": "Whether to use Nesterov's momentum. Only used when solver='sgd' andmomentum > 0.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "early_stopping": {
                "type": "boolean",
                "desc": "Whether to use early stopping to terminate training when validationscore is not improving. If set to true, it will automatically setaside 10% of training data as validation and terminate training whenvalidation score is not improving by at least ``tol`` for``n_iter_no_change`` consecutive epochs.Only effective when solver='sgd' or 'adam'",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "validation_fraction": {
                "type": "number",
                "desc": "The proportion of training data to set aside as validation set forearly stopping. Must be between 0 and 1.Only used if early_stopping is True",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "beta_1": {
                "type": "number",
                "desc": "Exponential decay rate for estimates of first moment vector in adam,should be in [0, 1). Only used when solver='adam'",
                "default": "0.9",
                "dictKeyOf": "initkargs"
            },
            "beta_2": {
                "type": "number",
                "desc": "Exponential decay rate for estimates of second moment vector in adam,should be in [0, 1). Only used when solver='adam'",
                "default": "0.999",
                "dictKeyOf": "initkargs"
            },
            "epsilon": {
                "type": "number",
                "desc": "Value for numerical stability in adam. Only used when solver='adam'",
                "default": "1e-8",
                "dictKeyOf": "initkargs"
            },
            "n_iter_no_change": {
                "type": "number",
                "desc": "Maximum number of epochs to not meet ``tol`` improvement.Only effective when solver='sgd' or 'adam'.. versionadded:: 0.20",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "max_fun": {
                "type": "number",
                "desc": "Only used when solver='lbfgs'. Maximum number of function calls.The solver iterates until convergence (determined by 'tol'), numberof iterations reaches max_iter, or this number of function calls.Note that number of function calls will be greater than or equal tothe number of iterations for the MLPRegressor... versionadded:: 0.22",
                "default": "15000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.neural_network.MLPRegressor"
        }
    },
    "sklearn.pipeline.FeatureUnion": {
        "cls": "Block",
        "typename": "FeatureUnion",
        "desc": "Concatenates results of multiple transformer objects.  This estimator applies a list of transformer objects in parallel to the input data, then concatenates the results. This is useful to combine several feature extraction mechanisms into a single transformer.  Parameters of the transformers may be set using its name and the parameter name separated by a '__'. A transformer may be replaced entirely by setting the parameter with its name to another transformer, or removed by setting to 'drop'.  Read more in the :ref:`User Guide <feature_union>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.pipeline",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "transformer_list": {
                "type": "list of (string, transformer) tuples",
                "desc": "List of transformer objects to be applied to the data. The firsthalf of each tuple is the name of the transformer. The tranformer canbe 'drop' for it to be ignored... versionchanged:: 0.22Deprecated `None` as a transformer in favor of 'drop'.",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "Number of jobs to run in parallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details... versionchanged:: v0.20`n_jobs` default changed from 1 to None",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "transformer_weights": {
                "type": "dict, default=None",
                "desc": "Multiplicative weights for features per transformer.Keys are transformer names, values the weights.Raises ValueError if key not present in ``transformer_list``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "If True, the time elapsed while fitting each transformer will beprinted as it is completed.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.pipeline.FeatureUnion"
        }
    },
    "sklearn.pipeline.Parallel": {
        "cls": "Block",
        "typename": "Parallel",
        "desc": "Helper class for readable parallel mapping.  Read more in the :ref:`User Guide <parallel>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.pipeline",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            }
        },
        "defaults": {
            "cls": "sklearn.pipeline.Parallel"
        }
    },
    "sklearn.pipeline.Pipeline": {
        "cls": "Block",
        "typename": "Pipeline",
        "desc": "Pipeline of transforms with a final estimator.  Sequentially apply a list of transforms and a final estimator. Intermediate steps of the pipeline must be 'transforms', that is, they must implement fit and transform methods. The final estimator only needs to implement fit. The transformers in the pipeline can be cached using ``memory`` argument.  The purpose of the pipeline is to assemble several steps that can be cross-validated together while setting different parameters. For this, it enables setting parameters of the various steps using their names and the parameter name separated by a '__', as in the example below. A step's estimator may be replaced entirely by setting the parameter with its name to another estimator, or a transformer removed by setting it to 'passthrough' or ``None``.  Read more in the :ref:`User Guide <pipeline>`.  .. versionadded:: 0.5",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.pipeline",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "steps": {
                "type": "list",
                "desc": "List of (name, transform) tuples (implementing fit/transform) that arechained, in the order in which they are chained, with the last objectan estimator.",
                "dictKeyOf": "initkargs"
            },
            "memory": {
                "type": "string",
                "desc": "Used to cache the fitted transformers of the pipeline. By default,no caching is performed. If a string is given, it is the path tothe caching directory. Enabling caching triggers a clone ofthe transformers before fitting. Therefore, the transformerinstance given to the pipeline cannot be inspecteddirectly. Use the attribute ``named_steps`` or ``steps`` toinspect estimators within the pipeline. Caching thetransformers is advantageous when fitting is time consuming.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "If True, the time elapsed while fitting each step will be printed as itis completed.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.pipeline.Pipeline"
        }
    },
    "sklearn.pipeline.deprecated": {
        "cls": "Block",
        "typename": "deprecated",
        "desc": "Decorator to mark a function or class as deprecated.  Issue a warning when the function is called/the class is instantiated and adds a warning to the docstring.  The optional extra argument will be appended to the deprecation message and the docstring. Note: to use this with the default value for extra, put in an empty of parentheses:  >>> from sklearn.utils import deprecated >>> deprecated() <sklearn.utils.deprecation.deprecated object at ...>  >>> @deprecated() ... def some_function(): pass",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.pipeline",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "extra": {
                "type": "string",
                "desc": "",
                "default": "''",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.pipeline.deprecated"
        }
    },
    "sklearn.preprocessing.Binarizer": {
        "cls": "Block",
        "typename": "Binarizer",
        "desc": "Binarize data (set feature values to 0 or 1) according to a threshold.  Values greater than the threshold map to 1, while values less than or equal to the threshold map to 0. With the default threshold of 0, only positive values map to 1.  Binarization is a common operation on text count data where the analyst can decide to only consider the presence or absence of a feature rather than a quantified number of occurrences for instance.  It can also be used as a pre-processing step for estimators that consider boolean random variables (e.g. modelled using the Bernoulli distribution in a Bayesian setting).  Read more in the :ref:`User Guide <preprocessing_binarization>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "threshold": {
                "type": "number",
                "desc": "Feature values below or equal to this are replaced by 0, above it by 1.Threshold may not be less than 0 for operations on sparse matrices.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "set to False to perform inplace binarization and avoid a copy (ifthe input is already a numpy array or a scipy.sparse CSR matrix).",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.Binarizer"
        }
    },
    "sklearn.preprocessing.FunctionTransformer": {
        "cls": "Block",
        "typename": "FunctionTransformer",
        "desc": "Constructs a transformer from an arbitrary callable.  A FunctionTransformer forwards its X (and optionally y) arguments to a user-defined function or function object and returns the result of this function. This is useful for stateless transformations such as taking the log of frequencies, doing custom scaling, etc.  Note: If a lambda is used as the function, then the resulting transformer will not be pickleable.  .. versionadded:: 0.17  Read more in the :ref:`User Guide <function_transformer>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "func": {
                "type": "callable, default=None",
                "desc": "The callable to use for the transformation. This will be passedthe same arguments as transform, with args and kwargs forwarded.If func is None, then func will be the identity function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "inverse_func": {
                "type": "callable, default=None",
                "desc": "The callable to use for the inverse transformation. This will bepassed the same arguments as inverse transform, with args andkwargs forwarded. If inverse_func is None, then inverse_funcwill be the identity function.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "validate": {
                "type": "boolean",
                "desc": "Indicate that the input X array should be checked before calling``func``. The possibilities are:- If False, there is no input validation.- If True, then X will be converted to a 2-dimensional NumPy array orsparse matrix. If the conversion is not possible an exception israised... versionchanged:: 0.22The default of ``validate`` changed from True to False.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "accept_sparse": {
                "type": "boolean",
                "desc": "Indicate that func accepts a sparse matrix as input. If validate isFalse, this has no effect. Otherwise, if accept_sparse is false,sparse matrix inputs will cause an exception to be raised.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "check_inverse": {
                "type": "boolean",
                "desc": "Whether to check that or ``func`` followed by ``inverse_func`` leads tothe original inputs. It can be used for a sanity check, raising awarning when the condition is not fulfilled... versionadded:: 0.20",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "kw_args": {
                "type": "dict, default=None",
                "desc": "Dictionary of additional keyword arguments to pass to func... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "inv_kw_args": {
                "type": "dict, default=None",
                "desc": "Dictionary of additional keyword arguments to pass to inverse_func... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.FunctionTransformer"
        }
    },
    "sklearn.preprocessing.KBinsDiscretizer": {
        "cls": "Block",
        "typename": "KBinsDiscretizer",
        "desc": "Bin continuous data into intervals.  Read more in the :ref:`User Guide <preprocessing_discretization>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_bins": {
                "type": "number",
                "desc": "The number of bins to produce. Raises ValueError if ``n_bins < 2``.",
                "default": "5",
                "dictKeyOf": "initkargs"
            },
            "encode": {
                "type": "option(onehot, onehot-dense, ordinal)",
                "desc": "Method used to encode the transformed result.onehotEncode the transformed result with one-hot encodingand return a sparse matrix. Ignored features are alwaysstacked to the right.onehot-denseEncode the transformed result with one-hot encodingand return a dense array. Ignored features are alwaysstacked to the right.ordinalReturn the bin identifier encoded as an integer value.",
                "default": "'onehot'",
                "dictKeyOf": "initkargs"
            },
            "strategy": {
                "type": "option(uniform, quantile, kmeans)",
                "desc": "Strategy used to define the widths of the bins.uniformAll bins in each feature have identical widths.quantileAll bins in each feature have the same number of points.kmeansValues in each bin have the same nearest center of a 1D k-meanscluster.",
                "default": "'quantile'",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "option(np.float32, np.float64)",
                "desc": "The desired data-type for the output. If None, output dtype isconsistent with input dtype. Only np.float32 and np.float64 aresupported... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.KBinsDiscretizer"
        }
    },
    "sklearn.preprocessing.LabelBinarizer": {
        "cls": "Block",
        "typename": "LabelBinarizer",
        "desc": "Binarize labels in a one-vs-all fashion.  Several regression and binary classification algorithms are available in scikit-learn. A simple way to extend these algorithms to the multi-class classification case is to use the so-called one-vs-all scheme.  At learning time, this simply consists in learning one regressor or binary classifier per class. In doing so, one needs to convert multi-class labels to binary labels (belong or does not belong to the class). LabelBinarizer makes this process easy with the transform method.  At prediction time, one assigns the class for which the corresponding model gave the greatest confidence. LabelBinarizer makes this easy with the inverse_transform method.  Read more in the :ref:`User Guide <preprocessing_targets>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "neg_label": {
                "type": "number",
                "desc": "Value with which negative labels must be encoded.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "pos_label": {
                "type": "number",
                "desc": "Value with which positive labels must be encoded.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "sparse_output": {
                "type": "boolean",
                "desc": "True if the returned array from transform is desired to be in sparseCSR format.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.LabelBinarizer"
        }
    },
    "sklearn.preprocessing.MaxAbsScaler": {
        "cls": "Block",
        "typename": "MaxAbsScaler",
        "desc": "Scale each feature by its maximum absolute value.  This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1.0. It does not shift/center the data, and thus does not destroy any sparsity.  This scaler can also be applied to sparse CSR or CSC matrices.  .. versionadded:: 0.17",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "copy": {
                "type": "boolean",
                "desc": "Set to False to perform inplace scaling and avoid a copy (if the inputis already a numpy array).",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.MaxAbsScaler"
        }
    },
    "sklearn.preprocessing.MinMaxScaler": {
        "cls": "Block",
        "typename": "MinMaxScaler",
        "desc": "Transform features by scaling each feature to a given range.  This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.  The transformation is given by::      X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))     X_scaled = X_std * (max - min) + min  where min, max = feature_range.  This transformation is often used as an alternative to zero mean, unit variance scaling.  Read more in the :ref:`User Guide <preprocessing_scaler>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "feature_range": {
                "type": "tuple (min, max), default=(0, 1)",
                "desc": "Desired range of transformed data.",
                "default": "(0",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Set to False to perform inplace row normalization and avoid acopy (if the input is already a numpy array).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "clip": {
                "type": "boolean",
                "desc": "Set to True to clip transformed values of held-out data toprovided `feature range`... versionadded:: 0.24",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.MinMaxScaler"
        }
    },
    "sklearn.preprocessing.MultiLabelBinarizer": {
        "cls": "Block",
        "typename": "MultiLabelBinarizer",
        "desc": "Transform between iterable of iterables and a multilabel format.  Although a list of sets or tuples is a very intuitive format for multilabel data, it is unwieldy to process. This transformer converts between this intuitive format and the supported multilabel format: a (samples x classes) binary matrix indicating the presence of a class label.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "classes": {
                "type": "array-like of shape (n_classes,), default=None",
                "desc": "Indicates an ordering for the class labels.All entries should be unique (cannot contain duplicate classes).",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "sparse_output": {
                "type": "boolean",
                "desc": "Set to True if output binary array is desired in CSR sparse format.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.MultiLabelBinarizer"
        }
    },
    "sklearn.preprocessing.Normalizer": {
        "cls": "Block",
        "typename": "Normalizer",
        "desc": "Normalize samples individually to unit norm.  Each sample (i.e. each row of the data matrix) with at least one non zero component is rescaled independently of other samples so that its norm (l1, l2 or inf) equals one.  This transformer is able to work both with dense numpy arrays and scipy.sparse matrix (use CSR format if you want to avoid the burden of a copy / conversion).  Scaling inputs to unit norms is a common operation for text classification or clustering for instance. For instance the dot product of two l2-normalized TF-IDF vectors is the cosine similarity of the vectors and is the base similarity metric for the Vector Space Model commonly used by the Information Retrieval community.  Read more in the :ref:`User Guide <preprocessing_normalization>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "norm": {
                "type": "option(l1, l2, max)",
                "desc": "The norm to use to normalize each non zero sample. If norm='max'is used, values will be rescaled by the maximum of the absolutevalues.",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "set to False to perform inplace row normalization and avoid acopy (if the input is already a numpy array or a scipy.sparseCSR matrix).",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.Normalizer"
        }
    },
    "sklearn.preprocessing.OneHotEncoder": {
        "cls": "Block",
        "typename": "OneHotEncoder",
        "desc": "Encode categorical features as a one-hot numeric array.  The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are encoded using a one-hot (aka 'one-of-K' or 'dummy') encoding scheme. This creates a binary column for each category and returns a sparse matrix or dense array (depending on the ``sparse`` parameter)  By default, the encoder derives the categories based on the unique values in each feature. Alternatively, you can also specify the `categories` manually.  This encoding is needed for feeding categorical data to many scikit-learn estimators, notably linear models and SVMs with the standard kernels.  Note: a one-hot encoding of y labels should use a LabelBinarizer instead.  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "categories": {
                "type": "'auto' or a list of array-like, default='auto'",
                "desc": "Categories (unique values) per feature:- 'auto' : Determine categories automatically from the training data.- list : ``categories[i]`` holds the categories expected in the ithcolumn. The passed categories should not mix strings and numericvalues within a single feature, and should be sorted in case ofnumeric values.The used categories can be found in the ``categories_`` attribute... versionadded:: 0.20",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "drop": {
                "type": "option(first, if_binary)",
                "desc": "Specifies a methodology to use to drop one of the categories perfeature. This is useful in situations where perfectly collinearfeatures cause problems, such as when feeding the resulting datainto a neural network or an unregularized regression.However, dropping one category breaks the symmetry of the originalrepresentation and can therefore induce a bias in downstream models,for instance for penalized linear classification or regression models.- None : retain all features (the default).- 'first' : drop the first category in each feature. If only onecategory is present, the feature will be dropped entirely.- 'if_binary' : drop the first category in each feature with twocategories. Features with 1 or more than 2 categories areleft intact.- array : ``drop[i]`` is the category in feature ``X[:, i]`` thatshould be dropped... versionadded:: 0.21The parameter `drop` was added in 0.21... versionchanged:: 0.23The option `drop='if_binary'` was added in 0.23.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "sparse": {
                "type": "boolean",
                "desc": "Will return sparse matrix if set True else will return an array.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "number type, default=float",
                "desc": "Desired dtype of output.",
                "default": "float",
                "dictKeyOf": "initkargs"
            },
            "handle_unknown": {
                "type": "option(error, ignore)",
                "desc": "Whether to raise an error or ignore if an unknown categorical featureis present during transform (default is to raise). When this parameteris set to 'ignore' and an unknown category is encountered duringtransform, the resulting one-hot encoded columns for this featurewill be all zeros. In the inverse transform, an unknown categorywill be denoted as None.",
                "default": "'error'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.OneHotEncoder"
        }
    },
    "sklearn.preprocessing.OrdinalEncoder": {
        "cls": "Block",
        "typename": "OrdinalEncoder",
        "desc": "Encode categorical features as an integer array.  The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature.  Read more in the :ref:`User Guide <preprocessing_categorical_features>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "categories": {
                "type": "'auto' or a list of array-like, default='auto'",
                "desc": "Categories (unique values) per feature:- 'auto' : Determine categories automatically from the training data.- list : ``categories[i]`` holds the categories expected in the ithcolumn. The passed categories should not mix strings and numericvalues, and should be sorted in case of numeric values.The used categories can be found in the ``categories_`` attribute.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "dtype": {
                "type": "number type, default np.float64",
                "desc": "Desired dtype of output.",
                "dictKeyOf": "initkargs"
            },
            "handle_unknown": {
                "type": "option(error, use_encoded_value)",
                "desc": "When set to 'error' an error will be raised in case an unknowncategorical feature is present during transform. When set to'use_encoded_value', the encoded value of unknown categories will beset to the value given for the parameter `unknown_value`. In:meth:`inverse_transform`, an unknown category will be denoted as None... versionadded:: 0.24",
                "default": "'error'",
                "dictKeyOf": "initkargs"
            },
            "unknown_value": {
                "type": "number",
                "desc": "When the parameter handle_unknown is set to 'use_encoded_value', thisparameter is required and will set the encoded value of unknowncategories. It has to be distinct from the values used to encode any ofthe categories in `fit`. If set to np.nan, the `dtype` parameter mustbe a float dtype... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.OrdinalEncoder"
        }
    },
    "sklearn.preprocessing.PolynomialFeatures": {
        "cls": "Block",
        "typename": "PolynomialFeatures",
        "desc": "Generate polynomial and interaction features.  Generate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "degree": {
                "type": "number",
                "desc": "The degree of the polynomial features.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "interaction_only": {
                "type": "boolean",
                "desc": "If true, only interaction features are produced: features that areproducts of at most ``degree`` *distinct* input features (so not``x[1] ** 2``, ``x[0] * x[2] ** 3``, etc.).",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "include_bias": {
                "type": "boolean",
                "desc": "If True (default), then include a bias column, the feature in whichall polynomial powers are zero (i.e. a column of ones - acts as anintercept term in a linear model).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "order": {
                "type": "option(C, F)",
                "desc": "Order of output array in the dense case. 'F' order is faster tocompute, but may slow down subsequent estimators... versionadded:: 0.21",
                "default": "'C'",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.PolynomialFeatures"
        }
    },
    "sklearn.preprocessing.PowerTransformer": {
        "cls": "Block",
        "typename": "PowerTransformer",
        "desc": "Apply a power transform featurewise to make data more Gaussian-like.  Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired.  Currently, PowerTransformer supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood.  Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data.  By default, zero-mean, unit-variance normalization is applied to the transformed data.  Read more in the :ref:`User Guide <preprocessing_transformer>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "method": {
                "type": "option(yeo-johnson, box-cox)",
                "desc": "The power transform method. Available methods are:- 'yeo-johnson' [1]_, works with positive and negative values- 'box-cox' [2]_, only works with strictly positive values",
                "default": "'yeo-johnson'",
                "dictKeyOf": "initkargs"
            },
            "standardize": {
                "type": "boolean",
                "desc": "Set to True to apply zero-mean, unit-variance normalization to thetransformed output.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Set to False to perform inplace computation during transformation.",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.PowerTransformer"
        }
    },
    "sklearn.preprocessing.QuantileTransformer": {
        "cls": "Block",
        "typename": "QuantileTransformer",
        "desc": "Transform features using quantiles information.  This method transforms the features to follow a uniform or a normal distribution. Therefore, for a given feature, this transformation tends to spread out the most frequent values. It also reduces the impact of (marginal) outliers: this is therefore a robust preprocessing scheme.  The transformation is applied on each feature independently. First an estimate of the cumulative distribution function of a feature is used to map the original values to a uniform distribution. The obtained values are then mapped to the desired output distribution using the associated quantile function. Features values of new/unseen data that fall below or above the fitted range will be mapped to the bounds of the output distribution. Note that this transform is non-linear. It may distort linear correlations between variables measured at the same scale but renders variables measured at different scales more directly comparable.  Read more in the :ref:`User Guide <preprocessing_transformer>`.  .. versionadded:: 0.19",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_quantiles": {
                "type": "number",
                "desc": "Number of quantiles to be computed. It corresponds to the numberof landmarks used to discretize the cumulative distribution function.If n_quantiles is larger than the number of samples, n_quantiles is setto the number of samples as a larger number of quantiles does not givea better approximation of the cumulative distribution functionestimator.",
                "default": "1000 or n_samples",
                "dictKeyOf": "initkargs"
            },
            "output_distribution": {
                "type": "option(uniform, normal)",
                "desc": "Marginal distribution for the transformed data. The choices are'uniform' (default) or 'normal'.",
                "default": "'uniform'",
                "dictKeyOf": "initkargs"
            },
            "ignore_implicit_zeros": {
                "type": "boolean",
                "desc": "Only applies to sparse matrices. If True, the sparse entries of thematrix are discarded to compute the quantile statistics. If False,these entries are treated as zeros.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "subsample": {
                "type": "number",
                "desc": "Maximum number of samples used to estimate the quantiles forcomputational efficiency. Note that the subsampling procedure maydiffer for value-identical sparse and dense matrices.",
                "default": "1e5",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for subsampling and smoothingnoise.Please see ``subsample`` for more details.Pass an int for reproducible results across multiple function calls.See :term:`Glossary <random_state>`",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Set to False to perform inplace transformation and avoid a copy (if theinput is already a numpy array).",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.QuantileTransformer"
        }
    },
    "sklearn.preprocessing.RobustScaler": {
        "cls": "Block",
        "typename": "RobustScaler",
        "desc": "Scale features using statistics that are robust to outliers.  This Scaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile (25th quantile) and the 3rd quartile (75th quantile).  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Median and interquartile range are then stored to be used on later data using the ``transform`` method.  Standardization of a dataset is a common requirement for many machine learning estimators. Typically this is done by removing the mean and scaling to unit variance. However, outliers can often influence the sample mean / variance in a negative way. In such cases, the median and the interquartile range often give better results.  .. versionadded:: 0.17  Read more in the :ref:`User Guide <preprocessing_scaler>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "with_centering": {
                "type": "boolean",
                "desc": "If True, center the data before scaling.This will cause ``transform`` to raise an exception when attempted onsparse matrices, because centering them entails building a densematrix which in common use cases is likely to be too large to fit inmemory.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "with_scaling": {
                "type": "boolean",
                "desc": "If True, scale the data to interquartile range.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "quantile_range": {
                "type": "tuple (q_min, q_max), 0.0 < q_min < q_max < 100.0,         default=(25.0, 75.0), == (1st quantile, 3rd quantile), == IQR",
                "desc": "Quantile range used to calculate ``scale_``... versionadded:: 0.18",
                "default": "(25.0",
                "dictKeyOf": "initkargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "If False, try to avoid a copy and do inplace scaling instead.This is not guaranteed to always work inplace; e.g. if the data isnot a NumPy array or scipy.sparse CSR matrix, a copy may still bereturned.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "unit_variance": {
                "type": "boolean",
                "desc": "If True, scale data so that normally distributed features have avariance of 1. In general, if the difference between the x-values of``q_max`` and ``q_min`` for a standard normal distribution is greaterthan 1, the dataset will be scaled down. If less than 1, the datasetwill be scaled up... versionadded:: 0.24",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.RobustScaler"
        }
    },
    "sklearn.preprocessing.StandardScaler": {
        "cls": "Block",
        "typename": "StandardScaler",
        "desc": "Standardize features by removing the mean and scaling to unit variance  The standard score of a sample `x` is calculated as:      z = (x - u) / s  where `u` is the mean of the training samples or zero if `with_mean=False`, and `s` is the standard deviation of the training samples or one if `with_std=False`.  Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using :meth:`transform`.  Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).  For instance many elements used in the objective function of a learning algorithm (such as the RBF kernel of Support Vector Machines or the L1 and L2 regularizers of linear models) assume that all features are centered around 0 and have variance in the same order. If a feature has a variance that is orders of magnitude larger that others, it might dominate the objective function and make the estimator unable to learn from other features correctly as expected.  This scaler can also be applied to sparse CSR or CSC matrices by passing `with_mean=False` to avoid breaking the sparsity structure of the data.  Read more in the :ref:`User Guide <preprocessing_scaler>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.preprocessing",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "copy": {
                "type": "boolean",
                "desc": "If False, try to avoid a copy and do inplace scaling instead.This is not guaranteed to always work inplace; e.g. if the data isnot a NumPy array or scipy.sparse CSR matrix, a copy may still bereturned.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "with_mean": {
                "type": "boolean",
                "desc": "If True, center the data before scaling.This does not work (and will raise an exception) when attempted onsparse matrices, because centering them entails building a densematrix which in common use cases is likely to be too large to fit inmemory.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "with_std": {
                "type": "boolean",
                "desc": "If True, scale the data to unit variance (or equivalently,unit standard deviation).",
                "default": "True",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.preprocessing.StandardScaler"
        }
    },
    "sklearn.random_projection.GaussianRandomProjection": {
        "cls": "Block",
        "typename": "GaussianRandomProjection",
        "desc": "Reduce dimensionality through Gaussian random projection.  The components of the random matrix are drawn from N(0, 1 / n_components).  Read more in the :ref:`User Guide <gaussian_random_matrix>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.random_projection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Dimensionality of the target projection space.n_components can be automatically adjusted according to thenumber of samples in the dataset and the bound given by theJohnson-Lindenstrauss lemma. In that case the quality of theembedding is controlled by the ``eps`` parameter.It should be noted that Johnson-Lindenstrauss lemma can yieldvery conservative estimated of the required number of componentsas it makes no assumption on the structure of the dataset.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "Parameter to control the quality of the embedding according tothe Johnson-Lindenstrauss lemma when `n_components` is set to'auto'. The value should be strictly positive.Smaller values lead to better embedding and higher number ofdimensions (n_components) in the target projection space.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the pseudo random number generator used to generate theprojection matrix at fit time.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.random_projection.GaussianRandomProjection"
        }
    },
    "sklearn.random_projection.SparseRandomProjection": {
        "cls": "Block",
        "typename": "SparseRandomProjection",
        "desc": "Reduce dimensionality through sparse random projection.  Sparse random matrix is an alternative to dense random projection matrix that guarantees similar embedding quality while being much more memory efficient and allowing faster computation of the projected data.  If we note `s = 1 / density` the components of the random matrix are drawn from:    - -sqrt(s) / sqrt(n_components)   with probability 1 / 2s   -  0                              with probability 1 - 1 / s   - +sqrt(s) / sqrt(n_components)   with probability 1 / 2s  Read more in the :ref:`User Guide <sparse_random_matrix>`.  .. versionadded:: 0.13",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.random_projection",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "n_components": {
                "type": "number",
                "desc": "Dimensionality of the target projection space.n_components can be automatically adjusted according to thenumber of samples in the dataset and the bound given by theJohnson-Lindenstrauss lemma. In that case the quality of theembedding is controlled by the ``eps`` parameter.It should be noted that Johnson-Lindenstrauss lemma can yieldvery conservative estimated of the required number of componentsas it makes no assumption on the structure of the dataset.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "density": {
                "type": "number",
                "desc": "Ratio in the range (0, 1] of non-zero component in the randomprojection matrix.If density = 'auto', the value is set to the minimum densityas recommended by Ping Li et al.: 1 / sqrt(n_features).Use density = 1 / 3.0 if you want to reproduce the results fromAchlioptas, 2001.",
                "default": "'auto'",
                "dictKeyOf": "initkargs"
            },
            "eps": {
                "type": "number",
                "desc": "Parameter to control the quality of the embedding according tothe Johnson-Lindenstrauss lemma when n_components is set to'auto'. This value should be strictly positive.Smaller values lead to better embedding and higher number ofdimensions (n_components) in the target projection space.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "dense_output": {
                "type": "boolean",
                "desc": "If True, ensure that the output of the random projection is adense numpy array even if the input and random projection matrixare both sparse. In practice, if the number of components issmall the number of zero components in the projected data willbe very small and it will be more CPU and memory efficient touse a dense representation.If False, the projected data uses a sparse representation ifthe input is sparse.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the pseudo random number generator used to generate theprojection matrix at fit time.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.random_projection.SparseRandomProjection"
        }
    },
    "sklearn.semi_supervised.LabelPropagation": {
        "cls": "Block",
        "typename": "LabelPropagation",
        "desc": "Label Propagation classifier  Read more in the :ref:`User Guide <label_propagation>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.semi_supervised",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "option(knn, rbf)",
                "desc": "String identifier for kernel function to use or the kernel functionitself. Only 'rbf' and 'knn' strings are valid inputs. The functionpassed should take two inputs, each of shape (n_samples, n_features),and return a (n_samples, n_samples) shaped weight matrix.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "number",
                "desc": "Parameter for rbf kernel.",
                "default": "20",
                "dictKeyOf": "initkargs"
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Parameter for knn kernel which need to be strictly positive.",
                "default": "7",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Change maximum number of iterations allowed.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Convergence tolerance: threshold to consider the system at steadystate.",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.semi_supervised.LabelPropagation"
        }
    },
    "sklearn.semi_supervised.LabelSpreading": {
        "cls": "Block",
        "typename": "LabelSpreading",
        "desc": "LabelSpreading model for semi-supervised learning  This model is similar to the basic Label Propagation algorithm, but uses affinity matrix based on the normalized graph Laplacian and soft clamping across the labels.  Read more in the :ref:`User Guide <label_propagation>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.semi_supervised",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "option(knn, rbf)",
                "desc": "String identifier for kernel function to use or the kernel functionitself. Only 'rbf' and 'knn' strings are valid inputs. The functionpassed should take two inputs, each of shape (n_samples, n_features),and return a (n_samples, n_samples) shaped weight matrix.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "number",
                "desc": "Parameter for rbf kernel.",
                "default": "20",
                "dictKeyOf": "initkargs"
            },
            "n_neighbors": {
                "type": "number",
                "desc": "Parameter for knn kernel which is a strictly positive integer.",
                "default": "7",
                "dictKeyOf": "initkargs"
            },
            "alpha": {
                "type": "number",
                "desc": "Clamping factor. A value in (0, 1) that specifies the relative amountthat an instance should adopt the information from its neighbors asopposed to its initial label.alpha=0 means keeping the initial label information; alpha=1 meansreplacing all initial information.",
                "default": "0.2",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations allowed.",
                "default": "30",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Convergence tolerance: threshold to consider the system at steadystate.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of parallel jobs to run.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.semi_supervised.LabelSpreading"
        }
    },
    "sklearn.semi_supervised.SelfTrainingClassifier": {
        "cls": "Parent",
        "typename": "SelfTrainingClassifier",
        "desc": "Self-training classifier.  This class allows a given supervised classifier to function as a semi-supervised classifier, allowing it to learn from unlabeled data. It does this by iteratively predicting pseudo-labels for the unlabeled data and adding them to the training set.  The classifier will continue iterating until either max_iter is reached, or no pseudo-labels were added to the training set in the previous iteration.  Read more in the :ref:`User Guide <self_training>`.",
        "childof": "skll.plugin.sklearn.block.SklWrappingClass",
        "pytype": "skll.plugin.sklearn.block.SklWrappingClass",
        "group": "sklearn.semi_supervised",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "estname": {
                "hidden": true
            },
            "multiple": {
                "hidden": true
            },
            "threshold": {
                "type": "number",
                "desc": "The decision threshold for use with `criterion='threshold'`.Should be in [0, 1). When using the 'threshold' criterion, a:ref:`well calibrated classifier <calibration>` should be used.",
                "default": "0.75",
                "dictKeyOf": "initkargs"
            },
            "criterion": {
                "type": "option(threshold, k_best)",
                "desc": "The selection criterion used to select which labels to add to thetraining set. If 'threshold', pseudo-labels with predictionprobabilities above `threshold` are added to the dataset. If 'k_best',the `k_best` pseudo-labels with highest prediction probabilities areadded to the dataset. When using the 'threshold' criterion, a:ref:`well calibrated classifier <calibration>` should be used.",
                "default": "'threshold'",
                "dictKeyOf": "initkargs"
            },
            "k_best": {
                "type": "number",
                "desc": "The amount of samples to add in each iteration. Only used when`criterion` is k_best'.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Maximum number of iterations allowed. Should be greater than or equalto 0. If it is ``None``, the classifier will continue to predict labelsuntil no new pseudo-labels are added, or all unlabeled samples havebeen labeled.",
                "default": "10",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Enable verbose output.",
                "default": "False",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.semi_supervised.SelfTrainingClassifier",
            "estname": "base_estimator",
            "multiple": false
        }
    },
    "sklearn.svm.LinearSVC": {
        "cls": "Block",
        "typename": "LinearSVC",
        "desc": "Linear Support Vector Classification.  Similar to SVC with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme.  Read more in the :ref:`User Guide <svm_classification>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.svm",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "penalty": {
                "type": "option(l1, l2)",
                "desc": "Specifies the norm used in the penalization. The 'l2'penalty is the standard used in SVC. The 'l1' leads to ``coef_``vectors that are sparse.",
                "default": "'l2'",
                "dictKeyOf": "initkargs"
            },
            "loss": {
                "type": "option(hinge, squared_hinge)",
                "desc": "Specifies the loss function. 'hinge' is the standard SVM loss(used e.g. by the SVC class) while 'squared_hinge' is thesquare of the hinge loss. The combination of ``penalty='l1'``and ``loss='hinge'`` is not supported.",
                "default": "'squared_hinge'",
                "dictKeyOf": "initkargs"
            },
            "dual": {
                "type": "boolean",
                "desc": "Select the algorithm to either solve the dual or primaloptimization problem. Prefer dual=False when n_samples > n_features.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criteria.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "C": {
                "type": "number",
                "desc": "Regularization parameter. The strength of the regularization isinversely proportional to C. Must be strictly positive.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "multi_class": {
                "type": "option(ovr, crammer_singer)",
                "desc": "Determines the multi-class strategy if `y` contains more thantwo classes.``\"ovr\"`` trains n_classes one-vs-rest classifiers, while``\"crammer_singer\"`` optimizes a joint objective over all classes.While `crammer_singer` is interesting from a theoretical perspectiveas it is consistent, it is seldom used in practice as it rarely leadsto better accuracy and is more expensive to compute.If ``\"crammer_singer\"`` is chosen, the options loss, penalty and dualwill be ignored.",
                "default": "'ovr'",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be already centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "intercept_scaling": {
                "type": "number",
                "desc": "When self.fit_intercept is True, instance vector x becomes``[x, self.intercept_scaling]``,i.e. a \"synthetic\" feature with constant value equals tointercept_scaling is appended to the instance vector.The intercept becomes intercept_scaling * synthetic feature weightNote! the synthetic feature weight is subject to l1/l2 regularizationas all other features.To lessen the effect of regularization on synthetic feature weight(and therefore on the intercept) intercept_scaling has to be increased.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict or 'balanced', default=None",
                "desc": "Set the parameter C of class i to ``class_weight[i]*C`` forSVC. If not given, all classes are supposed to haveweight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in liblinear that, if enabled, may not workproperly in a multithreaded context.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the pseudo random number generation for shuffling the data forthe dual coordinate descent (if ``dual=True``). When ``dual=False`` theunderlying implementation of :class:`LinearSVC` is not random and``random_state`` has no effect on the results.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations to be run.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.svm.LinearSVC"
        }
    },
    "sklearn.svm.LinearSVR": {
        "cls": "Block",
        "typename": "LinearSVR",
        "desc": "Linear Support Vector Regression.  Similar to SVR with parameter kernel='linear', but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.  This class supports both dense and sparse input.  Read more in the :ref:`User Guide <svm_regression>`.  .. versionadded:: 0.16",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.svm",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "epsilon": {
                "type": "number",
                "desc": "Epsilon parameter in the epsilon-insensitive loss function. Notethat the value of this parameter depends on the scale of the targetvariable y. If unsure, set ``epsilon=0``.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criteria.",
                "default": "1e-4",
                "dictKeyOf": "initkargs"
            },
            "C": {
                "type": "number",
                "desc": "Regularization parameter. The strength of the regularization isinversely proportional to C. Must be strictly positive.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "loss": {
                "type": "option(epsilon_insensitive, squared_epsilon_insensitive)",
                "desc": "Specifies the loss function. The epsilon-insensitive loss(standard SVR) is the L1 loss, while the squared epsilon-insensitiveloss ('squared_epsilon_insensitive') is the L2 loss.",
                "default": "'epsilon_insensitive'",
                "dictKeyOf": "initkargs"
            },
            "fit_intercept": {
                "type": "boolean",
                "desc": "Whether to calculate the intercept for this model. If setto false, no intercept will be used in calculations(i.e. data is expected to be already centered).",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "intercept_scaling": {
                "type": "number",
                "desc": "When self.fit_intercept is True, instance vector x becomes[x, self.intercept_scaling],i.e. a \"synthetic\" feature with constant value equals tointercept_scaling is appended to the instance vector.The intercept becomes intercept_scaling * synthetic feature weightNote! the synthetic feature weight is subject to l1/l2 regularizationas all other features.To lessen the effect of regularization on synthetic feature weight(and therefore on the intercept) intercept_scaling has to be increased.",
                "default": "1.",
                "dictKeyOf": "initkargs"
            },
            "dual": {
                "type": "boolean",
                "desc": "Select the algorithm to either solve the dual or primaloptimization problem. Prefer dual=False when n_samples > n_features.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "number",
                "desc": "Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in liblinear that, if enabled, may not workproperly in a multithreaded context.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the pseudo random number generation for shuffling the data.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "The maximum number of iterations to be run.",
                "default": "1000",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.svm.LinearSVR"
        }
    },
    "sklearn.svm.NuSVC": {
        "cls": "Block",
        "typename": "NuSVC",
        "desc": "Nu-Support Vector Classification.  Similar to SVC but uses a parameter to control the number of support vectors.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_classification>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.svm",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "nu": {
                "type": "number",
                "desc": "An upper bound on the fraction of margin errors (see :ref:`User Guide<nu_svc>`) and a lower bound of the fraction of support vectors.Should be in the interval (0, 1].",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "kernel": {
                "type": "option(linear, poly, rbf, sigmoid, precomputed)",
                "desc": "Specifies the kernel type to be used in the algorithm.It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' ora callable.If none is given, 'rbf' will be used. If a callable is given it isused to precompute the kernel matrix.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel function ('poly').Ignored by all other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "option(scale, auto)",
                "desc": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.- if ``gamma='scale'`` (default) is passed then it uses1 / (n_features * X.var()) as value of gamma,- if 'auto', uses 1 / n_features... versionchanged:: 0.22The default value of ``gamma`` changed from 'auto' to 'scale'.",
                "default": "'scale'",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Independent term in kernel function.It is only significant in 'poly' and 'sigmoid'.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "shrinking": {
                "type": "boolean",
                "desc": "Whether to use the shrinking heuristic.See the :ref:`User Guide <shrinking_svm>`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "probability": {
                "type": "boolean",
                "desc": "Whether to enable probability estimates. This must be enabled priorto calling `fit`, will slow down that method as it internally uses5-fold cross-validation, and `predict_proba` may be inconsistent with`predict`. Read more in the :ref:`User Guide <scores_probabilities>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criterion.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "cache_size": {
                "type": "number",
                "desc": "Specify the size of the kernel cache (in MB).",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "option(dict, balanced)",
                "desc": "Set the parameter C of class i to class_weight[i]*C forSVC. If not given, all classes are supposed to haveweight one. The \"balanced\" mode uses the values of y to automaticallyadjust weights inversely proportional to class frequencies as``n_samples / (n_classes * np.bincount(y))``",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in libsvm that, if enabled, may not workproperly in a multithreaded context.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Hard limit on iterations within solver, or -1 for no limit.",
                "default": "-1",
                "dictKeyOf": "initkargs"
            },
            "decision_function_shape": {
                "type": "option(ovo, ovr)",
                "desc": "Whether to return a one-vs-rest ('ovr') decision function of shape(n_samples, n_classes) as all other classifiers, or the originalone-vs-one ('ovo') decision function of libsvm which has shape(n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one('ovo') is always used as multi-class strategy. The parameter isignored for binary classification... versionchanged:: 0.19decision_function_shape is 'ovr' by default... versionadded:: 0.17*decision_function_shape='ovr'* is recommended... versionchanged:: 0.17Deprecated *decision_function_shape='ovo' and None*.",
                "default": "'ovr'",
                "dictKeyOf": "initkargs"
            },
            "break_ties": {
                "type": "boolean",
                "desc": "If true, ``decision_function_shape='ovr'``, and number of classes > 2,:term:`predict` will break ties according to the confidence values of:term:`decision_function`; otherwise the first class among the tiedclasses is returned. Please note that breaking ties comes at arelatively high computational cost compared to a simple predict... versionadded:: 0.22",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the pseudo random number generation for shuffling the data forprobability estimates. Ignored when `probability` is False.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.svm.NuSVC"
        }
    },
    "sklearn.svm.NuSVR": {
        "cls": "Block",
        "typename": "NuSVR",
        "desc": "Nu Support Vector Regression.  Similar to NuSVC, for regression, uses a parameter nu to control the number of support vectors. However, unlike NuSVC, where nu replaces C, here nu replaces the parameter epsilon of epsilon-SVR.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <svm_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.svm",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "nu": {
                "type": "number",
                "desc": "An upper bound on the fraction of training errors and a lower bound ofthe fraction of support vectors. Should be in the interval (0, 1]. Bydefault 0.5 will be taken.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "C": {
                "type": "number",
                "desc": "Penalty parameter C of the error term.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "kernel": {
                "type": "option(linear, poly, rbf, sigmoid, precomputed)",
                "desc": "Specifies the kernel type to be used in the algorithm.It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' ora callable.If none is given, 'rbf' will be used. If a callable is given it isused to precompute the kernel matrix.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel function ('poly').Ignored by all other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "option(scale, auto)",
                "desc": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.- if ``gamma='scale'`` (default) is passed then it uses1 / (n_features * X.var()) as value of gamma,- if 'auto', uses 1 / n_features... versionchanged:: 0.22The default value of ``gamma`` changed from 'auto' to 'scale'.",
                "default": "'scale'",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Independent term in kernel function.It is only significant in 'poly' and 'sigmoid'.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "shrinking": {
                "type": "boolean",
                "desc": "Whether to use the shrinking heuristic.See the :ref:`User Guide <shrinking_svm>`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criterion.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "cache_size": {
                "type": "number",
                "desc": "Specify the size of the kernel cache (in MB).",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in libsvm that, if enabled, may not workproperly in a multithreaded context.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Hard limit on iterations within solver, or -1 for no limit.",
                "default": "-1",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.svm.NuSVR"
        }
    },
    "sklearn.svm.OneClassSVM": {
        "cls": "Block",
        "typename": "OneClassSVM",
        "desc": "Unsupervised Outlier Detection.  Estimate the support of a high-dimensional distribution.  The implementation is based on libsvm.  Read more in the :ref:`User Guide <outlier_detection>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.svm",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "option(linear, poly, rbf, sigmoid, precomputed)",
                "desc": "Specifies the kernel type to be used in the algorithm.It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' ora callable.If none is given, 'rbf' will be used. If a callable is given it isused to precompute the kernel matrix.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel function ('poly').Ignored by all other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "option(scale, auto)",
                "desc": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.- if ``gamma='scale'`` (default) is passed then it uses1 / (n_features * X.var()) as value of gamma,- if 'auto', uses 1 / n_features... versionchanged:: 0.22The default value of ``gamma`` changed from 'auto' to 'scale'.",
                "default": "'scale'",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Independent term in kernel function.It is only significant in 'poly' and 'sigmoid'.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criterion.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "nu": {
                "type": "number",
                "desc": "An upper bound on the fraction of trainingerrors and a lower bound of the fraction of supportvectors. Should be in the interval (0, 1]. By default 0.5will be taken.",
                "default": "0.5",
                "dictKeyOf": "initkargs"
            },
            "shrinking": {
                "type": "boolean",
                "desc": "Whether to use the shrinking heuristic.See the :ref:`User Guide <shrinking_svm>`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "cache_size": {
                "type": "number",
                "desc": "Specify the size of the kernel cache (in MB).",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in libsvm that, if enabled, may not workproperly in a multithreaded context.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Hard limit on iterations within solver, or -1 for no limit.",
                "default": "-1",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.svm.OneClassSVM"
        }
    },
    "sklearn.svm.SVC": {
        "cls": "Block",
        "typename": "SVC",
        "desc": "C-Support Vector Classification.  The implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using :class:`~sklearn.svm.LinearSVC` or :class:`~sklearn.linear_model.SGDClassifier` instead, possibly after a :class:`~sklearn.kernel_approximation.Nystroem` transformer.  The multiclass support is handled according to a one-vs-one scheme.  For details on the precise mathematical formulation of the provided kernel functions and how `gamma`, `coef0` and `degree` affect each other, see the corresponding section in the narrative documentation: :ref:`svm_kernels`.  Read more in the :ref:`User Guide <svm_classification>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.svm",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "C": {
                "type": "number",
                "desc": "Regularization parameter. The strength of the regularization isinversely proportional to C. Must be strictly positive. The penaltyis a squared l2 penalty.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "kernel": {
                "type": "option(linear, poly, rbf, sigmoid, precomputed)",
                "desc": "Specifies the kernel type to be used in the algorithm.It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' ora callable.If none is given, 'rbf' will be used. If a callable is given it isused to pre-compute the kernel matrix from data matrices; that matrixshould be an array of shape ``(n_samples, n_samples)``.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel function ('poly').Ignored by all other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "option(scale, auto)",
                "desc": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.- if ``gamma='scale'`` (default) is passed then it uses1 / (n_features * X.var()) as value of gamma,- if 'auto', uses 1 / n_features... versionchanged:: 0.22The default value of ``gamma`` changed from 'auto' to 'scale'.",
                "default": "'scale'",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Independent term in kernel function.It is only significant in 'poly' and 'sigmoid'.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "shrinking": {
                "type": "boolean",
                "desc": "Whether to use the shrinking heuristic.See the :ref:`User Guide <shrinking_svm>`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "probability": {
                "type": "boolean",
                "desc": "Whether to enable probability estimates. This must be enabled priorto calling `fit`, will slow down that method as it internally uses5-fold cross-validation, and `predict_proba` may be inconsistent with`predict`. Read more in the :ref:`User Guide <scores_probabilities>`.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criterion.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "cache_size": {
                "type": "number",
                "desc": "Specify the size of the kernel cache (in MB).",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict or 'balanced', default=None",
                "desc": "Set the parameter C of class i to class_weight[i]*C forSVC. If not given, all classes are supposed to haveweight one.The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in libsvm that, if enabled, may not workproperly in a multithreaded context.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Hard limit on iterations within solver, or -1 for no limit.",
                "default": "-1",
                "dictKeyOf": "initkargs"
            },
            "decision_function_shape": {
                "type": "option(ovo, ovr)",
                "desc": "Whether to return a one-vs-rest ('ovr') decision function of shape(n_samples, n_classes) as all other classifiers, or the originalone-vs-one ('ovo') decision function of libsvm which has shape(n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one('ovo') is always used as multi-class strategy. The parameter isignored for binary classification... versionchanged:: 0.19decision_function_shape is 'ovr' by default... versionadded:: 0.17*decision_function_shape='ovr'* is recommended... versionchanged:: 0.17Deprecated *decision_function_shape='ovo' and None*.",
                "default": "'ovr'",
                "dictKeyOf": "initkargs"
            },
            "break_ties": {
                "type": "boolean",
                "desc": "If true, ``decision_function_shape='ovr'``, and number of classes > 2,:term:`predict` will break ties according to the confidence values of:term:`decision_function`; otherwise the first class among the tiedclasses is returned. Please note that breaking ties comes at arelatively high computational cost compared to a simple predict... versionadded:: 0.22",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the pseudo random number generation for shuffling the data forprobability estimates. Ignored when `probability` is False.Pass an int for reproducible output across multiple function calls.See :term:`Glossary <random_state>`.",
                "default": "None",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.svm.SVC"
        }
    },
    "sklearn.svm.SVR": {
        "cls": "Block",
        "typename": "SVR",
        "desc": "Epsilon-Support Vector Regression.  The free parameters in the model are C and epsilon.  The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to datasets with more than a couple of 10000 samples. For large datasets consider using :class:`~sklearn.svm.LinearSVR` or :class:`~sklearn.linear_model.SGDRegressor` instead, possibly after a :class:`~sklearn.kernel_approximation.Nystroem` transformer.  Read more in the :ref:`User Guide <svm_regression>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.svm",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "kernel": {
                "type": "option(linear, poly, rbf, sigmoid, precomputed)",
                "desc": "Specifies the kernel type to be used in the algorithm.It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed' ora callable.If none is given, 'rbf' will be used. If a callable is given it isused to precompute the kernel matrix.",
                "default": "'rbf'",
                "dictKeyOf": "initkargs"
            },
            "degree": {
                "type": "number",
                "desc": "Degree of the polynomial kernel function ('poly').Ignored by all other kernels.",
                "default": "3",
                "dictKeyOf": "initkargs"
            },
            "gamma": {
                "type": "option(scale, auto)",
                "desc": "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'.- if ``gamma='scale'`` (default) is passed then it uses1 / (n_features * X.var()) as value of gamma,- if 'auto', uses 1 / n_features... versionchanged:: 0.22The default value of ``gamma`` changed from 'auto' to 'scale'.",
                "default": "'scale'",
                "dictKeyOf": "initkargs"
            },
            "coef0": {
                "type": "number",
                "desc": "Independent term in kernel function.It is only significant in 'poly' and 'sigmoid'.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "tol": {
                "type": "number",
                "desc": "Tolerance for stopping criterion.",
                "default": "1e-3",
                "dictKeyOf": "initkargs"
            },
            "C": {
                "type": "number",
                "desc": "Regularization parameter. The strength of the regularization isinversely proportional to C. Must be strictly positive.The penalty is a squared l2 penalty.",
                "default": "1.0",
                "dictKeyOf": "initkargs"
            },
            "epsilon": {
                "type": "number",
                "desc": "Epsilon in the epsilon-SVR model. It specifies the epsilon-tubewithin which no penalty is associated in the training loss functionwith points predicted within a distance epsilon from the actualvalue.",
                "default": "0.1",
                "dictKeyOf": "initkargs"
            },
            "shrinking": {
                "type": "boolean",
                "desc": "Whether to use the shrinking heuristic.See the :ref:`User Guide <shrinking_svm>`.",
                "default": "True",
                "dictKeyOf": "initkargs"
            },
            "cache_size": {
                "type": "number",
                "desc": "Specify the size of the kernel cache (in MB).",
                "default": "200",
                "dictKeyOf": "initkargs"
            },
            "verbose": {
                "type": "boolean",
                "desc": "Enable verbose output. Note that this setting takes advantage of aper-process runtime setting in libsvm that, if enabled, may not workproperly in a multithreaded context.",
                "default": "False",
                "dictKeyOf": "initkargs"
            },
            "max_iter": {
                "type": "number",
                "desc": "Hard limit on iterations within solver, or -1 for no limit.",
                "default": "-1",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.svm.SVR"
        }
    },
    "sklearn.tree.DecisionTreeClassifier": {
        "cls": "Block",
        "typename": "DecisionTreeClassifier",
        "desc": "A decision tree classifier.  Read more in the :ref:`User Guide <tree>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.tree",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "criterion": {
                "type": "option(\"gini\", \"entropy\")",
                "desc": "The function to measure the quality of a split. Supported criteria are\"gini\" for the Gini impurity and \"entropy\" for the information gain.",
                "default": "\"gini\"",
                "dictKeyOf": "initkargs"
            },
            "splitter": {
                "type": "option(\"best\", \"random\")",
                "desc": "The strategy used to choose the split at each node. Supportedstrategies are \"best\" to choose the best split and \"random\" to choosethe best random split.",
                "default": "\"best\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`int(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=sqrt(n_features)`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness of the estimator. The features are alwaysrandomly permuted at each split, even if ``splitter`` is set to``\"best\"``. When ``max_features < n_features``, the algorithm willselect ``max_features`` at random at each split before finding the bestsplit among them. But the best found split may vary across differentruns, even if ``max_features=n_features``. That is the case, if theimprovement of the criterion is identical for several splits and onesplit has to be selected at random. To obtain a deterministic behaviourduring fitting, ``random_state`` has to be fixed to an integer.See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow a tree with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict, list of dict or \"balanced\", default=None",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If None, all classes are supposed to have weight one. Formulti-output problems, a list of dicts can be provided in the sameorder as the columns of y.Note that for multioutput (including multilabel) weights should bedefined for each class of every column in its own dict. For example,for four-class multilabel classification weights should be[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of[{1:1}, {2:5}, {3:1}, {4:1}].The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``For multi-output, the weights of each column of y will be multiplied.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.tree.DecisionTreeClassifier"
        }
    },
    "sklearn.tree.DecisionTreeRegressor": {
        "cls": "Block",
        "typename": "DecisionTreeRegressor",
        "desc": "A decision tree regressor.  Read more in the :ref:`User Guide <tree>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.tree",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "criterion": {
                "type": "option(\"mse\", \"friedman_mse\", \"mae\", \"poisson\")",
                "desc": "The function to measure the quality of a split. Supported criteriaare \"mse\" for the mean squared error, which is equal to variancereduction as feature selection criterion and minimizes the L2 lossusing the mean of each terminal node, \"friedman_mse\", which uses meansquared error with Friedman's improvement score for potential splits,\"mae\" for the mean absolute error, which minimizes the L1 loss usingthe median of each terminal node, and \"poisson\" which uses reduction inPoisson deviance to find splits... versionadded:: 0.18Mean Absolute Error (MAE) criterion... versionadded:: 0.24Poisson deviance criterion.",
                "default": "\"mse\"",
                "dictKeyOf": "initkargs"
            },
            "splitter": {
                "type": "option(\"best\", \"random\")",
                "desc": "The strategy used to choose the split at each node. Supportedstrategies are \"best\" to choose the best split and \"random\" to choosethe best random split.",
                "default": "\"best\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`int(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=n_features`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Controls the randomness of the estimator. The features are alwaysrandomly permuted at each split, even if ``splitter`` is set to``\"best\"``. When ``max_features < n_features``, the algorithm willselect ``max_features`` at random at each split before finding the bestsplit among them. But the best found split may vary across differentruns, even if ``max_features=n_features``. That is the case, if theimprovement of the criterion is identical for several splits and onesplit has to be selected at random. To obtain a deterministic behaviourduring fitting, ``random_state`` has to be fixed to an integer.See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow a tree with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "0",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.tree.DecisionTreeRegressor"
        }
    },
    "sklearn.tree.ExtraTreeClassifier": {
        "cls": "Block",
        "typename": "ExtraTreeClassifier",
        "desc": "An extremely randomized tree classifier.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split among those is chosen. When `max_features` is set 1, this amounts to building a totally random decision tree.  Warning: Extra-trees should only be used within ensemble methods.  Read more in the :ref:`User Guide <tree>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.tree",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "criterion": {
                "type": "option(\"gini\", \"entropy\")",
                "desc": "The function to measure the quality of a split. Supported criteria are\"gini\" for the Gini impurity and \"entropy\" for the information gain.",
                "default": "\"gini\"",
                "dictKeyOf": "initkargs"
            },
            "splitter": {
                "type": "option(\"random\", \"best\")",
                "desc": "The strategy used to choose the split at each node. Supportedstrategies are \"best\" to choose the best split and \"random\" to choosethe best random split.",
                "default": "\"random\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`int(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=sqrt(n_features)`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used to pick randomly the `max_features` used at each split.See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow a tree with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "class_weight": {
                "type": "dict, list of dict or \"balanced\", default=None",
                "desc": "Weights associated with classes in the form ``{class_label: weight}``.If None, all classes are supposed to have weight one. Formulti-output problems, a list of dicts can be provided in the sameorder as the columns of y.Note that for multioutput (including multilabel) weights should bedefined for each class of every column in its own dict. For example,for four-class multilabel classification weights should be[{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of[{1:1}, {2:5}, {3:1}, {4:1}].The \"balanced\" mode uses the values of y to automatically adjustweights inversely proportional to class frequencies in the input dataas ``n_samples / (n_classes * np.bincount(y))``For multi-output, the weights of each column of y will be multiplied.Note that these weights will be multiplied with sample_weight (passedthrough the fit method) if sample_weight is specified.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.tree.ExtraTreeClassifier"
        }
    },
    "sklearn.tree.ExtraTreeRegressor": {
        "cls": "Block",
        "typename": "ExtraTreeRegressor",
        "desc": "An extremely randomized tree regressor.  Extra-trees differ from classic decision trees in the way they are built. When looking for the best split to separate the samples of a node into two groups, random splits are drawn for each of the `max_features` randomly selected features and the best split among those is chosen. When `max_features` is set 1, this amounts to building a totally random decision tree.  Warning: Extra-trees should only be used within ensemble methods.  Read more in the :ref:`User Guide <tree>`.",
        "childof": "skll.plugin.sklearn.block.SklClass",
        "pytype": "skll.plugin.sklearn.block.SklClass",
        "group": "sklearn.tree",
        "properties": {
            "initkargs": {
                "hidden": true
            },
            "initargs": {
                "hidden": true
            },
            "cls": {
                "hidden": true
            },
            "trainmethod": {
                "hidden": true
            },
            "testmethod": {
                "hidden": true
            },
            "scoremethod": {
                "hidden": true
            },
            "criterion": {
                "type": "option(\"mse\", \"friedman_mse\", \"mae\")",
                "desc": "The function to measure the quality of a split. Supported criteriaare \"mse\" for the mean squared error, which is equal to variancereduction as feature selection criterion and \"mae\" for the meanabsolute error... versionadded:: 0.18Mean Absolute Error (MAE) criterion... versionadded:: 0.24Poisson deviance criterion.",
                "default": "\"mse\"",
                "dictKeyOf": "initkargs"
            },
            "splitter": {
                "type": "option(\"random\", \"best\")",
                "desc": "The strategy used to choose the split at each node. Supportedstrategies are \"best\" to choose the best split and \"random\" to choosethe best random split.",
                "default": "\"random\"",
                "dictKeyOf": "initkargs"
            },
            "max_depth": {
                "type": "number",
                "desc": "The maximum depth of the tree. If None, then nodes are expanded untilall leaves are pure or until all leaves contain less thanmin_samples_split samples.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_samples_split": {
                "type": "number",
                "desc": "The minimum number of samples required to split an internal node:- If int, then consider `min_samples_split` as the minimum number.- If float, then `min_samples_split` is a fraction and`ceil(min_samples_split * n_samples)` are the minimumnumber of samples for each split... versionchanged:: 0.18Added float values for fractions.",
                "default": "2",
                "dictKeyOf": "initkargs"
            },
            "min_samples_leaf": {
                "type": "number",
                "desc": "The minimum number of samples required to be at a leaf node.A split point at any depth will only be considered if it leaves atleast ``min_samples_leaf`` training samples in each of the left andright branches. This may have the effect of smoothing the model,especially in regression.- If int, then consider `min_samples_leaf` as the minimum number.- If float, then `min_samples_leaf` is a fraction and`ceil(min_samples_leaf * n_samples)` are the minimumnumber of samples for each node... versionchanged:: 0.18Added float values for fractions.",
                "default": "1",
                "dictKeyOf": "initkargs"
            },
            "min_weight_fraction_leaf": {
                "type": "number",
                "desc": "The minimum weighted fraction of the sum total of weights (of allthe input samples) required to be at a leaf node. Samples haveequal weight when sample_weight is not provided.",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "max_features": {
                "type": "number",
                "desc": "The number of features to consider when looking for the best split:- If int, then consider `max_features` features at each split.- If float, then `max_features` is a fraction and`int(max_features * n_features)` features are considered at eachsplit.- If \"auto\", then `max_features=n_features`.- If \"sqrt\", then `max_features=sqrt(n_features)`.- If \"log2\", then `max_features=log2(n_features)`.- If None, then `max_features=n_features`.Note: the search for a split does not stop until at least onevalid partition of the node samples is found, even if it requires toeffectively inspect more than ``max_features`` features.",
                "default": "\"auto\"",
                "dictKeyOf": "initkargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Used to pick randomly the `max_features` used at each split.See :term:`Glossary <random_state>` for details.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_decrease": {
                "type": "number",
                "desc": "A node will be split if this split induces a decrease of the impuritygreater than or equal to this value.The weighted impurity decrease equation is the following::N_t / N * (impurity - N_t_R / N_t * right_impurity- N_t_L / N_t * left_impurity)where ``N`` is the total number of samples, ``N_t`` is the number ofsamples at the current node, ``N_t_L`` is the number of samples in theleft child, and ``N_t_R`` is the number of samples in the right child.``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,if ``sample_weight`` is passed... versionadded:: 0.19",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            },
            "min_impurity_split": {
                "type": "number",
                "desc": "Threshold for early stopping in tree growth. A node will splitif its impurity is above the threshold, otherwise it is a leaf... deprecated:: 0.19``min_impurity_split`` has been deprecated in favor of``min_impurity_decrease`` in 0.19. The default value of``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and itwill be removed in 1.0 (renaming of 0.25).Use ``min_impurity_decrease`` instead.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "max_leaf_nodes": {
                "type": "number",
                "desc": "Grow a tree with ``max_leaf_nodes`` in best-first fashion.Best nodes are defined as relative reduction in impurity.If None then unlimited number of leaf nodes.",
                "default": "None",
                "dictKeyOf": "initkargs"
            },
            "ccp_alpha": {
                "type": "non-negative float, default=0.0",
                "desc": "Complexity parameter used for Minimal Cost-Complexity Pruning. Thesubtree with the largest cost complexity that is smaller than``ccp_alpha`` will be chosen. By default, no pruning is performed. See:ref:`minimal_cost_complexity_pruning` for details... versionadded:: 0.22",
                "default": "0.0",
                "dictKeyOf": "initkargs"
            }
        },
        "defaults": {
            "cls": "sklearn.tree.ExtraTreeRegressor"
        }
    },
    "sklearn.metrics.accuracy_score": {
        "cls": "Block",
        "typename": "accuracy_score",
        "desc": "Accuracy classification score.  In multilabel classification, this function computes subset accuracy: the set of labels predicted for a sample must *exactly* match the corresponding set of labels in y_true.  Read more in the :ref:`User Guide <accuracy_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "normalize": {
                "type": "boolean",
                "desc": "If ``False``, return the number of correctly classified samples.Otherwise, return the fraction of correctly classified samples.",
                "default": "True",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.accuracy_score",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.adjusted_mutual_info_score": {
        "cls": "Block",
        "typename": "adjusted_mutual_info_score",
        "desc": "Adjusted Mutual Information between two clusterings.  Adjusted Mutual Information (AMI) is an adjustment of the Mutual Information (MI) score to account for chance. It accounts for the fact that the MI is generally higher for two clusterings with a larger number of clusters, regardless of whether there is actually more information shared. For two clusterings :math:`U` and :math:`V`, the AMI is given as::      AMI(U, V) = [MI(U, V) - E(MI(U, V))] / [avg(H(U), H(V)) - E(MI(U, V))]  This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way.  This metric is furthermore symmetric: switching ``label_true`` with ``label_pred`` will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.  Be mindful that this function is an order of magnitude slower than other metrics, such as the Adjusted Rand Index.  Read more in the :ref:`User Guide <mutual_info_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "average_method": {
                "type": "string",
                "desc": "How to compute the normalizer in the denominator. Possible optionsare 'min', 'geometric', 'arithmetic', and 'max'... versionadded:: 0.20.. versionchanged:: 0.22The default value of ``average_method`` changed from 'max' to'arithmetic'.",
                "default": "'arithmetic'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.adjusted_mutual_info_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.adjusted_rand_score": {
        "cls": "Block",
        "typename": "adjusted_rand_score",
        "desc": "Rand index adjusted for chance.  The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.  The raw RI score is then \"adjusted for chance\" into the ARI score using the following scheme::      ARI = (RI - Expected_RI) / (max(RI) - Expected_RI)  The adjusted Rand index is thus ensured to have a value close to 0.0 for random labeling independently of the number of clusters and samples and exactly 1.0 when the clusterings are identical (up to a permutation).  ARI is a symmetric measure::      adjusted_rand_score(a, b) == adjusted_rand_score(b, a)  Read more in the :ref:`User Guide <adjusted_rand_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.adjusted_rand_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.auc": {
        "cls": "Block",
        "typename": "auc",
        "desc": "Compute Area Under the Curve (AUC) using the trapezoidal rule.  This is a general function, given points on a curve.  For computing the area under the ROC-curve, see :func:`roc_auc_score`.  For an alternative way to summarize a precision-recall curve, see :func:`average_precision_score`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "x": {
                "type": "ndarray of shape (n,)",
                "desc": "x coordinates. These must be either monotonic increasing or monotonicdecreasing.",
                "dictKeyOf": "kargs"
            },
            "y": {
                "type": "ndarray of shape, (n,)",
                "desc": "y coordinates.",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.auc"
        }
    },
    "sklearn.metrics.average_precision_score": {
        "cls": "Block",
        "typename": "average_precision_score",
        "desc": "Compute average precision (AP) from prediction scores.  AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:  .. math::     \\text{AP} = \\sum_n (R_n - R_{n-1}) P_n  where :math:`P_n` and :math:`R_n` are the precision and recall at the nth threshold [1]_. This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic.  Note: this implementation is restricted to the binary classification task or multilabel classification task.  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "average": {
                "type": "option(micro, samples, weighted, macro)",
                "desc": "If ``None``, the scores for each class are returned. Otherwise,this determines the type of averaging performed on the data:``'micro'``:Calculate metrics globally by considering each element of the labelindicator matrix as a label.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average, weightedby support (the number of true instances for each label).``'samples'``:Calculate metrics for each instance, and find their average.Will be ignored when ``y_true`` is binary.",
                "default": "'macro'",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "number",
                "desc": "The label of the positive class. Only applied to binary ``y_true``.For multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.average_precision_score",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.balanced_accuracy_score": {
        "cls": "Block",
        "typename": "balanced_accuracy_score",
        "desc": "Compute the balanced accuracy.  The balanced accuracy in binary and multiclass classification problems to deal with imbalanced datasets. It is defined as the average of recall obtained on each class.  The best value is 1 and the worst value is 0 when ``adjusted=False``.  Read more in the :ref:`User Guide <balanced_accuracy_score>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "adjusted": {
                "type": "boolean",
                "desc": "When true, the result is adjusted for chance, so that randomperformance would score 0, while keeping perfect performance at a scoreof 1.",
                "default": "False",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.balanced_accuracy_score",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.brier_score_loss": {
        "cls": "Block",
        "typename": "brier_score_loss",
        "desc": "Compute the Brier score loss.  The smaller the Brier score loss, the better, hence the naming with \"loss\". The Brier score measures the mean squared difference between the predicted probability and the actual outcome. The Brier score always takes on a value between zero and one, since this is the largest possible difference between a predicted probability (which must be between zero and one) and the actual outcome (which can take on values of only 0 and 1). It can be decomposed is the sum of refinement loss and calibration loss.  The Brier score is appropriate for binary and categorical outcomes that can be structured as true or false, but is inappropriate for ordinal variables which can take on three or more values (this is because the Brier score assumes that all possible outcomes are equivalently \"distant\" from one another). Which label is considered to be the positive label is controlled via the parameter `pos_label`, which defaults to the greater label unless `y_true` is all 0 or all -1, in which case `pos_label` defaults to 1.  Read more in the :ref:`User Guide <brier_score_loss>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "y_prob": {
                "type": "array of shape (n_samples,)",
                "desc": "Probabilities of the positive class.",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "number",
                "desc": "Label of the positive class. `pos_label` will be infered in thefollowing manner:* if `y_true` in {-1, 1} or {0, 1}, `pos_label` defaults to 1;* else if `y_true` contains string, an error will be raised and`pos_label` should be explicitely specified;* otherwise, `pos_label` defaults to the greater label,i.e. `np.unique(y_true)[-1]`.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.brier_score_loss",
            "yname": "y_true"
        }
    },
    "sklearn.metrics.calinski_harabasz_score": {
        "cls": "Block",
        "typename": "calinski_harabasz_score",
        "desc": "Compute the Calinski and Harabasz score.  It is also known as the Variance Ratio Criterion.  The score is defined as ratio between the within-cluster dispersion and the between-cluster dispersion.  Read more in the :ref:`User Guide <calinski_harabasz_index>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.calinski_harabasz_score",
            "xname": "X",
            "yname": "labels"
        }
    },
    "sklearn.metrics.check_scoring": {
        "cls": "Block",
        "typename": "check_scoring",
        "desc": "Determine scorer from user options.  A TypeError will be thrown if the estimator cannot be scored.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "estimator": {
                "type": "estimator object implementing 'fit'",
                "desc": "The object to use to fit the data.",
                "dictKeyOf": "kargs"
            },
            "scoring": {
                "type": "string",
                "desc": "A string (see model evaluation documentation) ora scorer callable object / function with signature``scorer(estimator, X, y)``.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "allow_none": {
                "type": "boolean",
                "desc": "If no scoring is specified and the estimator has no score function, wecan either return None or raise an exception.",
                "default": "False",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.check_scoring"
        }
    },
    "sklearn.metrics.classification_report": {
        "cls": "Block",
        "typename": "classification_report",
        "desc": "Build a text report showing the main classification metrics.  Read more in the :ref:`User Guide <classification_report>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "target_names": {
                "type": "list of str of shape (n_labels,), default=None",
                "desc": "Optional display names matching the labels (same order).",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "digits": {
                "type": "number",
                "desc": "Number of digits for formatting output floating point values.When ``output_dict`` is ``True``, this will be ignored and thereturned values will not be rounded.",
                "default": "2",
                "dictKeyOf": "kargs"
            },
            "output_dict": {
                "type": "boolean",
                "desc": "If True, return output as dict... versionadded:: 0.20",
                "default": "False",
                "dictKeyOf": "kargs"
            },
            "zero_division": {
                "type": "\"warn\", 0 or 1, default=\"warn\"",
                "desc": "Sets the value to return when there is a zero division. If set to\"warn\", this acts as 0, but warnings are also raised.",
                "default": "\"warn\"",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.classification_report",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.cohen_kappa_score": {
        "cls": "Block",
        "typename": "cohen_kappa_score",
        "desc": "Cohen's kappa: a statistic that measures inter-annotator agreement.  This function computes Cohen's kappa [1]_, a score that expresses the level of agreement between two annotators on a classification problem. It is defined as  .. math::     \\kappa = (p_o - p_e) / (1 - p_e)  where :math:`p_o` is the empirical probability of agreement on the label assigned to any sample (the observed agreement ratio), and :math:`p_e` is the expected agreement when both annotators assign labels randomly. :math:`p_e` is estimated using a per-annotator empirical prior over the class labels [2]_.  Read more in the :ref:`User Guide <cohen_kappa>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "y1": {
                "type": "array of shape (n_samples,)",
                "desc": "Labels assigned by the first annotator.",
                "dictKeyOf": "kargs"
            },
            "y2": {
                "type": "array of shape (n_samples,)",
                "desc": "Labels assigned by the second annotator. The kappa statistic issymmetric, so swapping ``y1`` and ``y2`` doesn't change the value.",
                "dictKeyOf": "kargs"
            },
            "weights": {
                "type": "option(linear, quadratic)",
                "desc": "Weighting type to calculate the score. None means no weighted;\"linear\" means linear weighted; \"quadratic\" means quadratic weighted.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.cohen_kappa_score",
            "yname": "labels"
        }
    },
    "sklearn.metrics.completeness_score": {
        "cls": "Block",
        "typename": "completeness_score",
        "desc": "Completeness metric of a cluster labeling given a ground truth.  A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.  This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way.  This metric is not symmetric: switching ``label_true`` with ``label_pred`` will return the :func:`homogeneity_score` which will be different in general.  Read more in the :ref:`User Guide <homogeneity_completeness>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.completeness_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.confusion_matrix": {
        "cls": "Block",
        "typename": "confusion_matrix",
        "desc": "Compute confusion matrix to evaluate the accuracy of a classification.  By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}` is equal to the number of observations known to be in group :math:`i` and predicted to be in group :math:`j`.  Thus in binary classification, the count of true negatives is :math:`C_{0,0}`, false negatives is :math:`C_{1,0}`, true positives is :math:`C_{1,1}` and false positives is :math:`C_{0,1}`.  Read more in the :ref:`User Guide <confusion_matrix>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "normalize": {
                "type": "option(true, pred, all)",
                "desc": "Normalizes confusion matrix over the true (rows), predicted (columns)conditions or all the population. If None, confusion matrix will not benormalized.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.confusion_matrix",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.consensus_score": {
        "cls": "Block",
        "typename": "consensus_score",
        "desc": "The similarity of two sets of biclusters.  Similarity between individual biclusters is computed. Then the best matching between sets is found using the Hungarian algorithm. The final score is the sum of similarities divided by the size of the larger set.  Read more in the :ref:`User Guide <biclustering>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "a": {
                "type": "(rows, columns)",
                "desc": "Tuple of row and column indicators for a set of biclusters.",
                "dictKeyOf": "kargs"
            },
            "b": {
                "type": "(rows, columns)",
                "desc": "Another set of biclusters like ``a``.",
                "dictKeyOf": "kargs"
            },
            "similarity": {
                "type": "'jaccard' or callable, default='jaccard'",
                "desc": "May be the string \"jaccard\" to use the Jaccard coefficient, orany function that takes four arguments, each of which is a 1dindicator vector: (a_rows, a_columns, b_rows, b_columns).",
                "default": "'jaccard'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.consensus_score"
        }
    },
    "sklearn.metrics.coverage_error": {
        "cls": "Block",
        "typename": "coverage_error",
        "desc": "Coverage error measure.  Compute how far we need to go through the ranked scores to cover all true labels. The best value is equal to the average number of labels in ``y_true`` per sample.  Ties in ``y_scores`` are broken by giving maximal rank that would have been assigned to all tied values.  Note: Our implementation's score is 1 greater than the one given in Tsoumakas et al., 2010. This extends it to handle the degenerate case in which an instance has 0 true labels.  Read more in the :ref:`User Guide <coverage_error>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.coverage_error",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.davies_bouldin_score": {
        "cls": "Block",
        "typename": "davies_bouldin_score",
        "desc": "Computes the Davies-Bouldin score.  The score is defined as the average similarity measure of each cluster with its most similar cluster, where similarity is the ratio of within-cluster distances to between-cluster distances. Thus, clusters which are farther apart and less dispersed will result in a better score.  The minimum score is zero, with lower values indicating better clustering.  Read more in the :ref:`User Guide <davies-bouldin_index>`.  .. versionadded:: 0.20",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.davies_bouldin_score",
            "xname": "X",
            "yname": "labels"
        }
    },
    "sklearn.metrics.dcg_score": {
        "cls": "Block",
        "typename": "dcg_score",
        "desc": "Compute Discounted Cumulative Gain.  Sum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount.  This ranking metric yields a high value if true labels are ranked high by ``y_score``.  Usually the Normalized Discounted Cumulative Gain (NDCG, computed by ndcg_score) is preferred.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "k": {
                "type": "number",
                "desc": "Only consider the highest k scores in the ranking. If None, use alloutputs.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "log_base": {
                "type": "number",
                "desc": "Base of the logarithm used for the discount. A low value means asharper discount (top results are more important).",
                "default": "2",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "ndarray of shape (n_samples,), default=None",
                "desc": "Sample weights. If None, all samples are given the same weight.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "ignore_ties": {
                "type": "boolean",
                "desc": "Assume that there are no ties in y_score (which is likely to be thecase if y_score is continuous) for efficiency gains.",
                "default": "False",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.dcg_score",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.det_curve": {
        "cls": "Block",
        "typename": "det_curve",
        "desc": "Compute error rates for different probability thresholds.  .. note::    This metric is used for evaluation of ranking and error tradeoffs of    a binary classification task.  Read more in the :ref:`User Guide <det_curve>`.  .. versionadded:: 0.24",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "pos_label": {
                "type": "number",
                "desc": "The label of the positive class.When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},``pos_label`` is set to 1, otherwise an error will be raised.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.det_curve",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.euclidean_distances": {
        "cls": "Block",
        "typename": "euclidean_distances",
        "desc": "Considering the rows of X (and Y=X) as vectors, compute the distance matrix between each pair of vectors.  For efficiency reasons, the euclidean distance between a pair of row vector x and y is computed as::      dist(x, y) = sqrt(dot(x, x) - 2 * dot(x, y) + dot(y, y))  This formulation has two advantages over other ways of computing distances. First, it is computationally efficient when dealing with sparse data. Second, if one argument varies but the other remains unchanged, then `dot(x, x)` and/or `dot(y, y)` can be pre-computed.  However, this is not the most precise way of doing this computation, because this equation potentially suffers from \"catastrophic cancellation\". Also, the distance matrix returned by this function may not be exactly symmetric as required by, e.g., ``scipy.spatial.distance`` functions.  Read more in the :ref:`User Guide <metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "Y": {
                "type": "option(array-like, sparse matrix)",
                "desc": "",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "Y_norm_squared": {
                "type": "array-like of shape (n_samples_Y,), default=None",
                "desc": "Pre-computed dot-products of vectors in Y (e.g.,``(Y**2).sum(axis=1)``)May be ignored in some cases, see the note below.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "squared": {
                "type": "boolean",
                "desc": "Return squared Euclidean distances.",
                "default": "False",
                "dictKeyOf": "kargs"
            },
            "X_norm_squared": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Pre-computed dot-products of vectors in X (e.g.,``(X**2).sum(axis=1)``)May be ignored in some cases, see the note below.Notes-----To achieve better accuracy, `X_norm_squared`and `Y_norm_squared` may beunused if they are passed as ``float32``.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.euclidean_distances",
            "xname": "X",
            "yname": "Y"
        }
    },
    "sklearn.metrics.explained_variance_score": {
        "cls": "Block",
        "typename": "explained_variance_score",
        "desc": "Explained variance regression score function.  Best possible score is 1.0, lower values are worse.  Read more in the :ref:`User Guide <explained_variance_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "multioutput": {
                "type": "option(raw_values, uniform_average, variance_weighted)",
                "desc": "Defines aggregating of multiple output scores.Array-like value defines weights used to average scores.'raw_values' :Returns a full set of scores in case of multioutput input.'uniform_average' :Scores of all outputs are averaged with uniform weight.'variance_weighted' :Scores of all outputs are averaged, weighted by the variancesof each individual output.",
                "default": "'uniform_average'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.explained_variance_score",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.f1_score": {
        "cls": "Block",
        "typename": "f1_score",
        "desc": "Compute the F1 score, also known as balanced F-score or F-measure.  The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is::      F1 = 2 * (precision * recall) / (precision + recall)  In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the ``average`` parameter.  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "pos_label": {
                "type": "string",
                "desc": "The class to report if ``average='binary'`` and the data is binary.If the data are multiclass or multilabel, this will be ignored;setting ``labels=[pos_label]`` and ``average != 'binary'`` will reportscores for that label only.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "average": {
                "type": "option(micro, macro, samples,weighted, binary)",
                "desc": "This parameter is required for multiclass/multilabel targets.If ``None``, the scores for each class are returned. Otherwise, thisdetermines the type of averaging performed on the data:``'binary'``:Only report results for the class specified by ``pos_label``.This is applicable only if targets (``y_{true,pred}``) are binary.``'micro'``:Calculate metrics globally by counting the total true positives,false negatives and false positives.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average weightedby support (the number of true instances for each label). Thisalters 'macro' to account for label imbalance; it can result in anF-score that is not between precision and recall.``'samples'``:Calculate metrics for each instance, and find their average (onlymeaningful for multilabel classification where this differs from:func:`accuracy_score`).",
                "default": "'binary'",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "zero_division": {
                "type": "\"warn\", 0 or 1, default=\"warn\"",
                "desc": "Sets the value to return when there is a zero division, i.e. when allpredictions and labels are negative. If set to \"warn\", this acts as 0,but warnings are also raised.",
                "default": "\"warn\"",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.f1_score",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.fbeta_score": {
        "cls": "Block",
        "typename": "fbeta_score",
        "desc": "Compute the F-beta score.  The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0.  The `beta` parameter determines the weight of recall in the combined score. ``beta < 1`` lends more weight to precision, while ``beta > 1`` favors recall (``beta -> 0`` considers only precision, ``beta -> +inf`` only recall).  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "beta": {
                "type": "number",
                "desc": "Determines the weight of recall in the combined score.",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The class to report if ``average='binary'`` and the data is binary.If the data are multiclass or multilabel, this will be ignored;setting ``labels=[pos_label]`` and ``average != 'binary'`` will reportscores for that label only.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "average": {
                "type": "option(micro, macro, samples, weighted, binary)",
                "desc": "This parameter is required for multiclass/multilabel targets.If ``None``, the scores for each class are returned. Otherwise, thisdetermines the type of averaging performed on the data:``'binary'``:Only report results for the class specified by ``pos_label``.This is applicable only if targets (``y_{true,pred}``) are binary.``'micro'``:Calculate metrics globally by counting the total true positives,false negatives and false positives.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average weightedby support (the number of true instances for each label). Thisalters 'macro' to account for label imbalance; it can result in anF-score that is not between precision and recall.``'samples'``:Calculate metrics for each instance, and find their average (onlymeaningful for multilabel classification where this differs from:func:`accuracy_score`).",
                "default": "'binary'",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "zero_division": {
                "type": "\"warn\", 0 or 1, default=\"warn\"",
                "desc": "Sets the value to return when there is a zero division, i.e. when allpredictions and labels are negative. If set to \"warn\", this acts as 0,but warnings are also raised.",
                "default": "\"warn\"",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.fbeta_score",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.fowlkes_mallows_score": {
        "cls": "Block",
        "typename": "fowlkes_mallows_score",
        "desc": "Measure the similarity of two clusterings of a set of points.  .. versionadded:: 0.18  The Fowlkes-Mallows index (FMI) is defined as the geometric mean between of the precision and recall::      FMI = TP / sqrt((TP + FP) * (TP + FN))  Where ``TP`` is the number of **True Positive** (i.e. the number of pair of points that belongs in the same clusters in both ``labels_true`` and ``labels_pred``), ``FP`` is the number of **False Positive** (i.e. the number of pair of points that belongs in the same clusters in ``labels_true`` and not in ``labels_pred``) and ``FN`` is the number of **False Negative** (i.e the number of pair of points that belongs in the same clusters in ``labels_pred`` and not in ``labels_True``).  The score ranges from 0 to 1. A high value indicates a good similarity between two clusters.  Read more in the :ref:`User Guide <fowlkes_mallows_scores>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sparse": {
                "type": "boolean",
                "desc": "Compute contingency matrix internally with sparse matrix.",
                "default": "False",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.fowlkes_mallows_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.get_scorer": {
        "cls": "Block",
        "typename": "get_scorer",
        "desc": "Get a scorer from string.  Read more in the :ref:`User Guide <scoring_parameter>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "scoring": {
                "type": "string",
                "desc": "Scoring method as string. If callable it is returned as is.",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.get_scorer"
        }
    },
    "sklearn.metrics.hamming_loss": {
        "cls": "Block",
        "typename": "hamming_loss",
        "desc": "Compute the average Hamming loss.  The Hamming loss is the fraction of labels that are incorrectly predicted.  Read more in the :ref:`User Guide <hamming_loss>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.hamming_loss",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.hinge_loss": {
        "cls": "Block",
        "typename": "hinge_loss",
        "desc": "Average hinge loss (non-regularized).  In binary class case, assuming labels in y_true are encoded with +1 and -1, when a prediction mistake is made, ``margin = y_true * pred_decision`` is always negative (since the signs disagree), implying ``1 - margin`` is always greater than 1.  The cumulated hinge loss is therefore an upper bound of the number of mistakes made by the classifier.  In multiclass case, the function expects that either all the labels are included in y_true or an optional labels argument is provided which contains all the labels. The multilabel margin is calculated according to Crammer-Singer's method. As in the binary case, the cumulated hinge loss is an upper bound of the number of mistakes made by the classifier.  Read more in the :ref:`User Guide <hinge_loss>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "pred_decision": {
                "type": "array of shape (n_samples,) or (n_samples, n_classes)",
                "desc": "Predicted decisions, as output by decision_function (floats).",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.hinge_loss",
            "yname": "labels"
        }
    },
    "sklearn.metrics.homogeneity_completeness_v_measure": {
        "cls": "Block",
        "typename": "homogeneity_completeness_v_measure",
        "desc": "Compute the homogeneity and completeness and V-Measure scores at once.  Those metrics are based on normalized conditional entropy measures of the clustering labeling to evaluate given the knowledge of a Ground Truth class labels of the same samples.  A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.  A clustering result satisfies completeness if all the data points that are members of a given class are elements of the same cluster.  Both scores have positive values between 0.0 and 1.0, larger values being desirable.  Those 3 metrics are independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score values in any way.  V-Measure is furthermore symmetric: swapping ``labels_true`` and ``label_pred`` will give the same score. This does not hold for homogeneity and completeness. V-Measure is identical to :func:`normalized_mutual_info_score` with the arithmetic averaging method.  Read more in the :ref:`User Guide <homogeneity_completeness>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "beta": {
                "type": "number",
                "desc": "Ratio of weight attributed to ``homogeneity`` vs ``completeness``.If ``beta`` is greater than 1, ``completeness`` is weighted morestrongly in the calculation. If ``beta`` is less than 1,``homogeneity`` is weighted more strongly.",
                "default": "1.0",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.homogeneity_completeness_v_measure",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.homogeneity_score": {
        "cls": "Block",
        "typename": "homogeneity_score",
        "desc": "Homogeneity metric of a cluster labeling given a ground truth.  A clustering result satisfies homogeneity if all of its clusters contain only data points which are members of a single class.  This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way.  This metric is not symmetric: switching ``label_true`` with ``label_pred`` will return the :func:`completeness_score` which will be different in general.  Read more in the :ref:`User Guide <homogeneity_completeness>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.homogeneity_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.jaccard_score": {
        "cls": "Block",
        "typename": "jaccard_score",
        "desc": "Jaccard similarity coefficient score.  The Jaccard index [1], or Jaccard similarity coefficient, defined as the size of the intersection divided by the size of the union of two label sets, is used to compare set of predicted labels for a sample to the corresponding set of labels in ``y_true``.  Read more in the :ref:`User Guide <jaccard_similarity_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "pos_label": {
                "type": "string",
                "desc": "The class to report if ``average='binary'`` and the data is binary.If the data are multiclass or multilabel, this will be ignored;setting ``labels=[pos_label]`` and ``average != 'binary'`` will reportscores for that label only.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "average": {
                "type": "option(None, micro, macro, samples, weighted,             binary)",
                "desc": "If ``None``, the scores for each class are returned. Otherwise, thisdetermines the type of averaging performed on the data:``'binary'``:Only report results for the class specified by ``pos_label``.This is applicable only if targets (``y_{true,pred}``) are binary.``'micro'``:Calculate metrics globally by counting the total true positives,false negatives and false positives.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average, weightedby support (the number of true instances for each label). Thisalters 'macro' to account for label imbalance.``'samples'``:Calculate metrics for each instance, and find their average (onlymeaningful for multilabel classification).",
                "default": "'binary'",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "zero_division": {
                "type": "\"warn\", {0.0, 1.0}, default=\"warn\"",
                "desc": "Sets the value to return when there is a zero division, i.e. when therethere are no negative values in predictions and labels. If set to\"warn\", this acts like 0, but a warning is also raised.",
                "default": "\"warn\"",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.jaccard_score",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.label_ranking_average_precision_score": {
        "cls": "Block",
        "typename": "label_ranking_average_precision_score",
        "desc": "Compute ranking-based average precision.  Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score.  This metric is used in multilabel ranking problem, where the goal is to give better rank to the labels associated to each sample.  The obtained score is always strictly greater than 0 and the best value is 1.  Read more in the :ref:`User Guide <label_ranking_average_precision>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights... versionadded:: 0.20",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.label_ranking_average_precision_score",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.label_ranking_loss": {
        "cls": "Block",
        "typename": "label_ranking_loss",
        "desc": "Compute Ranking loss measure.  Compute the average number of label pairs that are incorrectly ordered given y_score weighted by the size of the label set and the number of labels not in the label set.  This is similar to the error set size, but weighted by the number of relevant and irrelevant labels. The best performance is achieved with a ranking loss of zero.  Read more in the :ref:`User Guide <label_ranking_loss>`.  .. versionadded:: 0.17    A function *label_ranking_loss*",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.label_ranking_loss",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.log_loss": {
        "cls": "Block",
        "typename": "log_loss",
        "desc": "Log loss, aka logistic loss or cross-entropy loss.  This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns ``y_pred`` probabilities for its training data ``y_true``. The log loss is only defined for two or more labels. For a single sample with true label :math:`y \\in \\{0,1\\}` and and a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log loss is:  .. math::     L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))  Read more in the :ref:`User Guide <log_loss>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "eps": {
                "type": "number",
                "desc": "Log loss is undefined for p=0 or p=1, so probabilities areclipped to max(eps, min(1 - eps, p)).",
                "default": "1e-15",
                "dictKeyOf": "kargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "If true, return the mean loss per sample.Otherwise, return the sum of the per-sample losses.",
                "default": "True",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.log_loss",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.make_scorer": {
        "cls": "Block",
        "typename": "make_scorer",
        "desc": "Make a scorer from a performance metric or loss function.  This factory function wraps scoring functions for use in :class:`~sklearn.model_selection.GridSearchCV` and :func:`~sklearn.model_selection.cross_val_score`. It takes a score function, such as :func:`~sklearn.metrics.accuracy_score`, :func:`~sklearn.metrics.mean_squared_error`, :func:`~sklearn.metrics.adjusted_rand_index` or :func:`~sklearn.metrics.average_precision` and returns a callable that scores an estimator's output. The signature of the call is `(estimator, X, y)` where `estimator` is the model to be evaluated, `X` is the data and `y` is the ground truth labeling (or `None` in the case of unsupervised models).  Read more in the :ref:`User Guide <scoring>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "score_func": {
                "type": "callable",
                "desc": "Score function (or loss function) with signature``score_func(y, y_pred, **kwargs)``.",
                "dictKeyOf": "kargs"
            },
            "greater_is_better": {
                "type": "boolean",
                "desc": "Whether score_func is a score function (default), meaning high is good,or a loss function, meaning low is good. In the latter case, thescorer object will sign-flip the outcome of the score_func.",
                "default": "True",
                "dictKeyOf": "kargs"
            },
            "needs_proba": {
                "type": "boolean",
                "desc": "Whether score_func requires predict_proba to get probability estimatesout of a classifier.If True, for binary `y_true`, the score function is supposed to accepta 1D `y_pred` (i.e., probability of the positive class, shape`(n_samples,)`).",
                "default": "False",
                "dictKeyOf": "kargs"
            },
            "needs_threshold": {
                "type": "boolean",
                "desc": "Whether score_func takes a continuous decision certainty.This only works for binary classification using estimators thathave either a decision_function or predict_proba method.If True, for binary `y_true`, the score function is supposed to accepta 1D `y_pred` (i.e., probability of the positive class or the decisionfunction, shape `(n_samples,)`).For example ``average_precision`` or the area under the roc curvecan not be computed using discrete predictions alone.**kwargs : additional argumentsAdditional parameters to be passed to score_func.",
                "default": "False",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.make_scorer"
        }
    },
    "sklearn.metrics.matthews_corrcoef": {
        "cls": "Block",
        "typename": "matthews_corrcoef",
        "desc": "Compute the Matthews correlation coefficient (MCC).  The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction.  The statistic is also known as the phi coefficient. [source: Wikipedia]  Binary and multiclass labels are supported.  Only in the binary case does this relate to information about true and false positives and negatives. See references below.  Read more in the :ref:`User Guide <matthews_corrcoef>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights... versionadded:: 0.18",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.matthews_corrcoef",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.max_error": {
        "cls": "Block",
        "typename": "max_error",
        "desc": "max_error metric calculates the maximum residual error.  Read more in the :ref:`User Guide <max_error>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.max_error",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mean_absolute_error": {
        "cls": "Block",
        "typename": "mean_absolute_error",
        "desc": "Mean absolute error regression loss.  Read more in the :ref:`User Guide <mean_absolute_error>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "multioutput": {
                "type": "option(raw_values, uniform_average)",
                "desc": "Defines aggregating of multiple output values.Array-like value defines weights used to average errors.'raw_values' :Returns a full set of errors in case of multioutput input.'uniform_average' :Errors of all outputs are averaged with uniform weight.",
                "default": "'uniform_average'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mean_absolute_error",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mean_absolute_percentage_error": {
        "cls": "Block",
        "typename": "mean_absolute_percentage_error",
        "desc": "Mean absolute percentage error regression loss.  Note here that we do not represent the output as a percentage in range [0, 100]. Instead, we represent it in range [0, 1/eps]. Read more in the :ref:`User Guide <mean_absolute_percentage_error>`.  .. versionadded:: 0.24",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "multioutput": {
                "type": "option(raw_values, uniform_average)",
                "desc": "Defines aggregating of multiple output values.Array-like value defines weights used to average errors.If input is list then the shape must be (n_outputs,).'raw_values' :Returns a full set of errors in case of multioutput input.'uniform_average' :Errors of all outputs are averaged with uniform weight.",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mean_absolute_percentage_error",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mean_gamma_deviance": {
        "cls": "Block",
        "typename": "mean_gamma_deviance",
        "desc": "Mean Gamma deviance regression loss.  Gamma deviance is equivalent to the Tweedie deviance with the power parameter `power=2`. It is invariant to scaling of the target variable, and measures relative errors.  Read more in the :ref:`User Guide <mean_tweedie_deviance>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mean_gamma_deviance",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mean_poisson_deviance": {
        "cls": "Block",
        "typename": "mean_poisson_deviance",
        "desc": "Mean Poisson deviance regression loss.  Poisson deviance is equivalent to the Tweedie deviance with the power parameter `power=1`.  Read more in the :ref:`User Guide <mean_tweedie_deviance>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mean_poisson_deviance",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mean_squared_error": {
        "cls": "Block",
        "typename": "mean_squared_error",
        "desc": "Mean squared error regression loss.  Read more in the :ref:`User Guide <mean_squared_error>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "multioutput": {
                "type": "option(raw_values, uniform_average)",
                "desc": "Defines aggregating of multiple output values.Array-like value defines weights used to average errors.'raw_values' :Returns a full set of errors in case of multioutput input.'uniform_average' :Errors of all outputs are averaged with uniform weight.",
                "default": "'uniform_average'",
                "dictKeyOf": "kargs"
            },
            "squared": {
                "type": "boolean",
                "desc": "If True returns MSE value, if False returns RMSE value.",
                "default": "True",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mean_squared_error",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mean_squared_log_error": {
        "cls": "Block",
        "typename": "mean_squared_log_error",
        "desc": "Mean squared logarithmic error regression loss.  Read more in the :ref:`User Guide <mean_squared_log_error>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "multioutput": {
                "type": "option(raw_values, uniform_average)",
                "desc": "Defines aggregating of multiple output values.Array-like value defines weights used to average errors.'raw_values' :Returns a full set of errors when the input is of multioutputformat.'uniform_average' :Errors of all outputs are averaged with uniform weight.",
                "default": "'uniform_average'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mean_squared_log_error",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mean_tweedie_deviance": {
        "cls": "Block",
        "typename": "mean_tweedie_deviance",
        "desc": "Mean Tweedie deviance regression loss.  Read more in the :ref:`User Guide <mean_tweedie_deviance>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "power": {
                "type": "number",
                "desc": "Tweedie power parameter. Either power <= 0 or power >= 1.The higher `p` the less weight is given to extremedeviations between true and predicted targets.- power < 0: Extreme stable distribution. Requires: y_pred > 0.- power = 0 : Normal distribution, output corresponds tomean_squared_error. y_true and y_pred can be any real numbers.- power = 1 : Poisson distribution. Requires: y_true >= 0 andy_pred > 0.- 1 < p < 2 : Compound Poisson distribution. Requires: y_true >= 0and y_pred > 0.- power = 2 : Gamma distribution. Requires: y_true > 0 and y_pred > 0.- power = 3 : Inverse Gaussian distribution. Requires: y_true > 0and y_pred > 0.- otherwise : Positive stable distribution. Requires: y_true > 0and y_pred > 0.",
                "default": "0",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mean_tweedie_deviance",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.median_absolute_error": {
        "cls": "Block",
        "typename": "median_absolute_error",
        "desc": "Median absolute error regression loss.  Median absolute error output is non-negative floating point. The best value is 0.0. Read more in the :ref:`User Guide <median_absolute_error>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "multioutput": {
                "type": "option(raw_values, uniform_average)",
                "desc": "Defines aggregating of multiple output values. Array-like value definesweights used to average errors.'raw_values' :Returns a full set of errors in case of multioutput input.'uniform_average' :Errors of all outputs are averaged with uniform weight.",
                "default": "'uniform_average'",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.median_absolute_error",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.multilabel_confusion_matrix": {
        "cls": "Block",
        "typename": "multilabel_confusion_matrix",
        "desc": "Compute a confusion matrix for each class or sample.  .. versionadded:: 0.21  Compute class-wise (default) or sample-wise (samplewise=True) multilabel confusion matrix to evaluate the accuracy of a classification, and output confusion matrices for each class or sample.  In multilabel confusion matrix :math:`MCM`, the count of true negatives is :math:`MCM_{:,0,0}`, false negatives is :math:`MCM_{:,1,0}`, true positives is :math:`MCM_{:,1,1}` and false positives is :math:`MCM_{:,0,1}`.  Multiclass data will be treated as if binarized under a one-vs-rest transformation. Returned confusion matrices will be in the order of sorted unique labels in the union of (y_true, y_pred).  Read more in the :ref:`User Guide <multilabel_confusion_matrix>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "samplewise": {
                "type": "boolean",
                "desc": "In the multilabel case, this calculates a confusion matrix per sample.",
                "default": "False",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.multilabel_confusion_matrix",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.mutual_info_score": {
        "cls": "Block",
        "typename": "mutual_info_score",
        "desc": "Mutual Information between two clusterings.  The Mutual Information is a measure of the similarity between two labels of the same data. Where :math:`|U_i|` is the number of the samples in cluster :math:`U_i` and :math:`|V_j|` is the number of the samples in cluster :math:`V_j`, the Mutual Information between clusterings :math:`U` and :math:`V` is given as:  .. math::      MI(U,V)=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i\\cap V_j|}{N}     \\log\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}  This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way.  This metric is furthermore symmetric: switching ``label_true`` with ``label_pred`` will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.  Read more in the :ref:`User Guide <mutual_info_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "contingency": {
                "type": "option(ndarray, sparse matrix)",
                "desc": "A contingency matrix given by the :func:`contingency_matrix` function.If value is ``None``, it will be computed, otherwise the given value isused, with ``labels_true`` and ``labels_pred`` ignored.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.mutual_info_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.nan_euclidean_distances": {
        "cls": "Block",
        "typename": "nan_euclidean_distances",
        "desc": "Calculate the euclidean distances in the presence of missing values.  Compute the euclidean distance between each pair of samples in X and Y, where Y=X is assumed if Y=None. When calculating the distance between a pair of samples, this formulation ignores feature coordinates with a missing value in either sample and scales up the weight of the remaining coordinates:      dist(x,y) = sqrt(weight * sq. distance from present coordinates)     where,     weight = Total # of coordinates / # of present coordinates  For example, the distance between ``[3, na, na, 6]`` and ``[1, na, 4, 5]`` is:      .. math::         \\sqrt{\\frac{4}{2}((3-1)^2 + (6-5)^2)}  If all the coordinates are missing or if there are no common present coordinates then NaN is returned for that pair.  Read more in the :ref:`User Guide <metrics>`.  .. versionadded:: 0.22",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "Y": {
                "type": "array-like of shape=(n_samples_Y, n_features), default=None",
                "desc": "",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "squared": {
                "type": "boolean",
                "desc": "Return squared Euclidean distances.",
                "default": "False",
                "dictKeyOf": "kargs"
            },
            "missing_values": {
                "type": "np.nan or int, default=np.nan",
                "desc": "Representation of missing value.",
                "default": "np.nan",
                "dictKeyOf": "kargs"
            },
            "copy": {
                "type": "boolean",
                "desc": "Make and use a deep copy of X and Y (if Y exists).",
                "default": "True",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.nan_euclidean_distances",
            "xname": "X",
            "yname": "Y"
        }
    },
    "sklearn.metrics.ndcg_score": {
        "cls": "Block",
        "typename": "ndcg_score",
        "desc": "Compute Normalized Discounted Cumulative Gain.  Sum the true scores ranked in the order induced by the predicted scores, after applying a logarithmic discount. Then divide by the best possible score (Ideal DCG, obtained for a perfect ranking) to obtain a score between 0 and 1.  This ranking metric yields a high value if true labels are ranked high by ``y_score``.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "k": {
                "type": "number",
                "desc": "Only consider the highest k scores in the ranking. If None, use alloutputs.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "ndarray of shape (n_samples,), default=None",
                "desc": "Sample weights. If None, all samples are given the same weight.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "ignore_ties": {
                "type": "boolean",
                "desc": "Assume that there are no ties in y_score (which is likely to be thecase if y_score is continuous) for efficiency gains.",
                "default": "False",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.ndcg_score",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.normalized_mutual_info_score": {
        "cls": "Block",
        "typename": "normalized_mutual_info_score",
        "desc": "Normalized Mutual Information between two clusterings.  Normalized Mutual Information (NMI) is a normalization of the Mutual Information (MI) score to scale the results between 0 (no mutual information) and 1 (perfect correlation). In this function, mutual information is normalized by some generalized mean of ``H(labels_true)`` and ``H(labels_pred))``, defined by the `average_method`.  This measure is not adjusted for chance. Therefore :func:`adjusted_mutual_info_score` might be preferred.  This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way.  This metric is furthermore symmetric: switching ``label_true`` with ``label_pred`` will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.  Read more in the :ref:`User Guide <mutual_info_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "average_method": {
                "type": "string",
                "desc": "How to compute the normalizer in the denominator. Possible optionsare 'min', 'geometric', 'arithmetic', and 'max'... versionadded:: 0.20.. versionchanged:: 0.22The default value of ``average_method`` changed from 'geometric' to'arithmetic'.",
                "default": "'arithmetic'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.normalized_mutual_info_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.pair_confusion_matrix": {
        "cls": "Block",
        "typename": "pair_confusion_matrix",
        "desc": "Pair confusion matrix arising from two clusterings.  The pair confusion matrix :math:`C` computes a 2 by 2 similarity matrix between two clusterings by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings.  Considering a pair of samples that is clustered together a positive pair, then as in binary classification the count of true negatives is :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is :math:`C_{11}` and false positives is :math:`C_{01}`.  Read more in the :ref:`User Guide <pair_confusion_matrix>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.pair_confusion_matrix",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.pairwise_distances": {
        "cls": "Block",
        "typename": "pairwise_distances",
        "desc": "Compute the distance matrix from a vector array X and optional Y.  This method takes either a vector array or a distance matrix, and returns a distance matrix. If the input is a vector array, the distances are computed. If the input is a distances matrix, it is returned instead.  This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array.  If Y is given (default is None), then the returned matrix is the pairwise distance between the arrays from both X and Y.  Valid values for metric are:  - From scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2',   'manhattan']. These metrics support sparse matrix   inputs.   ['nan_euclidean'] but it does not yet support sparse matrices.  - From scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev',   'correlation', 'dice', 'hamming', 'jaccard', 'kulsinski', 'mahalanobis',   'minkowski', 'rogerstanimoto', 'russellrao', 'seuclidean',   'sokalmichener', 'sokalsneath', 'sqeuclidean', 'yule']   See the documentation for scipy.spatial.distance for details on these   metrics. These metrics do not support sparse matrix inputs.  Note that in the case of 'cityblock', 'cosine' and 'euclidean' (which are valid scipy.spatial.distance metrics), the scikit-learn implementation will be used, which is faster and has support for sparse matrices (except for 'cityblock'). For a verbose description of the metrics from scikit-learn, see the __doc__ of the sklearn.pairwise.distance_metrics function.  Read more in the :ref:`User Guide <metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "Y": {
                "type": "ndarray of shape (n_samples_Y, n_features), default=None",
                "desc": "An optional second feature array. Only allowed ifmetric != \"precomputed\".",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string, it must be one of the optionsallowed by scipy.spatial.distance.pdist for its metric parameter, ora metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``.If metric is \"precomputed\", X is assumed to be a distance matrix.Alternatively, if metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays from X as input and return a value indicatingthe distance between them.",
                "default": "'euclidean'",
                "dictKeyOf": "kargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This works by breakingdown the pairwise matrix into n_jobs even slices and computing them inparallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "force_all_finite": {
                "type": "boolean",
                "desc": "Whether to raise an error on np.inf, np.nan, pd.NA in array. Ignoredfor a metric listed in ``pairwise.PAIRWISE_DISTANCE_FUNCTIONS``. Thepossibilities are:- True: Force all values of array to be finite.- False: accepts np.inf, np.nan, pd.NA in array.- 'allow-nan': accepts only np.nan and pd.NA values in array. Valuescannot be infinite... versionadded:: 0.22``force_all_finite`` accepts the string ``'allow-nan'``... versionchanged:: 0.23Accepts `pd.NA` and converts it into `np.nan`.**kwds : optional keyword parametersAny further parameters are passed directly to the distance function.If using a scipy.spatial.distance metric, the parameters are stillmetric dependent. See the scipy docs for usage examples.",
                "default": "True",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.pairwise_distances",
            "xname": "X",
            "yname": "Y"
        }
    },
    "sklearn.metrics.pairwise_distances_argmin": {
        "cls": "Block",
        "typename": "pairwise_distances_argmin",
        "desc": "Compute minimum distances between one point and a set of points.  This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance).  This is mostly equivalent to calling:      pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis)  but uses much less memory, and is faster for large arrays.  This function works with dense 2D arrays only.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "Y": {
                "type": "array-like of shape (n_samples_Y, n_features)",
                "desc": "Arrays containing points.",
                "dictKeyOf": "kargs"
            },
            "axis": {
                "type": "number",
                "desc": "Axis along which the argmin and distances are to be computed.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "metric": {
                "type": "string",
                "desc": "Metric to use for distance computation. Any metric from scikit-learnor scipy.spatial.distance can be used.If metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays as input and return one value indicating thedistance between them. This works for Scipy's metrics, but is lessefficient than passing the metric name as a string.Distance matrices are not supported.Valid values for metric are:- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2','manhattan']- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev','correlation', 'dice', 'hamming', 'jaccard', 'kulsinski','mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao','seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean','yule']See the documentation for scipy.spatial.distance for details on thesemetrics.",
                "default": "\"euclidean\"",
                "dictKeyOf": "kargs"
            },
            "metric_kwargs": {
                "type": "dict, default=None",
                "desc": "Keyword arguments to pass to specified metric function.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.pairwise_distances_argmin",
            "xname": "X",
            "yname": "Y"
        }
    },
    "sklearn.metrics.pairwise_distances_argmin_min": {
        "cls": "Block",
        "typename": "pairwise_distances_argmin_min",
        "desc": "Compute minimum distances between one point and a set of points.  This function computes for each row in X, the index of the row of Y which is closest (according to the specified distance). The minimal distances are also returned.  This is mostly equivalent to calling:      (pairwise_distances(X, Y=Y, metric=metric).argmin(axis=axis),      pairwise_distances(X, Y=Y, metric=metric).min(axis=axis))  but uses much less memory, and is faster for large arrays.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "Y": {
                "type": "option(array-like, sparse matrix)",
                "desc": "Array containing points.",
                "dictKeyOf": "kargs"
            },
            "axis": {
                "type": "number",
                "desc": "Axis along which the argmin and distances are to be computed.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "metric": {
                "type": "string",
                "desc": "Metric to use for distance computation. Any metric from scikit-learnor scipy.spatial.distance can be used.If metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays as input and return one value indicating thedistance between them. This works for Scipy's metrics, but is lessefficient than passing the metric name as a string.Distance matrices are not supported.Valid values for metric are:- from scikit-learn: ['cityblock', 'cosine', 'euclidean', 'l1', 'l2','manhattan']- from scipy.spatial.distance: ['braycurtis', 'canberra', 'chebyshev','correlation', 'dice', 'hamming', 'jaccard', 'kulsinski','mahalanobis', 'minkowski', 'rogerstanimoto', 'russellrao','seuclidean', 'sokalmichener', 'sokalsneath', 'sqeuclidean','yule']See the documentation for scipy.spatial.distance for details on thesemetrics.",
                "default": "'euclidean'",
                "dictKeyOf": "kargs"
            },
            "metric_kwargs": {
                "type": "dict, default=None",
                "desc": "Keyword arguments to pass to specified metric function.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.pairwise_distances_argmin_min",
            "xname": "X",
            "yname": "Y"
        }
    },
    "sklearn.metrics.pairwise_distances_chunked": {
        "cls": "Block",
        "typename": "pairwise_distances_chunked",
        "desc": "Generate a distance matrix chunk by chunk with optional reduction.  In cases where not all of a pairwise distance matrix needs to be stored at once, this is used to calculate pairwise distances in ``working_memory``-sized chunks.  If ``reduce_func`` is given, it is run on each chunk and its return values are concatenated into lists, arrays or sparse matrices.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "Y": {
                "type": "ndarray of shape (n_samples_Y, n_features), default=None",
                "desc": "An optional second feature array. Only allowed ifmetric != \"precomputed\".",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "reduce_func": {
                "type": "callable, default=None",
                "desc": "The function which is applied on each chunk of the distance matrix,reducing it to needed values. ``reduce_func(D_chunk, start)``is called repeatedly, where ``D_chunk`` is a contiguous verticalslice of the pairwise distance matrix, starting at row ``start``.It should return one of: None; an array, a list, or a sparse matrixof length ``D_chunk.shape[0]``; or a tuple of such objects. ReturningNone is useful for in-place operations, rather than reductions.If None, pairwise_distances_chunked returns a generator of verticalchunks of the distance matrix.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string, it must be one of the optionsallowed by scipy.spatial.distance.pdist for its metric parameter, ora metric listed in pairwise.PAIRWISE_DISTANCE_FUNCTIONS.If metric is \"precomputed\", X is assumed to be a distance matrix.Alternatively, if metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two arrays from X as input and return a value indicatingthe distance between them.",
                "default": "'euclidean'",
                "dictKeyOf": "kargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This works by breakingdown the pairwise matrix into n_jobs even slices and computing them inparallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "working_memory": {
                "type": "number",
                "desc": "The sought maximum memory for temporary distance matrix chunks.When None (default), the value of``sklearn.get_config()['working_memory']`` is used.`**kwds` : optional keyword parametersAny further parameters are passed directly to the distance function.If using a scipy.spatial.distance metric, the parameters are stillmetric dependent. See the scipy docs for usage examples.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.pairwise_distances_chunked",
            "xname": "X",
            "yname": "Y"
        }
    },
    "sklearn.metrics.pairwise_kernels": {
        "cls": "Block",
        "typename": "pairwise_kernels",
        "desc": "Compute the kernel between arrays X and optional array Y.  This method takes either a vector array or a kernel matrix, and returns a kernel matrix. If the input is a vector array, the kernels are computed. If the input is a kernel matrix, it is returned instead.  This method provides a safe way to take a kernel matrix as input, while preserving compatibility with many other algorithms that take a vector array.  If Y is given (default is None), then the returned matrix is the pairwise kernel between the arrays from both X and Y.  Valid values for metric are:     ['additive_chi2', 'chi2', 'linear', 'poly', 'polynomial', 'rbf',     'laplacian', 'sigmoid', 'cosine']  Read more in the :ref:`User Guide <metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "Y": {
                "type": "ndarray of shape (n_samples_Y, n_features), default=None",
                "desc": "A second feature array only if X has shape (n_samples_X, n_features).",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating kernel between instances in afeature array. If metric is a string, it must be one of the metricsin pairwise.PAIRWISE_KERNEL_FUNCTIONS.If metric is \"precomputed\", X is assumed to be a kernel matrix.Alternatively, if metric is a callable function, it is called on eachpair of instances (rows) and the resulting value recorded. The callableshould take two rows from X as input and return the correspondingkernel value as a single number. This means that callables from:mod:`sklearn.metrics.pairwise` are not allowed, as they operate onmatrices, not single samples. Use the string identifying the kernelinstead.",
                "default": "\"linear\"",
                "dictKeyOf": "kargs"
            },
            "filter_params": {
                "type": "boolean",
                "desc": "Whether to filter invalid parameters or not.",
                "default": "False",
                "dictKeyOf": "kargs"
            },
            "n_jobs": {
                "type": "number",
                "desc": "The number of jobs to use for the computation. This works by breakingdown the pairwise matrix into n_jobs even slices and computing them inparallel.``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.``-1`` means using all processors. See :term:`Glossary <n_jobs>`for more details.**kwds : optional keyword parametersAny further parameters are passed directly to the kernel function.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.pairwise_kernels",
            "xname": "X",
            "yname": "Y"
        }
    },
    "sklearn.metrics.plot_confusion_matrix": {
        "cls": "Block",
        "typename": "plot_confusion_matrix",
        "desc": "Plot Confusion Matrix.  Read more in the :ref:`User Guide <confusion_matrix>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "estimator": {
                "type": "estimator instance",
                "desc": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`in which the last estimator is a classifier.",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "normalize": {
                "type": "option(true, pred, all)",
                "desc": "Normalizes confusion matrix over the true (rows), predicted (columns)conditions or all the population. If None, confusion matrix will not benormalized.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "display_labels": {
                "type": "array-like of shape (n_classes,), default=None",
                "desc": "Target names used for plotting. By default, `labels` will be used ifit is defined, otherwise the unique labels of `y_true` and `y_pred`will be used.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "include_values": {
                "type": "boolean",
                "desc": "Includes values in confusion matrix.",
                "default": "True",
                "dictKeyOf": "kargs"
            },
            "xticks_rotation": {
                "type": "option(vertical, horizontal)",
                "desc": "Rotation of xtick labels.",
                "default": "'horizontal'",
                "dictKeyOf": "kargs"
            },
            "values_format": {
                "type": "string",
                "desc": "Format specification for values in confusion matrix. If `None`,the format specification is 'd' or '.2g' whichever is shorter.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "cmap": {
                "type": "string",
                "desc": "Colormap recognized by matplotlib.",
                "default": "'viridis'",
                "dictKeyOf": "kargs"
            },
            "ax": {
                "type": "matplotlib Axes, default=None",
                "desc": "Axes object to plot on. If `None`, a new figure and axes iscreated.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "colorbar": {
                "type": "boolean",
                "desc": "Whether or not to add a colorbar to the plot... versionadded:: 0.24",
                "default": "True",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.plot_confusion_matrix",
            "xname": "X",
            "yname": "labels"
        }
    },
    "sklearn.metrics.plot_det_curve": {
        "cls": "Block",
        "typename": "plot_det_curve",
        "desc": "Plot detection error tradeoff (DET) curve.  Extra keyword arguments will be passed to matplotlib's `plot`.  Read more in the :ref:`User Guide <visualizations>`.  .. versionadded:: 0.24",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "estimator": {
                "type": "estimator instance",
                "desc": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`in which the last estimator is a classifier.",
                "dictKeyOf": "kargs"
            },
            "y": {
                "type": "array-like of shape (n_samples,)",
                "desc": "Target values.",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "response_method": {
                "type": "option(predict_proba, decision_function, auto)",
                "desc": "Specifies whether to use :term:`predict_proba` or:term:`decision_function` as the predicted target response. If set to'auto', :term:`predict_proba` is tried first and if it does not exist:term:`decision_function` is tried next.",
                "default": "'auto'",
                "dictKeyOf": "kargs"
            },
            "name": {
                "type": "string",
                "desc": "Name of DET curve for labeling. If `None`, use the name of theestimator.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "ax": {
                "type": "matplotlib axes, default=None",
                "desc": "Axes object to plot on. If `None`, a new figure and axes is created.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The label of the positive class.When `pos_label=None`, if `y_true` is in {-1, 1} or {0, 1},`pos_label` is set to 1, otherwise an error will be raised.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.plot_det_curve",
            "xname": "X"
        }
    },
    "sklearn.metrics.plot_precision_recall_curve": {
        "cls": "Block",
        "typename": "plot_precision_recall_curve",
        "desc": "Plot Precision Recall Curve for binary classifiers.  Extra keyword arguments will be passed to matplotlib's `plot`.  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "estimator": {
                "type": "estimator instance",
                "desc": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`in which the last estimator is a classifier.",
                "dictKeyOf": "kargs"
            },
            "y": {
                "type": "array-like of shape (n_samples,)",
                "desc": "Binary target values.",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "response_method": {
                "type": "option(predict_proba, decision_function, auto)",
                "desc": "Specifies whether to use :term:`predict_proba` or:term:`decision_function` as the target response. If set to 'auto',:term:`predict_proba` is tried first and if it does not exist:term:`decision_function` is tried next.",
                "default": "'auto'",
                "dictKeyOf": "kargs"
            },
            "name": {
                "type": "string",
                "desc": "Name for labeling curve. If `None`, the name of theestimator is used.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "ax": {
                "type": "matplotlib axes, default=None",
                "desc": "Axes object to plot on. If `None`, a new figure and axes is created.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The class considered as the positive class when computing the precisionand recall metrics. By default, `estimators.classes_[1]` is consideredas the positive class... versionadded:: 0.24**kwargs : dictKeyword arguments to be passed to matplotlib's `plot`.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.plot_precision_recall_curve",
            "xname": "X"
        }
    },
    "sklearn.metrics.plot_roc_curve": {
        "cls": "Block",
        "typename": "plot_roc_curve",
        "desc": "Plot Receiver operating characteristic (ROC) curve.  Extra keyword arguments will be passed to matplotlib's `plot`.  Read more in the :ref:`User Guide <visualizations>`.",
        "childof": "skll.plugin.sklearn.block.SklMethod",
        "pytype": "skll.plugin.sklearn.block.SklMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "estimator": {
                "type": "estimator instance",
                "desc": "Fitted classifier or a fitted :class:`~sklearn.pipeline.Pipeline`in which the last estimator is a classifier.",
                "dictKeyOf": "kargs"
            },
            "y": {
                "type": "array-like of shape (n_samples,)",
                "desc": "Target values.",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "drop_intermediate": {
                "type": "boolean",
                "desc": "Whether to drop some suboptimal thresholds which would not appearon a plotted ROC curve. This is useful in order to create lighterROC curves.",
                "default": "True",
                "dictKeyOf": "kargs"
            },
            "response_method": {
                "type": "option(predict_proba, decision_function, auto)",
                "desc": "Specifies whether to use :term:`predict_proba` or:term:`decision_function` as the target response. If set to 'auto',:term:`predict_proba` is tried first and if it does not exist:term:`decision_function` is tried next.",
                "default": "'auto'",
                "dictKeyOf": "kargs"
            },
            "name": {
                "type": "string",
                "desc": "Name of ROC Curve for labeling. If `None`, use the name of theestimator.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "ax": {
                "type": "matplotlib axes, default=None",
                "desc": "Axes object to plot on. If `None`, a new figure and axes is created.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The class considered as the positive class when computing the roc aucmetrics. By default, `estimators.classes_[1]` is consideredas the positive class... versionadded:: 0.24",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.plot_roc_curve",
            "xname": "X"
        }
    },
    "sklearn.metrics.precision_recall_curve": {
        "cls": "Block",
        "typename": "precision_recall_curve",
        "desc": "Compute precision-recall pairs for different probability thresholds.  Note: this implementation is restricted to the binary classification task.  The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of true positives and ``fp`` the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.  The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of true positives and ``fn`` the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.  The last precision and recall values are 1. and 0. respectively and do not have a corresponding threshold. This ensures that the graph starts on the y axis.  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "probas_pred": {
                "type": "ndarray of shape (n_samples,)",
                "desc": "Estimated probabilities or output of a decision function.",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "number",
                "desc": "The label of the positive class.When ``pos_label=None``, if y_true is in {-1, 1} or {0, 1},``pos_label`` is set to 1, otherwise an error will be raised.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.precision_recall_curve",
            "yname": "y_true"
        }
    },
    "sklearn.metrics.precision_recall_fscore_support": {
        "cls": "Block",
        "typename": "precision_recall_fscore_support",
        "desc": "Compute precision, recall, F-measure and support for each class.  The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of true positives and ``fp`` the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.  The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of true positives and ``fn`` the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.  The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.  The F-beta score weights recall more than precision by a factor of ``beta``. ``beta == 1.0`` means recall and precision are equally important.  The support is the number of occurrences of each class in ``y_true``.  If ``pos_label is None`` and in binary classification, this function returns the average precision, recall and F-measure if ``average`` is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "beta": {
                "type": "number",
                "desc": "The strength of recall versus precision in the F-score.",
                "default": "1.0",
                "dictKeyOf": "kargs"
            },
            "pos_label": {
                "type": "string",
                "desc": "The class to report if ``average='binary'`` and the data is binary.If the data are multiclass or multilabel, this will be ignored;setting ``labels=[pos_label]`` and ``average != 'binary'`` will reportscores for that label only.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "average": {
                "type": "option(binary, micro, macro, samples,weighted)",
                "desc": "If ``None``, the scores for each class are returned. Otherwise, thisdetermines the type of averaging performed on the data:``'binary'``:Only report results for the class specified by ``pos_label``.This is applicable only if targets (``y_{true,pred}``) are binary.``'micro'``:Calculate metrics globally by counting the total true positives,false negatives and false positives.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average weightedby support (the number of true instances for each label). Thisalters 'macro' to account for label imbalance; it can result in anF-score that is not between precision and recall.``'samples'``:Calculate metrics for each instance, and find their average (onlymeaningful for multilabel classification where this differs from:func:`accuracy_score`).",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "warn_for": {
                "type": "tuple or set, for internal use",
                "desc": "This determines which warnings will be made in the case that thisfunction is being used to return only one of its metrics.",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "zero_division": {
                "type": "\"warn\", 0 or 1, default=\"warn\"",
                "desc": "Sets the value to return when there is a zero division:- recall: when there are no positive labels- precision: when there are no positive predictions- f-score: bothIf set to \"warn\", this acts as 0, but warnings are also raised.",
                "default": "\"warn\"",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.precision_recall_fscore_support",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.precision_score": {
        "cls": "Block",
        "typename": "precision_score",
        "desc": "Compute the precision.  The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of true positives and ``fp`` the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.  The best value is 1 and the worst value is 0.  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "pos_label": {
                "type": "string",
                "desc": "The class to report if ``average='binary'`` and the data is binary.If the data are multiclass or multilabel, this will be ignored;setting ``labels=[pos_label]`` and ``average != 'binary'`` will reportscores for that label only.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "average": {
                "type": "option(micro, macro, samples, weighted, binary)",
                "desc": "This parameter is required for multiclass/multilabel targets.If ``None``, the scores for each class are returned. Otherwise, thisdetermines the type of averaging performed on the data:``'binary'``:Only report results for the class specified by ``pos_label``.This is applicable only if targets (``y_{true,pred}``) are binary.``'micro'``:Calculate metrics globally by counting the total true positives,false negatives and false positives.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average weightedby support (the number of true instances for each label). Thisalters 'macro' to account for label imbalance; it can result in anF-score that is not between precision and recall.``'samples'``:Calculate metrics for each instance, and find their average (onlymeaningful for multilabel classification where this differs from:func:`accuracy_score`).",
                "default": "'binary'",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "zero_division": {
                "type": "\"warn\", 0 or 1, default=\"warn\"",
                "desc": "Sets the value to return when there is a zero division. If set to\"warn\", this acts as 0, but warnings are also raised.",
                "default": "\"warn\"",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.precision_score",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.r2_score": {
        "cls": "Block",
        "typename": "r2_score",
        "desc": ":math:`R^2` (coefficient of determination) regression score function.  Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a :math:`R^2` score of 0.0.  Read more in the :ref:`User Guide <r2_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "multioutput": {
                "type": "option(raw_values, uniform_average, variance_weighted)",
                "desc": "Defines aggregating of multiple output scores.Array-like value defines weights used to average scores.Default is \"uniform_average\".'raw_values' :Returns a full set of scores in case of multioutput input.'uniform_average' :Scores of all outputs are averaged with uniform weight.'variance_weighted' :Scores of all outputs are averaged, weighted by the variancesof each individual output... versionchanged:: 0.19Default value of multioutput is 'uniform_average'.",
                "default": "'uniform_average'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.r2_score",
            "yname": "y_true",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.rand_score": {
        "cls": "Block",
        "typename": "rand_score",
        "desc": "Rand index.  The Rand Index computes a similarity measure between two clusterings by considering all pairs of samples and counting pairs that are assigned in the same or different clusters in the predicted and true clusterings.  The raw RI score is:      RI = (number of agreeing pairs) / (number of pairs)  Read more in the :ref:`User Guide <rand_score>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            }
        },
        "defaults": {
            "method": "sklearn.metrics.rand_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.recall_score": {
        "cls": "Block",
        "typename": "recall_score",
        "desc": "Compute the recall.  The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of true positives and ``fn`` the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.  The best value is 1 and the worst value is 0.  Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "pos_label": {
                "type": "string",
                "desc": "The class to report if ``average='binary'`` and the data is binary.If the data are multiclass or multilabel, this will be ignored;setting ``labels=[pos_label]`` and ``average != 'binary'`` will reportscores for that label only.",
                "default": "1",
                "dictKeyOf": "kargs"
            },
            "average": {
                "type": "option(micro, macro, samples, weighted, binary)",
                "desc": "This parameter is required for multiclass/multilabel targets.If ``None``, the scores for each class are returned. Otherwise, thisdetermines the type of averaging performed on the data:``'binary'``:Only report results for the class specified by ``pos_label``.This is applicable only if targets (``y_{true,pred}``) are binary.``'micro'``:Calculate metrics globally by counting the total true positives,false negatives and false positives.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average weightedby support (the number of true instances for each label). Thisalters 'macro' to account for label imbalance; it can result in anF-score that is not between precision and recall.``'samples'``:Calculate metrics for each instance, and find their average (onlymeaningful for multilabel classification where this differs from:func:`accuracy_score`).",
                "default": "'binary'",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "zero_division": {
                "type": "\"warn\", 0 or 1, default=\"warn\"",
                "desc": "Sets the value to return when there is a zero division. If set to\"warn\", this acts as 0, but warnings are also raised.",
                "default": "\"warn\"",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.recall_score",
            "yname": "labels",
            "xname": "y_pred"
        }
    },
    "sklearn.metrics.roc_auc_score": {
        "cls": "Block",
        "typename": "roc_auc_score",
        "desc": "Compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.  Note: this implementation can be used with binary, multiclass and multilabel classification, but some restrictions apply (see Parameters).  Read more in the :ref:`User Guide <roc_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "average": {
                "type": "option(micro, macro, samples, weighted)",
                "desc": "If ``None``, the scores for each class are returned. Otherwise,this determines the type of averaging performed on the data:Note: multiclass ROC AUC currently only handles the 'macro' and'weighted' averages.``'micro'``:Calculate metrics globally by considering each element of the labelindicator matrix as a label.``'macro'``:Calculate metrics for each label, and find their unweightedmean. This does not take label imbalance into account.``'weighted'``:Calculate metrics for each label, and find their average, weightedby support (the number of true instances for each label).``'samples'``:Calculate metrics for each instance, and find their average.Will be ignored when ``y_true`` is binary.",
                "default": "'macro'",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "max_fpr": {
                "type": "number",
                "desc": "If not ``None``, the standardized partial AUC [2]_ over the range[0, max_fpr] is returned. For the multiclass case, ``max_fpr``,should be either equal to ``None`` or ``1.0`` as AUC ROC partialcomputation currently is not supported for multiclass.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "multi_class": {
                "type": "option(raise, ovr, ovo)",
                "desc": "Only used for multiclass targets. Determines the type of configurationto use. The default value raises an error, so either``'ovr'`` or ``'ovo'`` must be passed explicitly.``'ovr'``:Stands for One-vs-rest. Computes the AUC of each classagainst the rest [3]_ [4]_. Thistreats the multiclass case in the same way as the multilabel case.Sensitive to class imbalance even when ``average == 'macro'``,because class imbalance affects the composition of each of the'rest' groupings.``'ovo'``:Stands for One-vs-one. Computes the average AUC of allpossible pairwise combinations of classes [5]_.Insensitive to class imbalance when``average == 'macro'``.",
                "default": "'raise'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.roc_auc_score",
            "yname": "labels",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.roc_curve": {
        "cls": "Block",
        "typename": "roc_curve",
        "desc": "Compute Receiver operating characteristic (ROC).  Note: this implementation is restricted to the binary classification task.  Read more in the :ref:`User Guide <roc_metrics>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "pos_label": {
                "type": "number",
                "desc": "The label of the positive class.When ``pos_label=None``, if `y_true` is in {-1, 1} or {0, 1},``pos_label`` is set to 1, otherwise an error will be raised.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "drop_intermediate": {
                "type": "boolean",
                "desc": "Whether to drop some suboptimal thresholds which would not appearon a plotted ROC curve. This is useful in order to create lighterROC curves... versionadded:: 0.17parameter *drop_intermediate*.",
                "default": "True",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.roc_curve",
            "yname": "y_true",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.silhouette_samples": {
        "cls": "Block",
        "typename": "silhouette_samples",
        "desc": "Compute the Silhouette Coefficient for each sample.  The Silhouette Coefficient is a measure of how well samples are clustered with samples that are similar to themselves. Clustering models with a high Silhouette Coefficient are said to be dense, where samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other.  The Silhouette Coefficient is calculated using the mean intra-cluster distance (``a``) and the mean nearest-cluster distance (``b``) for each sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a, b)``. Note that Silhouette Coefficient is only defined if number of labels is 2 ``<= n_labels <= n_samples - 1``.  This function returns the Silhouette Coefficient for each sample.  The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters.  Read more in the :ref:`User Guide <silhouette_coefficient>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string, it must be one of the optionsallowed by :func:`sklearn.metrics.pairwise.pairwise_distances`.If ``X`` is the distance array itself, use \"precomputed\" as the metric.Precomputed distance matrices must have 0 along the diagonal.`**kwds` : optional keyword parametersAny further parameters are passed directly to the distance function.If using a ``scipy.spatial.distance`` metric, the parameters are stillmetric dependent. See the scipy docs for usage examples.",
                "default": "'euclidean'",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.silhouette_samples",
            "xname": "X",
            "yname": "labels"
        }
    },
    "sklearn.metrics.silhouette_score": {
        "cls": "Block",
        "typename": "silhouette_score",
        "desc": "Compute the mean Silhouette Coefficient of all samples.  The Silhouette Coefficient is calculated using the mean intra-cluster distance (``a``) and the mean nearest-cluster distance (``b``) for each sample.  The Silhouette Coefficient for a sample is ``(b - a) / max(a, b)``.  To clarify, ``b`` is the distance between a sample and the nearest cluster that the sample is not a part of. Note that Silhouette Coefficient is only defined if number of labels is ``2 <= n_labels <= n_samples - 1``.  This function returns the mean Silhouette Coefficient over all samples. To obtain the values for each sample, use :func:`silhouette_samples`.  The best value is 1 and the worst value is -1. Values near 0 indicate overlapping clusters. Negative values generally indicate that a sample has been assigned to the wrong cluster, as a different cluster is more similar.  Read more in the :ref:`User Guide <silhouette_coefficient>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "metric": {
                "type": "string",
                "desc": "The metric to use when calculating distance between instances in afeature array. If metric is a string, it must be one of the optionsallowed by :func:`metrics.pairwise.pairwise_distances<sklearn.metrics.pairwise.pairwise_distances>`. If ``X`` isthe distance array itself, use ``metric=\"precomputed\"``.",
                "default": "'euclidean'",
                "dictKeyOf": "kargs"
            },
            "sample_size": {
                "type": "number",
                "desc": "The size of the sample to use when computing the Silhouette Coefficienton a random subset of the data.If ``sample_size is None``, no sampling is used.",
                "default": "None",
                "dictKeyOf": "kargs"
            },
            "random_state": {
                "type": "number",
                "desc": "Determines random number generation for selecting a subset of samples.Used when ``sample_size is not None``.Pass an int for reproducible results across multiple function calls.See :term:`Glossary <random_state>`.**kwds : optional keyword parametersAny further parameters are passed directly to the distance function.If using a scipy.spatial.distance metric, the parameters are stillmetric dependent. See the scipy docs for usage examples.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.silhouette_score",
            "xname": "X",
            "yname": "labels"
        }
    },
    "sklearn.metrics.top_k_accuracy_score": {
        "cls": "Block",
        "typename": "top_k_accuracy_score",
        "desc": "Top-k Accuracy classification score.  This metric computes the number of times where the correct label is among the top `k` labels predicted (ranked by predicted scores). Note that the multilabel case isn't covered here.  Read more in the :ref:`User Guide <top_k_accuracy_score>`",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "k": {
                "type": "number",
                "desc": "Number of most likely outcomes considered to find the correct label.",
                "default": "2",
                "dictKeyOf": "kargs"
            },
            "normalize": {
                "type": "boolean",
                "desc": "If `True`, return the fraction of correctly classified samples.Otherwise, return the number of correctly classified samples.",
                "default": "True",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights. If `None`, all samples are given the same weight.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.top_k_accuracy_score",
            "yname": "labels",
            "xname": "y_score"
        }
    },
    "sklearn.metrics.v_measure_score": {
        "cls": "Block",
        "typename": "v_measure_score",
        "desc": "V-measure cluster labeling given a ground truth.  This score is identical to :func:`normalized_mutual_info_score` with the ``'arithmetic'`` option for averaging.  The V-measure is the harmonic mean between homogeneity and completeness::      v = (1 + beta) * homogeneity * completeness          / (beta * homogeneity + completeness)  This metric is independent of the absolute values of the labels: a permutation of the class or cluster label values won't change the score value in any way.  This metric is furthermore symmetric: switching ``label_true`` with ``label_pred`` will return the same score value. This can be useful to measure the agreement of two independent label assignments strategies on the same dataset when the real ground truth is not known.   Read more in the :ref:`User Guide <homogeneity_completeness>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "beta": {
                "type": "number",
                "desc": "Ratio of weight attributed to ``homogeneity`` vs ``completeness``.If ``beta`` is greater than 1, ``completeness`` is weighted morestrongly in the calculation. If ``beta`` is less than 1,``homogeneity`` is weighted more strongly.",
                "default": "1.0",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.v_measure_score",
            "yname": "labels_true",
            "xname": "labels_pred"
        }
    },
    "sklearn.metrics.zero_one_loss": {
        "cls": "Block",
        "typename": "zero_one_loss",
        "desc": "Zero-one classification loss.  If normalize is ``True``, return the fraction of misclassifications (float), else it returns the number of misclassifications (int). The best performance is 0.  Read more in the :ref:`User Guide <zero_one_loss>`.",
        "childof": "skll.plugin.sklearn.block.SklScoringMethod",
        "pytype": "skll.plugin.sklearn.block.SklScoringMethod",
        "group": "sklearn.metrics",
        "properties": {
            "args": {
                "hidden": true
            },
            "kargs": {
                "hidden": true
            },
            "method": {
                "hidden": true
            },
            "xname": {
                "hidden": true
            },
            "yname": {
                "hidden": true
            },
            "normalize": {
                "type": "boolean",
                "desc": "If ``False``, return the number of misclassifications.Otherwise, return the fraction of misclassifications.",
                "default": "True",
                "dictKeyOf": "kargs"
            },
            "sample_weight": {
                "type": "array-like of shape (n_samples,), default=None",
                "desc": "Sample weights.",
                "default": "None",
                "dictKeyOf": "kargs"
            }
        },
        "defaults": {
            "method": "sklearn.metrics.zero_one_loss",
            "yname": "y_true",
            "xname": "y_pred"
        }
    }
}
export default sklearn;